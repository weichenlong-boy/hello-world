<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>常见损失函数和评价指标总结</title>
    <link href="/2020/06/05/undefined/"/>
    <url>/2020/06/05/undefined/</url>
    
    <content type="html"><![CDATA[<h1 id="常见损失函数和评价指标总结"><a href="#常见损失函数和评价指标总结" class="headerlink" title="常见损失函数和评价指标总结"></a>常见损失函数和评价指标总结</h1><h2 id="一、损失函数"><a href="#一、损失函数" class="headerlink" title="一、损失函数"></a>一、损失函数</h2><h3 id="1-1-回归问题"><a href="#1-1-回归问题" class="headerlink" title="1.1 回归问题"></a>1.1 回归问题</h3><h4 id="1-平方损失函数（最小二乘法）："><a href="#1-平方损失函数（最小二乘法）：" class="headerlink" title="1. 平方损失函数（最小二乘法）："></a>1. 平方损失函数（最小二乘法）：</h4><p><img src="/img/%E5%B9%B3%E5%9D%87%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" srcset="/img/loading.gif" alt="平均损失函数"></p><p>回归问题中常用的损失函数，在线性回归中，可以通过极大似然估计（MLE）推导。计算的是预测值与真实值之间距离的平方和。实际更常用的是<strong>均方误差</strong>（Mean Squared Error-MSE）：</p><p><img src="/img/%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE.png" srcset="/img/loading.gif" alt="均方误差"></p><h4 id="2-平均绝对值误差（L1）–-MAE："><a href="#2-平均绝对值误差（L1）–-MAE：" class="headerlink" title="2. 平均绝对值误差（L1）– MAE："></a>2. 平均绝对值误差（L1）– MAE：</h4><p><img src="/img/%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E5%80%BC%E8%AF%AF%E5%B7%AE.png" srcset="/img/loading.gif" alt="平均绝对值误"></p><p>MAE是目标值和预测值之差的<strong>绝对值之和</strong>，可以用来衡量预测值和真实值的距离。<strong>但是它不能给出，模型的预测值是比真实值小还是大。</strong></p><h4 id="3-MAE（L1）-VS-MSE（L2）："><a href="#3-MAE（L1）-VS-MSE（L2）：" class="headerlink" title="3. MAE（L1） VS MSE（L2）："></a>3. MAE（L1） VS MSE（L2）：</h4><ul><li><strong>MSE计算简便，但MAE对异常点有更好的鲁棒性：</strong>当数据中存在异常点时，用MSE/RMSE计算损失的模型会以牺牲了其他样本的误差为代价，朝着减小异常点误差的方向更新。然而这就会降低模型的整体性能。</li></ul><blockquote><p>直观上可以这样理解：如果我们最小化MSE来对所有的样本点只给出一个预测值，那么这个值一定是所有目标值的平均值。但如果是最小化MAE，那么这个值，则会是所有样本点目标值的中位数。众所周知，对异常值而言，中位数比均值更加鲁棒，因此MAE对于异常值也比MSE更稳定。</p></blockquote><ul><li><p><strong>NN中MAE更新梯度始终相同，而MSE则不同：</strong>MSE损失的梯度随损失增大而增大，而损失趋于0时则会减小。</p></li><li><p><strong>Loss选择建议：</strong></p></li><li><ul><li><strong>MSE：</strong>如果异常点代表在商业中很重要的异常情况，并且需要被检测出来</li><li><strong>MAE：</strong>如果只把异常值当作受损数据</li></ul></li></ul><h4 id="4-Huber损失："><a href="#4-Huber损失：" class="headerlink" title="4. Huber损失："></a><strong>4. Huber损失：</strong></h4><p><img src="/img/Huber%E6%8D%9F%E5%A4%B1.png" srcset="/img/loading.gif" alt="Huber损失"></p><p>Huber损失是绝对误差，只是在误差很小时，就变为平方误差。</p><ul><li>当Huber损失在[-a，a]之间时，等价为MSE</li><li>在[负无穷，-a]和[a，正无穷]时等价为MAE</li></ul><blockquote><p>使用MAE训练神经网络最大的一个问题就是不变的大梯度，这可能导致在使用梯度下降快要结束时，错过了最小点。而对于MSE，梯度会随着损失的减小而减小，使结果更加精确。在这种情况下，Huber损失就非常有用。它会由于梯度的减小而落在最小值附近。比起MSE，它对异常点更加鲁棒。因此，Huber损失结合了MSE和MAE的优点。但是，Huber损失的问题是我们可能需要不断调整超参数delta。</p></blockquote><p>下图是Huber跟a随的变化曲线。当a很大时，等价为<strong>MSE</strong>曲线，当a很小时，等价为<strong>MAE</strong>曲线。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/heS6wRSHVMnicjgQKMJqBDKca9Ujw9RFnyBu58P802WmLricA3Yra21kdgojwKuEkeKkRl0Lf04Wzo1GiaQ9G1Qag/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1" srcset="/img/loading.gif" alt="img"></p><h3 id="1-2-分类问题："><a href="#1-2-分类问题：" class="headerlink" title="1.2 分类问题："></a>1.2 分类问题：</h3><h4 id="1-LogLoss："><a href="#1-LogLoss：" class="headerlink" title="1. LogLoss："></a><strong>1. LogLoss：</strong></h4><p><img src="/img/LogLoss.png" srcset="/img/loading.gif" alt=""></p><p><strong>二分类</strong>任务中常用的损失函数，在LR中，通过对<strong>似然函数</strong>取对数得到。也就是<strong>交叉熵</strong>损失函数。</p><h4 id="2-指数损失函数："><a href="#2-指数损失函数：" class="headerlink" title="2. 指数损失函数："></a>2. 指数损失函数：</h4><p>Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为：</p><p><img src="/img/%E6%8C%87%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" srcset="/img/loading.gif" alt=""></p><h2 id="二、评价指标"><a href="#二、评价指标" class="headerlink" title="二、评价指标"></a>二、评价指标</h2><p>如何评估机器学习算法模型是任何项目中一个非常重要的环节。分类问题一般会选择准确率（Accuracy）或者AUC作为metric，回归问题使用MSE，但这些指标并不足以评判一个模型的好坏，接下来的内容我将尽可能包括各个评价指标。因为损失函数大部分可以直接作为评价指标，所以损失函数中出现过的简单介绍。</p><h3 id="2-1-回归问题"><a href="#2-1-回归问题" class="headerlink" title="2.1 回归问题"></a>2.1 回归问题</h3><p><strong>1. MAE：</strong>平均绝对误差（Mean Absolute Error），范围[0，正无穷]</p><p><img src="/img/%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E5%80%BC%E8%AF%AF%E5%B7%AE.png" srcset="/img/loading.gif" alt=""></p><p><strong>2. MSE：</strong>均方误差（Mean Square Error），范围[0，正无穷]</p><p><img src="/img/%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE.png" srcset="/img/loading.gif" alt=""></p><p><strong>3. RMSE：</strong>根均方误差（Root Mean Square Error），范围[0，正无穷]</p><p><img src="/img/RMSE.png" srcset="/img/loading.gif" alt=""></p><p>取均方误差的平方根可以使得量纲一致，这对于描述和表示是有意义的。</p><p><strong>4. MAPE：</strong>平均绝对百分比误差（Mean Absolute Percentage Error）</p><p><img src="/img/MAPE.png" srcset="/img/loading.gif" alt=""></p><p><strong>注意点：</strong>当真实值有数据等于0时，存在分母0除问题，该公式不可用！</p><p><strong>5. SMAPE：</strong>对称平均绝对百分比误差（Symmetric Mean Absolute Percentage Error）</p><p><img src="/img/SMAPE.png" srcset="/img/loading.gif" alt=""></p><p><strong>注意点</strong>：真实值、预测值均等于0时，存在分母为0，该公式不可用！</p><p><strong>6. R Squared:</strong></p><p><img src="/img/R_Squared.png" srcset="/img/loading.gif" alt=""></p><p>R平方即<strong>决定系数（Coefficient of determination）</strong>，被人们称为最好的衡量线性回归法的指标。</p><p>如果我们使用同一个算法模型，解决不同的问题，由于不同的数据集的量纲不同，MSE、RMSE等指标不能体现此模型针对不同问题所表现的优劣，也就无法判断模型更适合预测哪个问题。R平方得到的性能度量都在[0, 1]之间，可以判断此模型更适合预测哪个问题。</p><p><strong>公式的理解：</strong></p><ol><li>分母代表baseline（平均值）的误差，分子代表模型的预测结果产生的误差；</li><li>预测结果越大越好，R平方为1说明完美拟合，R平方为0说明和baseline一致；</li></ol><p><strong>7. 代码实现：</strong></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> r2_score<span class="hljs-comment"># MAPE和SMAPE需要自己实现</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mape</span><span class="hljs-params">(y_true, y_pred)</span>:</span>    <span class="hljs-keyword">return</span> np.mean(np.abs((y_pred - y_true) / y_true)) * <span class="hljs-number">100</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">smape</span><span class="hljs-params">(y_true, y_pred)</span>:</span>    <span class="hljs-keyword">return</span> <span class="hljs-number">2.0</span> * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true))) * <span class="hljs-number">100</span>y_true = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">-3.0</span>])y_pred = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">4.5</span>, <span class="hljs-number">3.5</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">8.0</span>, <span class="hljs-number">4.5</span>, <span class="hljs-number">1.0</span>])<span class="hljs-comment"># MSE</span>print(metrics.mean_squared_error(y_true, y_pred)) <span class="hljs-comment"># 8.107142857142858</span><span class="hljs-comment"># RMSE</span>print(np.sqrt(metrics.mean_squared_error(y_true, y_pred))) <span class="hljs-comment"># 2.847304489713536</span><span class="hljs-comment"># MAE</span>print(metrics.mean_absolute_error(y_true, y_pred)) <span class="hljs-comment"># 1.9285714285714286</span><span class="hljs-comment"># MAPE</span>print(mape(y_true, y_pred)) <span class="hljs-comment"># 76.07142857142858</span><span class="hljs-comment"># SMAPE</span>print(smape(y_true, y_pred)) <span class="hljs-comment"># 57.76942355889724</span><span class="hljs-comment"># R Squared</span>print(r2_score(y_true, y_pred))</code></pre><h3 id="2-2-分类问题"><a href="#2-2-分类问题" class="headerlink" title="2.2 分类问题"></a>2.2 分类问题</h3><h4 id="0-Confusion-Matrix（混淆矩阵）："><a href="#0-Confusion-Matrix（混淆矩阵）：" class="headerlink" title="0. Confusion Matrix（混淆矩阵）："></a><strong>0. Confusion Matrix（混淆矩阵）：</strong></h4><p>混淆矩阵一般不直接作为模型的评价指标，但是他是后续多个指标的基础。以下为二分类的混淆矩阵，多分类的混淆矩阵和这个类似。</p><table><thead><tr><th align="center"></th><th align="center">预测正例</th><th align="center">预测反例</th></tr></thead><tbody><tr><td align="center">真实正例</td><td align="center">TP（真正例）</td><td align="center">FN（假反例）</td></tr><tr><td align="center">真实反例</td><td align="center">FP（假正例）</td><td align="center">TN（真反例）</td></tr></tbody></table><p>我们训练模型的目的是为了降低FP和FN。很难说什么时候降低FP，什么时候降低FN。基于我们不同的需求，来决定降低FP还是FN。</p><ul><li><strong>降低假负数例（FN）：</strong>假设在一个癌症检测问题中，每100个人中就有5个人患有癌症。在这种情况下，即使是一个非常差的模型也可以为我们提供95％的准确度。但是，为了捕获所有癌症病例，当一个人实际上没有患癌症时，我们可能最终将其归类为癌症。因为它比不识别为癌症患者的危险要小，因为我们可以进一步检查。但是，错过癌症患者将是一个巨大的错误，因为不会对其进行进一步检查。</li><li><strong>降低假正例（FP）：</strong>假设在垃圾邮件分类任务中，垃圾邮件为正样本。如果我们收到一个正常的邮件，比如某个公司或学校的offer，模型却识别为垃圾邮件（FP），那将损失非常大。所以在这种任务中，需要尽可能降低假正例。</li></ul><h4 id="1-Accuacy（准确率）："><a href="#1-Accuacy（准确率）：" class="headerlink" title="1. Accuacy（准确率）："></a><strong>1. Accuacy（准确率）：</strong></h4><p><img src="/img/Accuracy.png" srcset="/img/loading.gif" alt=""></p><p>准确率也就是在所有样本中，有多少样本被预测正确。</p><p>当样本类别均衡时，Accuracy是一个很好的指标。</p><p>但在样本不平衡的情况下，产生效果较差。假设我们的训练数据中只有2%的正样本，98%的负样本，那么如果模型全部预测为负样本，准确率便是98%,。分类的准确率指标很高，会给我们一种模型很好的假象。</p><h4 id="2-Precision（精准率）："><a href="#2-Precision（精准率）：" class="headerlink" title="2. Precision（精准率）："></a><strong>2. Precision（精准率）：</strong></h4><p><img src="/img/Precision.png" srcset="/img/loading.gif" alt=""></p><p><strong>含义：</strong>预测为正例的样本中有多少实际为正；</p><h4 id="3-Recall（召回率）："><a href="#3-Recall（召回率）：" class="headerlink" title="3. Recall（召回率）："></a><strong>3. Recall（召回率）：</strong></h4><p><img src="/img/Recall.png" srcset="/img/loading.gif" alt=""></p><p><strong>含义：</strong>实际为正例的样本有多少被预测为正；</p><h4 id="4-P-R曲线："><a href="#4-P-R曲线：" class="headerlink" title="4. P-R曲线："></a><strong>4. P-R曲线：</strong></h4><p>通过选择不同的阈值，得到Recall和Precision，以Recall为横坐标，Precision为纵坐标得到的曲线图。</p><p><img src="/img/ps.png" srcset="/img/loading.gif" alt="img"></p><p><strong>PR曲线性质：</strong></p><ul><li><p>如果一个学习器的P-R曲线被另一个学习器的曲线完全包住，后者性能优于前者;</p></li><li><p>如果两个学习器的曲线相交，可以通过平衡点（如上图所示）来度量性能；</p></li><li><p><strong>阈值下降：</strong></p></li><li><ul><li><strong>Recall：</strong>不断增加，因为越来越多的样本被划分为正例，假设阈值为0.，全都划分为正样本了，此时recall为1；</li><li><strong>Precision：</strong>正例被判为正例的变多，但负例被判为正例的也变多了，因此precision会振荡下降，不是严格递减；</li></ul></li><li><p>如果有个划分点可以把正负样本完全区分开，那么P-R曲线面积是1*1；</p></li></ul><h4 id="5-ROC-AUC："><a href="#5-ROC-AUC：" class="headerlink" title="5. ROC-AUC："></a><strong>5. ROC-AUC：</strong></h4><p><strong>Area Under Curve(AUC)</strong> 是<strong>二分类</strong>问题中使用非常广泛的一个评价指标。<strong>AUC的本质是，任取一个正样本和负样本，模型输出正样本的值大于负样本值的概率</strong>。构成AUC的两个基本指标是假正例率和真正例率。</p><ul><li><strong>横轴-假正例率：</strong> 实际为负的样本多少被预测为正；</li></ul><p><img src="/img/%E6%A8%AA%E8%BD%B4-%E5%81%87%E6%AD%A3%E4%BE%8B%E7%8E%87.png" srcset="/img/loading.gif" alt=""></p><ul><li><strong>纵轴-真正例率：</strong> 实际为正的样本多少被预测为正；</li></ul><p><img src="/img/%E7%BA%B5%E8%BD%B4-%E7%9C%9F%E6%AD%A3%E4%BE%8B%E7%8E%87.png" srcset="/img/loading.gif" alt=""></p><p>TPR和FPR的范围均是[0,1]，通过选择不同的阈值得到TPR和FPR，然后绘制ROC曲线。</p><p><img src="/img/ew.png" srcset="/img/loading.gif" alt="img"></p><p><strong>曲线性质：</strong></p><ol><li>阈值最大时，对应<strong>坐标点为(0,0)</strong>,阈值最小时，对应<strong>坐标点(1,1)</strong>；</li><li>ROC曲线越靠近左上角，该分类器的性能越好；</li><li>对角线表示一个随机猜测分类器；</li><li>若一个学习器的ROC曲线被另一个学习器的曲线完全包住，后者性能优于前者；</li></ol><p><strong>AUC：</strong> ROC曲线下的面积为AUC值。</p><h4 id="7-代码实现："><a href="#7-代码实现：" class="headerlink" title="7. 代码实现："></a>7. 代码实现：</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score,precision_score,recall_score,f1_score,fbeta_scorey_test = [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]y_pred = [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]print(<span class="hljs-string">"准确率为:&#123;0:%&#125;"</span>.format(accuracy_score(y_test, y_pred)))print(<span class="hljs-string">"精确率为:&#123;0:%&#125;"</span>.format(precision_score(y_test, y_pred)))print(<span class="hljs-string">"召回率为:&#123;0:%&#125;"</span>.format(recall_score(y_test, y_pred)))print(<span class="hljs-string">"F1分数为:&#123;0:%&#125;"</span>.format(f1_score(y_test, y_pred)))print(<span class="hljs-string">"Fbeta为:&#123;0:%&#125;"</span>.format(fbeta_score(y_test, y_pred,beta =<span class="hljs-number">1.2</span>)))</code></pre>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Word2vec特征构造</title>
    <link href="/2020/06/04/undefined/"/>
    <url>/2020/06/04/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="Word2vec特征构造"><a href="#Word2vec特征构造" class="headerlink" title="Word2vec特征构造"></a>Word2vec特征构造</h2><p>我参考了近期数据挖掘比赛的方案，发现使用深度学习里Word2vec词向量越来越多，而且采用这种Embedding的方法很有成效，主要针对时序列数据，所以我来总结一下Word2vec特征构造。</p><p>Word2vec下有两种算法最常用：CBOW和Skip-Gram，这里就不谈Word2vec的原理了。</p><p>在数据挖掘中，常使用groupby函数来实现句子构造，例如对每个用户的行为进行分组，形成多个句子。</p><h4 id="创建句子"><a href="#创建句子" class="headerlink" title="创建句子"></a>创建句子</h4><pre><code class="hljs python">groups = df.groupby([<span class="hljs-string">'user_id'</span>])df_log[<span class="hljs-string">'creative_id'</span>] = df[<span class="hljs-string">'creative_id'</span>].astype(<span class="hljs-string">'str'</span>)print(<span class="hljs-string">'starting....'</span>)sentences = [groups.get_group(g)[<span class="hljs-string">'creative_id'</span>].tolist() <span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> groups.groups]</code></pre><h4 id="通过gensim（tutorials）库来实现"><a href="#通过gensim（tutorials）库来实现" class="headerlink" title="通过gensim（tutorials）库来实现"></a>通过gensim（<a href="https://radimrehurek.com/gensim/auto_examples/index.html" target="_blank" rel="noopener">tutorials</a>）库来实现</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> gensim.models <span class="hljs-keyword">import</span> Word2Vecsize = <span class="hljs-number">20</span>  <span class="hljs-comment">#20维，可变</span>model = Word2Vec(sentences, size=size, window=<span class="hljs-number">20</span>, min_count=<span class="hljs-number">1</span>, sg=<span class="hljs-number">1</span>, workers=<span class="hljs-number">12</span>, iter=<span class="hljs-number">10</span>)</code></pre><h4 id="创建Word2vec特征"><a href="#创建Word2vec特征" class="headerlink" title="创建Word2vec特征"></a>创建Word2vec特征</h4><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npemb = []<span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> df[<span class="hljs-string">'id'</span>].unique():    vec = [w]    <span class="hljs-keyword">try</span>:        vec.extend(model[str(w)])    <span class="hljs-keyword">except</span>:        vec.extend(np.ones(size) * -size)    emb = np.vstack((emb, vec))emb_df = pd.DataFrame(emb)emb_cols = [<span class="hljs-string">'id'</span>]<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(size):    emb_cols.append(<span class="hljs-string">'x_y_bin1_emb_&#123;&#125;'</span>.format(i))emb_df.columns = emb_cols</code></pre>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Word2vec</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>XGBoost构造特征</title>
    <link href="/2020/06/03/undefined/"/>
    <url>/2020/06/03/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="XGBoost特征构造"><a href="#XGBoost特征构造" class="headerlink" title="XGBoost特征构造"></a>XGBoost特征构造</h2><p>本文利用XGBoost构造特征，返回是每棵树的叶子节点序列，构造的特征可以用于Logistic Regression和LibFFM，下面代码实现：</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> OneHotEncoder<span class="hljs-keyword">from</span> xgboost.sklearn <span class="hljs-keyword">import</span> XGBClassifier<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">XgboostFeature</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">'''</span><span class="hljs-string">    可以传入xgboost的参数</span><span class="hljs-string">    常用传入特征的个数 即树的个数 默认30</span><span class="hljs-string">    name表示构造之后的特征用于LR还是LibFFM</span><span class="hljs-string">    '''</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,name=<span class="hljs-string">'LR'</span>,n_estimators=<span class="hljs-number">30</span>,learning_rate =<span class="hljs-number">0.3</span>,max_depth=<span class="hljs-number">3</span>,min_child_weight=<span class="hljs-number">1</span>,gamma=<span class="hljs-number">0.3</span>,subsample=<span class="hljs-number">0.8</span>,colsample_bytree=<span class="hljs-number">0.8</span>,objective= <span class="hljs-string">'binary:logistic'</span>,nthread=<span class="hljs-number">4</span>,scale_pos_weight=<span class="hljs-number">1</span>,reg_alpha=<span class="hljs-number">1e-05</span>,reg_lambda=<span class="hljs-number">1</span>,seed=<span class="hljs-number">27</span>)</span>:</span>        self.n_estimators=n_estimators        self.learning_rate=learning_rate        self.max_depth=max_depth        self.min_child_weight=min_child_weight        self.gamma=gamma        self.subsample=subsample        self.colsample_bytree=colsample_bytree        self.objective=objective        self.nthread=nthread        self.scale_pos_weight=scale_pos_weight        self.reg_alpha=reg_alpha        self.reg_lambda=reg_lambda        self.seed=seed        self.name=name        <span class="hljs-keyword">print</span> (<span class="hljs-string">'Xgboost Feature start, new_feature number:'</span>,n_estimators)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LR</span><span class="hljs-params">(self,X,Y)</span>:</span>        onehot = OneHotEncoder()        onehot_X = onehot.fit_transform(X)        onehot_Y = onehot.transform(Y)        <span class="hljs-keyword">return</span> onehot_X,onehot_Y    <span class="hljs-comment">##整体训练</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit_model</span><span class="hljs-params">(self,X_train,y_train,X_test)</span>:</span>        clf = XGBClassifier(             learning_rate =self.learning_rate,             n_estimators=self.n_estimators,             max_depth=self.max_depth,             min_child_weight=self.min_child_weight,             gamma=self.gamma,             subsample=self.subsample,             colsample_bytree=self.colsample_bytree,             objective= self.objective,             nthread=self.nthread,             scale_pos_weight=self.scale_pos_weight,             reg_alpha=self.reg_alpha,             reg_lambda=self.reg_lambda,             seed=self.seed)        clf.fit(X_train, y_train)        auc = cross_val_score(clf, X_train, y_train, scoring=<span class="hljs-string">'roc_auc'</span>, cv=<span class="hljs-number">10</span>)        <span class="hljs-keyword">print</span> (<span class="hljs-string">f'XGBoost交叉验证分数:<span class="hljs-subst">&#123;auc.mean()&#125;</span>'</span>)        new_feature= clf.apply(X_train)        new_feature_test= clf.apply(X_test)        <span class="hljs-keyword">if</span> self.name == <span class="hljs-string">'LR'</span>:            onehot_x,onehot_y = self.LR(new_feature,new_feature_test)            <span class="hljs-keyword">print</span> (<span class="hljs-string">"Training set sample number remains the same"</span>)            <span class="hljs-keyword">return</span> onehot_x,onehot_y        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">return</span> new_feature,new_feature_test</code></pre><p>调用示例，<code>name</code>为<code>&#39;LR&#39;</code>，即是onehot后的用于Logistic Regression</p><pre><code class="hljs python">xgb = XgboostFeature(name=<span class="hljs-string">'LR'</span>,n_estimators=<span class="hljs-number">100</span>)X_train_new,X_test_new = xgb.fit_model(train,label,test)</code></pre>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>XGBoost</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>XGBoost调参演示</title>
    <link href="/2020/06/02/undefined/"/>
    <url>/2020/06/02/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="XGBoost调参演示"><a href="#XGBoost调参演示" class="headerlink" title="XGBoost调参演示"></a>XGBoost调参演示</h2><p><a href="https://github.com/lytforgood/MachineLearningTrick" target="_blank" rel="noopener">本文参考</a></p><h4 id="导入模型库"><a href="#导入模型库" class="headerlink" title="导入模型库"></a>导入模型库</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_hastie_10_2<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> GradientBoostingClassifier<span class="hljs-keyword">from</span> xgboost.sklearn <span class="hljs-keyword">import</span> XGBClassifier<span class="hljs-comment">#载入示例数据 10维度</span>X, y = make_hastie_10_2(random_state=<span class="hljs-number">0</span>)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.5</span>, random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">##test_size测试集合所占比例</span></code></pre><h4 id="默认GBDT参数"><a href="#默认GBDT参数" class="headerlink" title="默认GBDT参数"></a>默认GBDT参数</h4><pre><code class="hljs python">clf = GradientBoostingClassifier()clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre))AUC Score : <span class="hljs-number">0.974248</span>Accuracy : <span class="hljs-number">0.8995</span></code></pre><h4 id="默认XGBoost参数"><a href="#默认XGBoost参数" class="headerlink" title="默认XGBoost参数"></a>默认XGBoost参数</h4><pre><code class="hljs python">auc_Score=[]accuracy=[]clf = XGBClassifier()clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre))auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))AUC Score : <span class="hljs-number">0.972424</span>Accuracy : <span class="hljs-number">0.8993</span></code></pre><h4 id="调整XGBoost参数"><a href="#调整XGBoost参数" class="headerlink" title="调整XGBoost参数"></a>调整XGBoost参数</h4><p>第一步：初始学习速率0.1和tree_based参数调优的估计器数目100。</p><p>给其他参数一个初始值。</p><ul><li>max_depth = 5 :默认6树的最大深度，这个参数的取值最好在3-10之间。</li><li>min_child_weight = 1:默认是1决定最小叶子节点样本权重和。如果是一个极不平衡的分类问题，某些叶子节点下的值会比较小，这个值取小点。</li><li>gamma = 0: 默认0，在0.1到0.2之间就可以。树的叶子节点上作进一步分裂所需的最小损失减少。这个参数后继也是要调整的。</li><li>subsample, colsample_bytree = 0.8: 样本采样、列采样。典型值的范围在0.5-0.9之间。</li><li>scale_pos_weight = 1:默认1,如果类别十分不平衡取较大正值。 </li></ul><pre><code class="hljs python">clf = XGBClassifier(         learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span>         n_estimators=<span class="hljs-number">100</span>, <span class="hljs-comment">#树的个数</span>         max_depth=<span class="hljs-number">5</span>,         min_child_weight=<span class="hljs-number">1</span>,         gamma=<span class="hljs-number">0</span>,         subsample=<span class="hljs-number">0.8</span>,         colsample_bytree=<span class="hljs-number">0.8</span>,         objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span>         nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span>         scale_pos_weight=<span class="hljs-number">1</span>,         seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))AUC Score : <span class="hljs-number">0.978546</span>Accuracy : <span class="hljs-number">0.9133</span></code></pre><p>‘n_estimators’:[100,200,500,1000,1500] ，取1000最好</p><pre><code class="hljs python">clf = XGBClassifier(         learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span>         n_estimators=<span class="hljs-number">1000</span>, <span class="hljs-comment">#树的个数</span>         max_depth=<span class="hljs-number">5</span>,         min_child_weight=<span class="hljs-number">1</span>,         gamma=<span class="hljs-number">0</span>,         subsample=<span class="hljs-number">0.8</span>,         colsample_bytree=<span class="hljs-number">0.8</span>,         objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span>         nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span>         scale_pos_weight=<span class="hljs-number">1</span>,         seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))AUC Score : <span class="hljs-number">0.992504</span>Accuracy : <span class="hljs-number">0.954</span></code></pre><p>第二步： max_depth 和 min_weight 它们对最终结果有很大的影响</p><p>max_depth range(3,10,2)=[3, 5, 7, 9]</p><p>min_weight range(1,6,2)=[1, 3, 5]</p><p>max_depth=3  min_weight=1 最好</p><pre><code class="hljs python">clf = XGBClassifier(         learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span>         n_estimators=<span class="hljs-number">1000</span>, <span class="hljs-comment">#树的个数</span>         max_depth=<span class="hljs-number">3</span>,         min_child_weight=<span class="hljs-number">1</span>,         gamma=<span class="hljs-number">0</span>,         subsample=<span class="hljs-number">0.8</span>,         colsample_bytree=<span class="hljs-number">0.8</span>,         objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span>         nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span>         scale_pos_weight=<span class="hljs-number">1</span>,         seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))</code></pre><pre><code>AUC Score : 0.991693Accuracy : 0.9485</code></pre><p>第三步：gamma参数调优</p><p>‘gamma’:[i/10.0 for i in range(0,7)]=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]</p><p>gamma=0.5 最好</p><pre><code class="hljs python">clf = XGBClassifier( learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span> n_estimators=<span class="hljs-number">1000</span>, <span class="hljs-comment">#树的个数</span> max_depth=<span class="hljs-number">3</span>, min_child_weight=<span class="hljs-number">1</span>, gamma=<span class="hljs-number">0.5</span>, subsample=<span class="hljs-number">0.8</span>, colsample_bytree=<span class="hljs-number">0.8</span>, objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span> nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span> scale_pos_weight=<span class="hljs-number">1</span>, seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))</code></pre><pre><code>AUC Score : 0.991749Accuracy : 0.9497</code></pre><p>第四步：调整subsample 和 colsample_bytree 参数</p><p> ‘subsample’:[i/10.0 for i in range(6,10)]=[0.6, 0.7, 0.8, 0.9]</p><p> ‘colsample_bytree’:[i/10.0 for i in range(6,10)]=[0.6, 0.7, 0.8, 0.9]</p><p> ‘subsample’: 0.6, ‘colsample_bytree’: 0.6 最好</p><pre><code class="hljs python">clf = XGBClassifier( learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span> n_estimators=<span class="hljs-number">1000</span>, <span class="hljs-comment">#树的个数</span> max_depth=<span class="hljs-number">3</span>, min_child_weight=<span class="hljs-number">1</span>, gamma=<span class="hljs-number">0.5</span>, subsample=<span class="hljs-number">0.6</span>, colsample_bytree=<span class="hljs-number">0.6</span>, objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span> nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span> scale_pos_weight=<span class="hljs-number">1</span>, seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))</code></pre><pre><code>AUC Score : 0.992504Accuracy : 0.954</code></pre><p>第五步：正则化参数调优</p><p>‘reg_alpha’:[1e-5, 1e-2, 0.1, 1, 100]=[1e-05, 0.01, 0.1, 1, 100] 默认0 L1正则项参数，参数值越大，模型越不容易过拟合</p><p>‘reg_lambda’:[1,5,10,50]        默认1L2正则项参数，参数值越大，模型越不容易过拟合</p><p>{‘reg_alpha’: 1e-05, ‘reg_lambda’: 1}  正则变化不大</p><pre><code class="hljs python">clf = XGBClassifier( learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span> n_estimators=<span class="hljs-number">1000</span>, <span class="hljs-comment">#树的个数</span> max_depth=<span class="hljs-number">3</span>, min_child_weight=<span class="hljs-number">1</span>, gamma=<span class="hljs-number">0.5</span>, subsample=<span class="hljs-number">0.6</span>, colsample_bytree=<span class="hljs-number">0.6</span>, objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span> nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span> scale_pos_weight=<span class="hljs-number">1</span>, reg_alpha=<span class="hljs-number">1e-05</span>, reg_lambda=<span class="hljs-number">1</span>, seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))</code></pre><pre><code>AUC Score : 0.992504Accuracy : 0.954</code></pre><p>第6步：进一步 降低学习速率 增加更多的树</p><p>‘learning_rate’:[0.01,0.1,0.3]</p><p>‘learning_rate’: 0.1 不变</p><p>‘n_estimators’:[1000,1200,1500,2000,2500]</p><p>‘n_estimators’: 2000 较好</p><pre><code class="hljs python">clf = XGBClassifier(         learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span>         n_estimators=<span class="hljs-number">2000</span>, <span class="hljs-comment">#树的个数</span>         max_depth=<span class="hljs-number">3</span>,         min_child_weight=<span class="hljs-number">1</span>,         gamma=<span class="hljs-number">0.5</span>,         subsample=<span class="hljs-number">0.6</span>,         colsample_bytree=<span class="hljs-number">0.6</span>,         objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span>         nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span>         scale_pos_weight=<span class="hljs-number">1</span>,         reg_alpha=<span class="hljs-number">1e-05</span>,         reg_lambda=<span class="hljs-number">1</span>,         seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))AUC Score : <span class="hljs-number">0.993114</span>Accuracy : <span class="hljs-number">0.957</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>XGBoost</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GBDT特征+LR</title>
    <link href="/2020/06/01/undefined/"/>
    <url>/2020/06/01/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="GBDT特征构造"><a href="#GBDT特征构造" class="headerlink" title="GBDT特征构造"></a>GBDT特征构造</h2><p><a href="https://flashgene.com/archives/71504.html" target="_blank" rel="noopener">本文参考</a></p><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>GBDT 是一种常用的非线性模型，基于集成学习中 boosting 的思想，由于GBDT本身可以发现多种有区分性的特征以及特征组合，决策树的路径可以直接作为 LR 输入特征使用，省去了人工寻找特征、特征组合的步骤。所以可以将 GBDT 的叶子结点输出，作为LR的输入，如图所示：</p><p><img src="/img/GBDT.png" srcset="/img/loading.gif" alt="GBDT"></p><p>就是先用已有特征训练GBDT模型，然后利用GBDT模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值0/1的，向量的每个元素对应于GBDT模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于GBDT模型里所有树包含的叶子结点数之和。图中的两棵树是GBDT学习到的，第一棵树有3个叶子结点，而第二棵树有2个叶子节点。对于一个输入样本点x，如果它在第一棵树最后落在其中的第二个叶子结点，而在第二棵树里最后落在其中的第一个叶子结点。那么通过GBDT获得的新特征向量为[0, 1, 0, 1, 0]，其中向量中的前三位对应第一棵树的3个叶子结点，后两位对应第二棵树的2个叶子结点。</p><p>这种通过 GBDT 生成LR特征的方式（GBDT+LR），业界已有实践（Facebook，Kaggle-2014），且效果不错，是非常值得尝试的思路。</p><h4 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h4><ul><li><strong>采用ensemble决策树而非单颗树</strong></li></ul><p>一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT 每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少颗树。按 paper 以及 Kaggle 竞赛中的 GBDT+LR 融合方式，多棵树正好满足 LR 每条训练样本可以通过 GBDT 映射成多个特征的需求。</p><ul><li><strong>采用 GBDT 而非 RF</strong></li></ul><p>RF 也是多棵树，但从效果上有实践证明不如 GBDT。且 GBDT 前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前 N 颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用 GBDT 的原因。</p><h4 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> GradientBoostingClassifier<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> OneHotEncoder<span class="hljs-comment"># 训练GBDT模型</span>gbdt = GradientBoostingClassifier(n_estimators=<span class="hljs-number">100</span>)  <span class="hljs-comment">#100颗树生成100个特征</span>gbdt.fit(data[col],data[<span class="hljs-string">'Label'</span>])<span class="hljs-comment"># 对GBDT预测结果进行onehot编码</span>onehot = OneHotEncoder()onehot1 = onehot.fit_transform(gbdt.apply(data[col])[:, :, <span class="hljs-number">0</span>])test_onehot = onehot.transform(gbdt.apply(test[col])[:, :, <span class="hljs-number">0</span>])<span class="hljs-comment"># 训练LR模型</span>lr = LogisticRegression()lr.fit(onehot1, Y_train)<span class="hljs-comment"># 测试集预测</span>Y_pred = lr.predict_proba(test_onehot)[:, <span class="hljs-number">1</span>]</code></pre><p>原文论文谈到GBDT的参数，树的数量最多500颗（500以上就没有提升了），每棵树的节点不多于12。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>贝叶斯优化调参</title>
    <link href="/2020/05/31/undefined/"/>
    <url>/2020/05/31/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="Hyperopt-Bayesian-optimization"><a href="#Hyperopt-Bayesian-optimization" class="headerlink" title="Hyperopt Bayesian optimization"></a>Hyperopt Bayesian optimization</h2><h4 id="安装（Hyperopt-Distributed-Asynchronous-Hyper-parameter-Optimization）"><a href="#安装（Hyperopt-Distributed-Asynchronous-Hyper-parameter-Optimization）" class="headerlink" title="安装（Hyperopt: Distributed Asynchronous Hyper-parameter Optimization）"></a>安装（<a href="http://hyperopt.github.io/hyperopt/#hyperopt-distributed-asynchronous-hyper-parameter-optimization" target="_blank" rel="noopener">Hyperopt: Distributed Asynchronous Hyper-parameter Optimization</a>）</h4><pre><code class="hljs python">pip install hyperopt</code></pre><h4 id="Hyperopt步骤"><a href="#Hyperopt步骤" class="headerlink" title="Hyperopt步骤"></a>Hyperopt步骤</h4><ul><li>the objective function to minimize（定义最小化函数）</li><li>the space over which to search（搜索空间）</li><li>the database to store all point evaluations of the search（存储搜索的所有点计算的数据库）</li><li>the search algorithm to use（使用的搜索算法）</li></ul><h4 id="定义搜索空间"><a href="#定义搜索空间" class="headerlink" title="定义搜索空间"></a>定义搜索空间</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> hyperopt <span class="hljs-keyword">import</span> hpspace = hp.choice(<span class="hljs-string">'a'</span>,    [        (<span class="hljs-string">'case 1'</span>, <span class="hljs-number">1</span> + hp.lognormal(<span class="hljs-string">'c1'</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)),        (<span class="hljs-string">'case 2'</span>, hp.uniform(<span class="hljs-string">'c2'</span>, <span class="hljs-number">-10</span>, <span class="hljs-number">10</span>))    ])</code></pre><p>所描述的搜索空间<code>space</code>具有3个参数：</p><ul><li><code>a</code>-选择大小写</li><li><code>c1</code>-在情况1中使用的正值参数</li><li><code>c2</code>-在情况2中使用的有界实值参数</li></ul><h5 id="常用参数表达式"><a href="#常用参数表达式" class="headerlink" title="常用参数表达式"></a>常用参数表达式</h5><p><strong>hp.choice(label, options)</strong></p><p>返回选项之一，options应该是列表或元组。元素<code>options</code>本身可以是[嵌套]随机表达。在这种情况下，仅出现在某些选项中的随机选择成为<em>条件</em>参数。</p><p><strong>hp.randint(label, upper)</strong></p><p>返回范围为[0，upper）的随机整数。这种分布的语义是，与更远的整数值相比，附近的整数值之间的损失函数<em>不再</em>具有相关性。例如，这是用于描述随机种子的适当分布。如果损失函数可能更多附近的整数值相关联，那么你或许应该用“量化”连续分布的一个，比如要么<code>quniform</code>，<code>qloguniform</code>，<code>qnormal</code>或<code>qlognormal</code>。</p><p><strong>hp.uniform(label, low, high)</strong></p><ul><li>在<code>low</code>和<code>high</code>之间返回一个值。</li><li>优化时，此变量被限制为两侧间隔。</li></ul><p><strong>hp.normal(label, mu, sigma)</strong></p><p>返回一个正态分布的实数值，平均值为<code>mu</code>，标准偏差为<code>sigma</code>。优化时，这是一个不受约束的变量。</p><h4 id="调参逻辑回归示例"><a href="#调参逻辑回归示例" class="headerlink" title="调参逻辑回归示例"></a>调参逻辑回归示例</h4><pre><code class="hljs python"><span class="hljs-comment">#hyperopt调参</span><span class="hljs-keyword">from</span> hyperopt <span class="hljs-keyword">import</span> fmin, tpe, hp, partial, Trials<span class="hljs-comment"># 自定义hyperopt的参数空间</span>space = &#123;<span class="hljs-string">'max_iter'</span>: hp.choice(<span class="hljs-string">"max_iter"</span>, range(<span class="hljs-number">1</span>,<span class="hljs-number">1000</span>)),         <span class="hljs-string">'C'</span>: hp.uniform(<span class="hljs-string">"C"</span>, <span class="hljs-number">0.001</span>,<span class="hljs-number">1000</span>),         <span class="hljs-string">'tol'</span>:hp.uniform(<span class="hljs-string">'tol'</span>, <span class="hljs-number">0.00001</span>,<span class="hljs-number">0.1</span>)         &#125;<span class="hljs-comment">#这里使用的sklearn的cross_val_score交叉验证调参，但与实际交叉验证结果会有出入</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forest1</span><span class="hljs-params">(params)</span>:</span>    logis = LogisticRegression(max_iter=params[<span class="hljs-string">"max_iter"</span>],C=params[<span class="hljs-string">"C"</span>],tol=params[<span class="hljs-string">"tol"</span>])    accuracy = cross_val_score(logis, X_cleaned, data[Y_], scoring=<span class="hljs-string">'roc_auc'</span>, cv=<span class="hljs-number">10</span>)    <span class="hljs-keyword">return</span> -accuracy.max()<span class="hljs-comment"># 开始使用hyperopt进行自动调参</span>best = fmin(forest1, space, algo=tpe.suggest,trials=Trials(), max_evals=<span class="hljs-number">50</span>)</code></pre><h4 id="调参XGBoost示例"><a href="#调参XGBoost示例" class="headerlink" title="调参XGBoost示例"></a>调参XGBoost示例</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> hyperopt <span class="hljs-keyword">import</span> fmin, tpe, hp, partial, Trials<span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb<span class="hljs-keyword">from</span> numpy.random <span class="hljs-keyword">import</span> RandomStatedtrain = xgb.DMatrix(data=x_train,label=y_train)dtest = xgb.DMatrix(data=x_test,label=y_test)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hyperopt_objective</span><span class="hljs-params">(params)</span>:</span>    model = xgb.XGBRegressor(        max_depth=int(params[<span class="hljs-string">'max_depth'</span>])+<span class="hljs-number">5</span>,        learning_rate=params[<span class="hljs-string">'learning_rate'</span>],        silent=<span class="hljs-number">1</span>,        objective=<span class="hljs-string">'reg:linear'</span>,        eval_metric=<span class="hljs-string">'rmse'</span>,        seed=<span class="hljs-number">42</span>,        nthread=<span class="hljs-number">-1</span>,    )    res = xgb.cv(model.get_params(), dtrain, num_boost_round=<span class="hljs-number">10</span>, nfold=<span class="hljs-number">5</span>,             callbacks=[xgb.callback.print_evaluation(show_stdv=<span class="hljs-literal">False</span>),                        xgb.callback.early_stop(<span class="hljs-number">3</span>)])    <span class="hljs-keyword">return</span> np.min(res[<span class="hljs-string">'test-rmse-mean'</span>]) <span class="hljs-comment"># as hyperopt minimises</span>params_space = &#123;    <span class="hljs-string">'max_depth'</span>: hp.randint(<span class="hljs-string">'max_depth'</span>, <span class="hljs-number">6</span>),    <span class="hljs-string">'learning_rate'</span>: hp.uniform(<span class="hljs-string">'learning_rate'</span>, <span class="hljs-number">1e-3</span>, <span class="hljs-number">5e-1</span>),&#125;trials = hyperopt.Trials()best = fmin(    hyperopt_objective,    space=params_space,    algo=tpe.suggest,    max_evals=<span class="hljs-number">50</span>,    trials=trials,    rstate=RandomState(<span class="hljs-number">123</span>))print(<span class="hljs-string">"\n展示hyperopt获取的最佳结果，但是要注意的是我们对hyperopt最初的取值范围做过一次转换"</span>)print(best)</code></pre><h4 id="调参LightGBM示例"><a href="#调参LightGBM示例" class="headerlink" title="调参LightGBM示例"></a>调参LightGBM示例</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> hyperopt <span class="hljs-keyword">import</span> fmin, tpe, hp, partial, Trials<span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb<span class="hljs-keyword">from</span> numpy.random <span class="hljs-keyword">import</span> RandomState<span class="hljs-keyword">import</span> warningswarnings.filterwarnings(<span class="hljs-string">'ignore'</span>)dtrain = lgb.Dataset(data=x_train,label=y_train)dtest = lgb.Dataset(data=x_test,label=y_test)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hyperopt_objective</span><span class="hljs-params">(params)</span>:</span>    model = lgb.LGBMRegressor(        num_leaves = int(params[<span class="hljs-string">'num_leaves'</span>]),        max_depth=int(params[<span class="hljs-string">'max_depth'</span>]),        learning_rate=params[<span class="hljs-string">'learning_rate'</span>],        n_estimators=int(params[<span class="hljs-string">'n_estimators'</span>]),        objective=<span class="hljs-string">'binary'</span>,        eval_metric=<span class="hljs-string">'auc'</span>,        seed=<span class="hljs-number">42</span>,        nthread=<span class="hljs-number">-1</span>,    )    res = lgb.cv(model.get_params(), dtrain, num_boost_round=<span class="hljs-number">10</span>, nfold=<span class="hljs-number">5</span>,             metrics=<span class="hljs-string">'auc'</span>,early_stopping_rounds=<span class="hljs-number">10</span>)    <span class="hljs-keyword">return</span> -np.min(res[<span class="hljs-string">'auc-mean'</span>]) <span class="hljs-comment"># as hyperopt minimises</span>params_space = &#123;    <span class="hljs-string">'max_depth'</span>: hp.uniform(<span class="hljs-string">'max_depth'</span>, <span class="hljs-number">1</span>,<span class="hljs-number">15</span>),    <span class="hljs-string">'learning_rate'</span>: hp.uniform(<span class="hljs-string">'learning_rate'</span>, <span class="hljs-number">1e-3</span>, <span class="hljs-number">3e-1</span>),    <span class="hljs-string">'num_leaves'</span>: hp.uniform(<span class="hljs-string">'num_leaves'</span>, <span class="hljs-number">1</span>,<span class="hljs-number">100</span>),    <span class="hljs-string">'n_estimators'</span>: hp.uniform(<span class="hljs-string">'n_estimators'</span>, <span class="hljs-number">1</span>,<span class="hljs-number">2000</span>),&#125;trials = Trials()best = fmin(    hyperopt_objective,    space=params_space,    algo=tpe.suggest,    max_evals=<span class="hljs-number">100</span>,    trials=trials,    rstate=RandomState(<span class="hljs-number">1</span>))print(<span class="hljs-string">"\n展示hyperopt获取的最佳结果，但是要注意的是我们对hyperopt最初的取值范围做过一次转换"</span>)print(best)</code></pre>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动特征挖掘</title>
    <link href="/2020/05/30/undefined/"/>
    <url>/2020/05/30/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="Symbolic-Learning"><a href="#Symbolic-Learning" class="headerlink" title="Symbolic Learning"></a>Symbolic Learning</h2><h4 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h4><p>在讲自动化挖掘特征之前，先来了解一下传统的<strong>遗传算法</strong></p><p>遗传算法（Genetic Algorithm, GA）起源于对生物系统所进行的计算机模拟研究。它是模仿自然界生物进化机制发展起来的随机全局搜索和优化方法，借鉴了达尔文的进化论和孟德尔的遗传学说。其本质是一种高效、并行、全局搜索的方法，能在搜索过程中自动获取和积累有关搜索空间的知识，并自适应地控制搜索过程以求得最佳解。</p><p>其主要特点是直接对结构对象进行操作，不存在求导和函数连续性的限定；具有内在的隐并行性和更好的全局寻优能力；采用概率化的寻优方法，能自动获取和指导优化的搜索空间，自适应地调整搜索方向，不需要确定的规则。遗传算法的这些性质，已被人们广泛地应用于组合优化、机器学习、信号处理、自适应控制和人工生命等领域。</p><table><thead><tr><th align="center">生物遗传概念</th><th align="center">遗传算法中的作用</th></tr></thead><tbody><tr><td align="center">适者生存</td><td align="center">在算法停止时，最优目标值的解有最大的可能被保留</td></tr><tr><td align="center">个体（individual）</td><td align="center">解</td></tr><tr><td align="center">染色体（chromosome）</td><td align="center">解的编码（字符串，向量等）</td></tr><tr><td align="center">基因（gene）</td><td align="center">解中每一分量的特征（如各分量的值）</td></tr><tr><td align="center">适应度（fitness）</td><td align="center">适应函数值</td></tr><tr><td align="center">种群（population）</td><td align="center">根据适应函数值选取的一组解</td></tr><tr><td align="center">交叉（crossover）</td><td align="center">通过交配原则产出一组新解的过程</td></tr><tr><td align="center">变异（mutation）</td><td align="center">编码的某一个分量发生变化的过程</td></tr></tbody></table><p><strong>遗传算法流程：</strong></p><p>一次迭代包括以下几个过程：</p><ol><li>染色体变异。即改变某个染色体的值；适应度越优的个体染色体变化范围越小，通过一个随机数让染色体值变化</li><li>染色体交叉。任意选择两个染色体交换部分基因；随机选择一对节点，相互交换对应的值即可</li><li>计算适应度。计算每个染色体在当前迭代下对应的适应度</li><li>优胜劣汰。杀死排名最后的那个个体。杀死之后种群数量就变少了，所以就必须要让比较优良的个体多生点来把种群数量补回来</li></ol><h4 id="Symbolic-Transformer（SymbolicTransformer）"><a href="#Symbolic-Transformer（SymbolicTransformer）" class="headerlink" title="Symbolic Transformer（SymbolicTransformer）"></a>Symbolic Transformer（<a href="https://gplearn.readthedocs.io/en/stable/reference.html#gplearn.genetic.SymbolicTransformer" target="_blank" rel="noopener"><code>SymbolicTransformer</code></a>）</h4><p>遗传程序符号转换器是一种受监督的转换器，它首先构建一组简单的随机公式来表示关系。这些公式以树状结构表示，数学函数递归地应用于变量和常量。然后，每一代连续的程序都是通过从人群中选择最适合的个体进行遗传操作，如交叉、突变或繁殖，从之前的程序进化而来。搜索最终种群，寻找彼此相关性最小的个体。</p><p>下例演示了如何使用Symbolic Transformer自动生成新的非线性特性。</p><pre><code class="hljs python"><span class="hljs-comment">#生成数据</span>rng = check_random_state(<span class="hljs-number">0</span>)  <span class="hljs-comment">#随机种子</span>boston = load_boston()perm = rng.permutation(boston.target.size)boston.data = boston.data[perm]boston.target = boston.target[perm]</code></pre><p>在这个例子中，我们将使用岭回归，并在前300个样本上训练我们的回归器，然后看看它如何在不可见的最后200个样本上执行。要击败的基准是简单的岭回归运行在数据集作为:</p><pre><code class="hljs python">est = Ridge()est.fit(boston.data[:<span class="hljs-number">300</span>, :], boston.target[:<span class="hljs-number">300</span>])print(est.score(boston.data[<span class="hljs-number">300</span>:, :], boston.target[<span class="hljs-number">300</span>:]))<span class="hljs-number">0.759145222183</span></code></pre><p>因此，现在我们将在相同的前300个示例上transformer，以生成一些新特征。让我们以20代人的2000个体为例。我们将选择其中最好的100个作为hall_of_fame，然后使用相关性最小的10个作为我们的新特性。max_samples=0.9可以控制膨胀，但是我们将其余的演变选项保留为默认值。 此处默认的metric =’pearson’是合适的，因为我们使用线性模型作为估计量。 如果我们要使用基于树的估计器，那么也可以尝试使用Spearman相关性：</p><pre><code class="hljs python">function_set = [<span class="hljs-string">'add'</span>, <span class="hljs-string">'sub'</span>, <span class="hljs-string">'mul'</span>, <span class="hljs-string">'div'</span>,                <span class="hljs-string">'sqrt'</span>, <span class="hljs-string">'log'</span>, <span class="hljs-string">'abs'</span>, <span class="hljs-string">'neg'</span>, <span class="hljs-string">'inv'</span>,                <span class="hljs-string">'max'</span>, <span class="hljs-string">'min'</span>]   <span class="hljs-comment">#交叉、突变或繁殖方式</span>gp = SymbolicTransformer(generations=<span class="hljs-number">20</span>, population_size=<span class="hljs-number">2000</span>,                         hall_of_fame=<span class="hljs-number">100</span>, n_components=<span class="hljs-number">10</span>,                         function_set=function_set,                         parsimony_coefficient=<span class="hljs-number">0.0005</span>,                         max_samples=<span class="hljs-number">0.9</span>, verbose=<span class="hljs-number">1</span>,                         random_state=<span class="hljs-number">0</span>, n_jobs=<span class="hljs-number">3</span>)gp.fit(boston.data[:<span class="hljs-number">300</span>, :], boston.target[:<span class="hljs-number">300</span>])</code></pre><p>然后，我们将把经过训练的transformer应用到整个Boston数据集(请记住，它仍然没有看到最后的200个样本)，并将其连接到原始数据:</p><pre><code class="hljs python">gp_features = gp.transform(boston.data)new_boston = np.hstack((boston.data, gp_features))  <span class="hljs-comment">#连接特征和原始数据</span></code></pre><p>现在，我们在转换后的数据集的前300个样本上训练岭回归器，看看它在最后200个样本上的表现如何:</p><pre><code class="hljs python">est = Ridge()est.fit(new_boston[:<span class="hljs-number">300</span>, :], boston.target[:<span class="hljs-number">300</span>])print(est.score(new_boston[<span class="hljs-number">300</span>:, :], boston.target[<span class="hljs-number">300</span>:]))<span class="hljs-number">0.841750404385</span></code></pre><p>可以看出效果还是比较明显的</p>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>特征降维</title>
    <link href="/2020/05/29/undefined/"/>
    <url>/2020/05/29/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="特征降维"><a href="#特征降维" class="headerlink" title="特征降维"></a>特征降维</h2><h4 id="为什么要降维"><a href="#为什么要降维" class="headerlink" title="为什么要降维"></a>为什么要降维</h4><ul><li>找到宏观信息</li><li>找到交叉效应</li><li>减少，防止过拟合</li><li>不建议先降维后拟合模型(丢失部分特征信息)</li></ul><h4 id="常用的几种降维方法"><a href="#常用的几种降维方法" class="headerlink" title="常用的几种降维方法"></a>常用的几种降维方法</h4><h5 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h5><p>API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#" target="_blank" rel="noopener"><code>sklearn.decomposition</code>.PCA</a></p><p>设有 m 条 n 维数据</p><ul><li>将原始数据按列组成 n 行 m 列矩阵 X</li><li>将 X 的每一行进行零均值化，即减去这一行的均值(标准化或者归一化，消除量纲不一致问题)</li><li>求出协方差矩阵 </li><li>求出协方差矩阵的特征值及对应的特征向量</li><li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P</li><li>Y = PX 即为降维到 k 维后的数据</li></ul><p>然而实际上，当样本维度很高时，协方差矩阵计算太慢，方针特征值分解计算效率不高，所以PCA经常是使用SVD(奇异值分解)进行求解，具体可看API里面的参数<strong>svd_solver</strong></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCAX = np.array([[<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>], [<span class="hljs-number">-2</span>, <span class="hljs-number">-1</span>], [<span class="hljs-number">-3</span>, <span class="hljs-number">-2</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">2</span>]])pca = PCA(n_components=<span class="hljs-number">2</span>)   <span class="hljs-comment">#隐变量个数，可变参数</span>pca.fit(X)result = pca.transform(X)</code></pre><h5 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h5><p>上面已经解释过。API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html?highlight=svd#" target="_blank" rel="noopener"><code>sklearn.decomposition</code>.TruncatedSVD</a></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> TruncatedSVDsvd = TruncatedSVD(n_components=<span class="hljs-number">5</span>, n_iter=<span class="hljs-number">7</span>, random_state=<span class="hljs-number">42</span>)  <span class="hljs-comment">#可变参数</span>svd.fit(X)result = svd.transform(X)</code></pre><p>对于密集数据使用PCA，对于稀疏数据使用TruncatedSVD</p><h5 id="TSNE"><a href="#TSNE" class="headerlink" title="TSNE"></a>TSNE</h5><p>t分布随机邻接嵌入。API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html?highlight=tsne#" target="_blank" rel="noopener"><code>sklearn.manifold</code>.TSNE</a></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNEX_embedded = TSNE(n_components=<span class="hljs-number">2</span>,perplexity=<span class="hljs-number">30.0</span>).fit_transform(X)<span class="hljs-comment">#tSNE中重要的参数是n_components和perplexity</span></code></pre><h5 id="NMF"><a href="#NMF" class="headerlink" title="NMF"></a>NMF</h5><p>找到两个非负矩阵(W, H)，它们的乘积近似于非负矩阵x。这种分解可以用于降维、源分离或主题提取。</p><p>API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#" target="_blank" rel="noopener"><code>sklearn.decomposition</code>.NMF</a></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> NMFmodel = NMF(n_components=<span class="hljs-number">2</span>, init=<span class="hljs-string">'random'</span>, random_state=<span class="hljs-number">0</span>)W = model.fit_transform(X)</code></pre><p>以上是常用的几种降维方法，还有其他的方法请读者自己学习。</p>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动特征编码</title>
    <link href="/2020/05/28/undefined/"/>
    <url>/2020/05/28/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="类别编码—Category-Encoders"><a href="#类别编码—Category-Encoders" class="headerlink" title="类别编码—Category Encoders"></a>类别编码—Category Encoders</h2><p>一组scikit-learn-style转换器，用于使用不同的技术将分类变量编码为数字。 在现有的scikit-learn版本中，序数编码，一键编码和哈希编码具有相似的等效项，但该库中的转换器都共享一些有用的属性：</p><ul><li>对pandas dataframes的一流支持，可作为输入（也可以作为输出）</li><li>可以显式配置按名称或索引对数据中的哪些列进行编码，或者不管输入类型如何，推断非数字列</li><li>可以基于训练集随意删除方差非常低的任何列</li><li>可移植性:对转换器进行数据培训，对其进行pickle，然后重用它，最后得到相同的结果</li><li>与sklearn完全兼容，像其他任何转换器一样输入类似数组的数据集</li></ul><p>安装：</p><pre><code class="hljs plain">pip install category_encoders</code></pre><p>导入包：</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> category_encoders <span class="hljs-keyword">as</span> ce</code></pre><p>需要注意的参数：handle_unknown和handle_missing</p><p>在目标编码中，handle_unknown 和 handle_missing 仅接受 ‘error’, ‘return_nan’ 及 ‘value’ 设定<br>两者的默认值均为 ‘value’, 即对未知类别或缺失值填充训练集的因变量平均值</p><h4 id="Label-Encoder-Ordinal"><a href="#Label-Encoder-Ordinal" class="headerlink" title="Label Encoder (Ordinal)"></a>Label Encoder (<a href="http://contrib.scikit-learn.org/category_encoders/ordinal.html" target="_blank" rel="noopener">Ordinal</a>)</h4><pre><code class="hljs python">encoder = ce.OrdinalEncoder(feature_list,handle_unknown=<span class="hljs-string">'value'</span>,handle_missing=<span class="hljs-string">'value'</span>) train_le = encoder.fit_transform(train)  <span class="hljs-comment">#训练数据,fit，然后对其进行转换</span>test_le = encoder.transform(test)    <span class="hljs-comment">#编码到test数据</span></code></pre><h4 id="One-Hot-Encoder-One-Hot"><a href="#One-Hot-Encoder-One-Hot" class="headerlink" title="One-Hot Encoder(One Hot)"></a>One-Hot Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/onehot.html" target="_blank" rel="noopener">One Hot</a>)</h4><p>一个编码分类特征的编码，每个分类产生一个特征，每个二进制。</p><pre><code class="hljs python">OHE_encoder = ce.OneHotEncoder(feature_list)train_ohe = OHE_encoder.fit_transform(train)test_ohe = OHE_encoder.transform(test)</code></pre><h4 id="Target-Encoder-Target-Encoder"><a href="#Target-Encoder-Target-Encoder" class="headerlink" title="Target Encoder(Target Encoder)"></a>Target Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/targetencoder.html" target="_blank" rel="noopener">Target Encoder</a>)</h4><pre><code class="hljs python"><span class="hljs-comment">#筛选类别小于60的特征</span>feature_list = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> data.columns <span class="hljs-keyword">if</span> len(set(data[i])) &lt; <span class="hljs-number">60</span> <span class="hljs-keyword">and</span> i != label]TE_encoder = ce.TargetEncoder(feature_list)train_te = TE_encoder.fit_transform(train[feature_list], target)test_te = TE_encoder.transform(test[feature_list])</code></pre><h4 id="Weight-of-Evidence-Encoder-Weight-of-Evidence"><a href="#Weight-of-Evidence-Encoder-Weight-of-Evidence" class="headerlink" title="Weight of Evidence Encoder(Weight of Evidence)"></a>Weight of Evidence Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/woe.html" target="_blank" rel="noopener">Weight of Evidence</a>)</h4><p>WOE是信用评分中常用的基于目标的编码器，WOE计算公式：</p><p><img src="/img/woe.png" srcset="/img/loading.gif" alt="woe"></p><p>其中Event%是每个类别中正样本占总样本比例。</p><pre><code class="hljs python">WOE_encoder = ce.WOEEncoder()train_woe = WOE_encoder.fit_transform(train[feature_list], target)test_woe = WOE_encoder.transform(test[feature_list])</code></pre><h4 id="James-Stein-Encoder-James-Stein-Encoder"><a href="#James-Stein-Encoder-James-Stein-Encoder" class="headerlink" title="James-Stein Encoder(James-Stein Encoder)"></a>James-Stein Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/jamesstein.html" target="_blank" rel="noopener">James-Stein Encoder</a>)</h4><p>James-Stein编码也是一种基于目标编码的编码方法。与M估计量编码一样，James-Stein编码器也尝试通过参数B来平衡先验概率与观测到的条件概率。但与目标编码与M估计量编码不同的是，James-Stein编码器通过方差比而不是样本大小来平衡两个概率。</p><pre><code class="hljs python">JSE_encoder = ce.JamesSteinEncoder()train_jse = JSE_encoder.fit_transform(train[feature_list], target)test_jse = JSE_encoder.transform(test[feature_list])</code></pre><h4 id="Leave-one-out-Encoder-Leave-One-Out"><a href="#Leave-one-out-Encoder-Leave-One-Out" class="headerlink" title="Leave-one-out Encoder(Leave One Out)"></a>Leave-one-out Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/leaveoneout.html" target="_blank" rel="noopener">Leave One Out</a>)</h4><p>留一法编码器通过组因变量均值对每个组进行编码，此处组指的是类别变量中的不同类别。</p><pre><code class="hljs python">LOOE_encoder = ce.LeaveOneOutEncoder()train_looe = LOOE_encoder.fit_transform(train[feature_list], target)test_looe = LOOE_encoder.transform(test[feature_list])</code></pre><h4 id="Catboost-Encoder-CatBoost-Encoder"><a href="#Catboost-Encoder-CatBoost-Encoder" class="headerlink" title="Catboost Encoder(CatBoost Encoder)"></a>Catboost Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/catboost.html" target="_blank" rel="noopener">CatBoost Encoder</a>)</h4><pre><code class="hljs python">CBE_encoder = ce.CatBoostEncoder()train_cbe = CBE_encoder.fit_transform(train[feature_list], target)test_cbe = CBE_encoder.transform(test[feature_list])</code></pre><h4 id="Helmert-Encoder-Helmert-Coding"><a href="#Helmert-Encoder-Helmert-Coding" class="headerlink" title="Helmert Encoder(Helmert Coding)"></a>Helmert Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/helmert.html" target="_blank" rel="noopener">Helmert Coding</a>)</h4><pre><code class="hljs python">HE_encoder = ce.HelmertEncoder(feature_list)train_he = HE_encoder.fit_transform(train[feature_list], target)test_he = HE_encoder.transform(test[feature_list])</code></pre><h4 id="Polynomial-Encoder-Polynomial-Coding"><a href="#Polynomial-Encoder-Polynomial-Coding" class="headerlink" title="Polynomial Encoder(Polynomial Coding)"></a>Polynomial Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/polynomial.html" target="_blank" rel="noopener">Polynomial Coding</a>)</h4><pre><code class="hljs python">PE_encoder = ce.PolynomialEncoder(feature_list)train_pe = PE_encoder.fit_transform(train[feature_list], target)test_pe = PE_encoder.transform(test[feature_list])</code></pre><p>以上编码在实战中都可以尝试，选择或者全部使用。</p>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pdpipe库的使用和实战</title>
    <link href="/2020/05/27/undefined/"/>
    <url>/2020/05/27/undefined/</url>
    
    <content type="html"><![CDATA[<h3 id="Pdpipe初识"><a href="#Pdpipe初识" class="headerlink" title="Pdpipe初识"></a>Pdpipe初识</h3><p>“在<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/#pdpipe" target="_blank" rel="noopener"><code>pdpipe</code></a>Python包为构建简洁的界面<code>pandas</code> 是有先决条件，是冗长的管线，支持scikit学习变压器的装修改造设计，具有很强的可序列化。<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/#pdpipe" target="_blank" rel="noopener"><code>pdpipe</code></a> 管道具有简单的界面，提供有关管道应用程序的信息和错误信息，支持管道算术，并使混合类型数据的处理更加容易。”   这是官方文档的解释，我个人认为pdpipe在处理生成特征变量方面确实很方便。</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>win10的Anaconda(推荐)环境下就可以。打开命令行输入：</p><pre><code class="hljs python">pip install pdpipe</code></pre><p>pdpipe的一些模块需要scikit-learn和nltk库的支持，如果没有，pdpipe则会警告，提示你需要安装。</p><h4 id="实战学习"><a href="#实战学习" class="headerlink" title="实战学习"></a>实战学习</h4><p>我们的数据集是汽车行驶信息，如下图：</p><p><img src="/img/Snipaste_2020-05-27_19-05-27.png" srcset="/img/loading.gif" alt="Snipaste_2020-05-27_19-05-27"></p><h5 id="读取代码："><a href="#读取代码：" class="headerlink" title="读取代码："></a>读取代码：</h5><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">'./data.csv'</span>,nrows=<span class="hljs-number">100000</span>,encoding=<span class="hljs-string">'gbk'</span>)  <span class="hljs-comment">#读取数据</span>df.columns = [<span class="hljs-string">'字段'</span>+str(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">14</span>)]   <span class="hljs-comment">#设置列名</span>df = df.drop([<span class="hljs-string">'字段5'</span>,<span class="hljs-string">'字段12'</span>], axis=<span class="hljs-number">1</span>)  <span class="hljs-comment">#删除无用变量</span></code></pre><p>这里需要说的一点是read_csv里的nrows参数，含义是指定读取文件的前n行，我这里n取10w，减少计算时间。</p><h5 id="数据描述："><a href="#数据描述：" class="headerlink" title="数据描述："></a>数据描述：</h5><p>字段1为id，字段3为车牌，字段4为车牌颜色，日期为时间戳，字段11和字段13为路口编码。</p><h4 id="一、清理数据"><a href="#一、清理数据" class="headerlink" title="一、清理数据"></a>一、清理数据</h4><p>使用pdpipe下的<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/basic_stages.html" target="_blank" rel="noopener">basic_stages</a>模块，该模块主要是做数据清洗。</p><ol><li>删除掉字段2为99、16、23、31、32的行数据</li><li>删除字段3不是车牌的行数据</li><li>删除掉字段3只出现过一次的行数据</li><li>将日期转化为时间戳</li></ol><p><img src="/img/carbon.png" srcset="/img/loading.gif" alt="carbon"></p><p>其中调用的函数：</p><p><img src="/img/match.png" srcset="/img/loading.gif" alt="match"></p><p>这里使用正则来配备车牌，调用re下的group(0)属性返回配备值，使用异常处理来避免不是车牌的情况下配备值不存在。</p><h4 id="二、处理特征"><a href="#二、处理特征" class="headerlink" title="二、处理特征"></a>二、处理特征</h4><p>使用pdpipe下的<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/col_generation.html" target="_blank" rel="noopener">col_generation</a>模块，该模块使用函数来生成新的变量。</p><p>将剩下的数据按照字段3相同（同一车牌）的时间顺序排列。使用groupby函数按照字段3聚合，然后对每个group排序。</p><pre><code class="hljs python">data1 = data.groupby(<span class="hljs-string">'字段3'</span>).apply(<span class="hljs-keyword">lambda</span> x: x.sort_values(<span class="hljs-string">'日期'</span>))data1.index = data.index</code></pre><p>然后开始处理：</p><ol><li>前后相邻两行记录时间差小于30min，则删除靠后一条的行数据</li><li>前后相邻两行记录的字段13相同，则删除靠前的行数据</li><li>若最后同一车牌下只剩一个行数据，则删除</li></ol><p><img src="/img/2.png" srcset="/img/loading.gif" alt="2"></p><p>函数模块：</p><p><img src="/img/1.png" srcset="/img/loading.gif" alt="1"></p><h4 id="三、小知识点"><a href="#三、小知识点" class="headerlink" title="三、小知识点"></a>三、小知识点</h4><p>groupby函数下，可以对每个group操作，group可以和dataframe一样处理。</p><pre><code class="hljs python">groupby(<span class="hljs-string">'字段3'</span>).apply(<span class="hljs-keyword">lambda</span> j: j[<span class="hljs-string">'字段13'</span>].iloc[<span class="hljs-number">0</span>])   <span class="hljs-comment">#取出每个group的第一行</span></code></pre><pre><code class="hljs python"><span class="hljs-comment">#对每个group应用函数，还可以实时操作</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f4</span><span class="hljs-params">(x)</span>:</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x)):        x[<span class="hljs-string">'OD'</span>].iloc[i] = x[<span class="hljs-string">'字段11'</span>].iloc[i][i]    <span class="hljs-keyword">return</span> xx.groupby(<span class="hljs-string">'字段3'</span>).apply(<span class="hljs-keyword">lambda</span> j: f4(j))</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>python练习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模型保存与读取</title>
    <link href="/2020/05/26/undefined/"/>
    <url>/2020/05/26/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="模型保存与读取"><a href="#模型保存与读取" class="headerlink" title="模型保存与读取"></a>模型保存与读取</h2><p>当模型训练完成后，尤其是训练集上做交叉验证，及时保存模型就可以保存精度方便下一次直接预测。本文介绍Python保存模型的方法。</p><p>有两个模块用来保存模型 : pickle和joblib</p><h4 id="导入数据和训练模型"><a href="#导入数据和训练模型" class="headerlink" title="导入数据和训练模型"></a>导入数据和训练模型</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<span class="hljs-comment">#定义一个分类器</span>svm = SVC()iris = datasets.load_iris()X = iris.datay = iris.targetX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">1</span>)<span class="hljs-comment">#训练模型</span>svm.fit(X_train, Y_train)</code></pre><h4 id="pickle"><a href="#pickle" class="headerlink" title="pickle"></a>pickle</h4><pre><code class="hljs python"><span class="hljs-keyword">import</span> pickle <span class="hljs-comment"># 保存，在当前目录下可以看到svm.pickle</span><span class="hljs-keyword">with</span> open(<span class="hljs-string">'svm.pickle'</span>,<span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> fw:    pickle.dump(svm,fw)    <span class="hljs-comment">#读取，加载svm.pickle</span><span class="hljs-keyword">with</span> open(<span class="hljs-string">'svm.pickle'</span>,<span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> fr:    new_svm = pickle.load(fr)    <span class="hljs-comment">#预测    </span>result = new_svm.predictn(X_test)</code></pre><h4 id="joblib"><a href="#joblib" class="headerlink" title="joblib"></a>joblib</h4><p>Sklearn的模型导出本质上是利用Python的Pickle机制。对Python的函数进行序列化，也就是把训练好的Transformer函数序列化并存为文件。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.externals <span class="hljs-keyword">import</span> joblib<span class="hljs-comment"># 保存，在当前目录下可以看到svm.pickle</span>joblib.dump(svm,<span class="hljs-string">'svm.pkl'</span>)<span class="hljs-comment">#读取，加载svm.pickle</span>new_svm = joblib.load(<span class="hljs-string">'svm.pkl'</span>)<span class="hljs-comment">#预测    </span>result = new_svm.predictn(X_test)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python内置的小函数</title>
    <link href="/2020/05/25/undefined/"/>
    <url>/2020/05/25/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="python内置的小函数"><a href="#python内置的小函数" class="headerlink" title="python内置的小函数"></a>python内置的小函数</h2><p>都是个人觉得强大而精致的几个函数。</p><h4 id="一用而过：lambda"><a href="#一用而过：lambda" class="headerlink" title="一用而过：lambda"></a>一用而过：lambda</h4><p>很多语言都有匿名函数，python的匿名函数写作lambda，当需要实现一定功能而又不想“大张旗鼓”的def一个函数时，lambda就是最优的选择。</p><p>其语法格式一般是这样的：</p><pre><code class="hljs python"><span class="hljs-keyword">lambda</span> x:x**<span class="hljs-number">2</span><span class="hljs-comment"># &lt;function __main__.&lt;lambda&gt;(x)&gt;</span></code></pre><p>也可以将它赋值给一个变量，由于python中一切皆对象，所以后续程序中就可以用该变量调用这个匿名函数。</p><pre><code class="hljs python">f = <span class="hljs-keyword">lambda</span> x:x**<span class="hljs-number">2</span>f(<span class="hljs-number">2</span>)<span class="hljs-comment"># 4</span></code></pre><p>当然，这里其实没必要应用lambda来实现，因为既然要显式调用，还不如干脆直接写个明确的函数罢了。lambda函数更广泛的应用场景在于该匿名函数作为另一个函数的参数传递时，应用就比较合适了，例如，将lambda作为sort()函数的key参数，就可以实现特定功能的排序。</p><pre><code class="hljs python">dyct = &#123;<span class="hljs-string">'a'</span>:<span class="hljs-number">2</span>, <span class="hljs-string">'b'</span>:<span class="hljs-number">1</span>, <span class="hljs-string">'c'</span>:<span class="hljs-number">5</span>&#125;sorted(dyct.items(), key = <span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>])<span class="hljs-comment">#日期排序</span>df.sort_values(<span class="hljs-string">'日期'</span>)<span class="hljs-comment"># [('b', 1), ('a', 2), ('c', 5)]</span></code></pre><h4 id="智能解压：zip"><a href="#智能解压：zip" class="headerlink" title="智能解压：zip"></a>智能解压：zip</h4><p>zip函数人如其名，是打包或者解包的函数，接受2个以上可迭代变量，输出对应位置组成元组后的迭代类型。例如：</p><pre><code class="hljs python">a = [<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>]b = (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>)zip(a,b)<span class="hljs-comment"># &lt;zip at 0x1da016d15c8&gt;</span>list(zip(a,b))<span class="hljs-comment"># [('a', 4), ('b', 5), ('c', 6)]</span>tuple(zip(a,b))<span class="hljs-comment"># (('a', 4), ('b', 5), ('c', 6))</span></code></pre><p>也可以接受多于2个输入可迭代变量，而且如果各迭代变量长度不一致也不会报错，只是此时返回迭代变量取决于输入总长度最短的一个。例如：</p><pre><code class="hljs python">a = [<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>, <span class="hljs-string">'e'</span>]b = (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>)c = [<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">True</span>]list(zip(a,b,c))<span class="hljs-comment"># (('a', 4, True), ('b', 5, False), ('c', 6, True))</span></code></pre><p>与zip打包相对应的用法是解包，即对一个打包形式的元素进行依次解包，并返回多个新的列表。例如：</p><pre><code class="hljs python">aZip = ((<span class="hljs-string">'a'</span>, <span class="hljs-number">4</span>, <span class="hljs-literal">True</span>), (<span class="hljs-string">'b'</span>, <span class="hljs-number">5</span>, <span class="hljs-literal">False</span>), (<span class="hljs-string">'c'</span>, <span class="hljs-number">6</span>, <span class="hljs-literal">True</span>))a, b, c = zip(*aZip)<span class="hljs-comment"># a:('a', 'b', 'c')</span><span class="hljs-comment"># b:(4, 5, 6)</span><span class="hljs-comment"># c:(True, False, True)</span></code></pre><h4 id="一一映射：map"><a href="#一一映射：map" class="headerlink" title="一一映射：map"></a>一一映射：map</h4><p>map函数也正如其取名一样，是一个将接受的迭代变量依次经过某种映射，并输出映射后的迭代变量。例如，如果对列表中的某个变量依次完成求值，并返回一个新的列表，则可以应用map：</p><pre><code class="hljs python">a = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]map(str, a)<span class="hljs-comment"># &lt;map at 0x1da017136d8&gt;</span>list(map(str, a))<span class="hljs-comment"># ['1', '2', '3', '4']</span></code></pre><p>这是map函数的一个典型用法：接受2个参数，第一个参数(上例中是str()函数）是一个要作用的函数，第二个参数是可迭代变量。</p><p>当第一个函数的参数是是一个多变量函数时，map也可以接受更多的参数。例如：</p><pre><code class="hljs python">a = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]b = [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>]list(map(<span class="hljs-keyword">lambda</span> x, y:x**y, a, b))<span class="hljs-comment"># [1, 4, 27, 64]</span></code></pre><p>与zip函数中类似，当map里的函数参数长度不匹配时并不会报错，只是输出结果将由最短的决定：</p><pre><code class="hljs python">a = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]b = [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>]list(map(<span class="hljs-keyword">lambda</span> x, y:x**y, a, b))<span class="hljs-comment"># [1, 4]</span></code></pre><h4 id="一夫当关：filter"><a href="#一夫当关：filter" class="headerlink" title="一夫当关：filter"></a>一夫当关：filter</h4><p>与map函数类似，filter函数也接受一个函数及其变量作为参数，只是要求这个函数的返回结果是bool型，并用这个bool的结果决定输出的取舍问题。例如需要对一个输入列表过滤，要求保留3的倍数：</p><pre><code class="hljs python">a = range(<span class="hljs-number">10</span>)filter(<span class="hljs-keyword">lambda</span> x:x%<span class="hljs-number">3</span>==<span class="hljs-number">0</span>, a)<span class="hljs-comment"># &lt;filter at 0x1da0171c0f0&gt;</span>list(filter(<span class="hljs-keyword">lambda</span> x:x%<span class="hljs-number">3</span>==<span class="hljs-number">0</span>, a))<span class="hljs-comment"># [0, 3, 6, 9]</span></code></pre><p>这里需注意，当filter的第一个函数返回值不是bool型时不会报错，只是它会转化为bool型判断，如果判断结果不是False（python中会判为False的变量包括0、None、[]等等），则会将其输出，否则过滤掉：</p><pre><code class="hljs python">a = range(<span class="hljs-number">10</span>)list(filter(<span class="hljs-keyword">lambda</span> x:x%<span class="hljs-number">3</span>, a))<span class="hljs-comment"># [1, 2, 4, 5, 7, 8]</span></code></pre><h4 id="万剑归宗：reduce"><a href="#万剑归宗：reduce" class="headerlink" title="万剑归宗：reduce"></a>万剑归宗：reduce</h4><p>map和filter函数都是多入多出型，实质上是完成了特定的变换或筛选。reduce则是归约函数，将一系列输入变量经过特定的函数后转化为一个结果输出。不过可能是由于应用场景有限的原因，reduce在python3中已不再是全局调用函数，必须要从functools包中导入方可使用：</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> reducea = range(<span class="hljs-number">5</span>)reduce(<span class="hljs-keyword">lambda</span> x, y: x+y, a)<span class="hljs-comment"># 10</span></code></pre><p>reduce函数还可以接受一个可选的初始值作为参数。应用reduce函数可以实现很多小trick，就看能不能想的到用的出：</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> reduces = <span class="hljs-string">'abcdefg'</span>reduce(<span class="hljs-keyword">lambda</span> x, y: y+x, s, <span class="hljs-string">'AA'</span>)<span class="hljs-comment"># 'gfedcbaAA'</span></code></pre><h4 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h4><p>当然，尽管几个函数用法不可谓不优美、功能不可谓不强大，但都不是必须的，甚至某种程度上都可以用其他形式加以替代，例如map和filter函数都可以用列表推导式来简单实现，reduce函数功能也顶多用一个for循环迭代就能解决。</p>]]></content>
    
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 10个习惯用法</title>
    <link href="/2020/05/24/undefined/"/>
    <url>/2020/05/24/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="Python-10个习惯用法"><a href="#Python-10个习惯用法" class="headerlink" title="Python 10个习惯用法"></a>Python 10个习惯用法</h2><h3 id="1、-if-not-x"><a href="#1、-if-not-x" class="headerlink" title="1、 if not x"></a>1、 if not x</h3><p>直接使用 x 和 not x 判断 x 是否为 None 或空</p><pre><code class="hljs python">x = [<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">5</span>]<span class="hljs-keyword">if</span> x:    print(<span class="hljs-string">'x is not empty '</span>)<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> x:    print(<span class="hljs-string">'x is empty'</span>)</code></pre><h3 id="2、enumerate-枚举"><a href="#2、enumerate-枚举" class="headerlink" title="2、enumerate 枚举"></a>2、enumerate 枚举</h3><p>直接使用 enumerate 枚举容器，第二个参数表示索引的起始值</p><pre><code class="hljs python">x = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>]<span class="hljs-keyword">for</span> i, e <span class="hljs-keyword">in</span> enumerate(x, <span class="hljs-number">10</span>): <span class="hljs-comment"># 枚举</span>    print(i, e)<span class="hljs-number">10</span> <span class="hljs-number">1</span><span class="hljs-number">11</span> <span class="hljs-number">3</span><span class="hljs-number">12</span> <span class="hljs-number">5</span></code></pre><h3 id="3-、in"><a href="#3-、in" class="headerlink" title="3.、in"></a>3.、in</h3><p>判断字符串是否包含某个子串，使用<code>in</code>明显更加可读：</p><pre><code class="hljs python">x = <span class="hljs-string">'zen_of_python'</span><span class="hljs-keyword">if</span> <span class="hljs-string">'zen'</span> <span class="hljs-keyword">in</span> x:    print(<span class="hljs-string">'zen is in'</span>)    zen <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span></code></pre><h3 id="4、zip-打包"><a href="#4、zip-打包" class="headerlink" title="4、zip 打包"></a>4、zip 打包</h3><p>使用 zip 打包后结合 for 使用输出一对，更加符合习惯：</p><pre><code class="hljs python">keys = [<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>]values = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>]<span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> zip(keys, values):    print(k, v)    a <span class="hljs-number">1</span>b <span class="hljs-number">3</span>c <span class="hljs-number">5</span></code></pre><h3 id="5、一对-‘’’"><a href="#5、一对-‘’’" class="headerlink" title="5、一对 ‘’’"></a>5、一对 ‘’’</h3><p>打印被分为多行的字符串，使用一对 <code>&#39;&#39;&#39;</code> 更加符合 Python 习惯：</p><pre><code class="hljs python">print(<span class="hljs-string">'''"Oh no!" He exclaimed.</span><span class="hljs-string">"It's the blemange!"'''</span>)<span class="hljs-string">"Oh no!"</span> He exclaimed.<span class="hljs-string">"It's the blemange!"</span></code></pre><h3 id="6、交换元素"><a href="#6、交换元素" class="headerlink" title="6、交换元素"></a>6、交换元素</h3><p>直接解包赋值，不要再用临时变量 ，更加符合 Python 风格：</p><pre><code class="hljs python">a, b = <span class="hljs-number">1</span>, <span class="hljs-number">3</span>a, b = b, a  <span class="hljs-comment"># 交换a,b</span></code></pre><h3 id="7、join-串联"><a href="#7、join-串联" class="headerlink" title="7、join 串联"></a>7、join 串联</h3><p>串联字符串，dataframe，更习惯使用 join：</p><pre><code class="hljs python">chars = [<span class="hljs-string">'P'</span>, <span class="hljs-string">'y'</span>, <span class="hljs-string">'t'</span>, <span class="hljs-string">'h'</span>, <span class="hljs-string">'o'</span>, <span class="hljs-string">'n'</span>]name = <span class="hljs-string">''</span>.join(chars)print(name)Python</code></pre><h3 id="8、列表生成式"><a href="#8、列表生成式" class="headerlink" title="8、列表生成式"></a>8、列表生成式</h3><p>列表生成式构建高效，符合 Python 习惯：</p><pre><code class="hljs python">data = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span>]result = [i * <span class="hljs-number">2</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> data <span class="hljs-keyword">if</span> i &amp; <span class="hljs-number">1</span>] <span class="hljs-comment"># 奇数则乘以2</span>print(result) <span class="hljs-comment"># [2, 6, 10]</span>[<span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">10</span>]</code></pre><h3 id="9、字典生成式"><a href="#9、字典生成式" class="headerlink" title="9、字典生成式"></a>9、字典生成式</h3><p>除了列表生成式，还有字典生成式：</p><pre><code class="hljs python">keys = [<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>]values = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>]d = &#123;k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> zip(keys, values)&#125;print(d)&#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">3</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">5</span>&#125;</code></pre><h3 id="10、使用any-all函数"><a href="#10、使用any-all函数" class="headerlink" title="10、使用any/all函数"></a>10、<strong>使用any/all函数</strong></h3><pre><code class="hljs python">all()方法的语法:all(iterable)</code></pre><p>iterable – 元组或列表。<br>返回值：如果iterable的所有元素不为0、’’、False或者iterable为空，all(iterable)返回True，否则返回False；<br><strong>注意：空元组、空列表返回值为True，这里要特别注意。</strong></p><pre><code class="hljs python">any()方法的语法:any(iterable)</code></pre><p>iterable – 元组或列表。<br>返回值：如果都为空、0、false，则返回false，如果不都为空、0、false，<strong>即有一个</strong>，则返回true。</p><p>例子：</p><pre><code class="hljs python"><span class="hljs-keyword">print</span> (any(c.isalnum()<span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> s))</code></pre>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python练习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习的5种采样方法</title>
    <link href="/2020/05/23/undefined/"/>
    <url>/2020/05/23/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="机器学习的5种采样方法"><a href="#机器学习的5种采样方法" class="headerlink" title="机器学习的5种采样方法"></a>机器学习的5种采样方法</h2><p>采样问题是数据科学中的常见问题，对此，WalmartLabs 的数据科学家 Rahul Agarwal 分享了数据科学家需要了解的 5 种采样方法，编译整理如下，本文介绍了在处理数据时可以使用的一些最常见的采样技术。</p><h4 id="简单随机抽样"><a href="#简单随机抽样" class="headerlink" title="简单随机抽样"></a>简单随机抽样</h4><p>假设您要选择一个群体的子集，其中该子集的每个成员被选择的概率都相等。</p><p>下面我们从一个数据集中选择 100 个采样点。</p><pre><code class="hljs python">sample_df = df.sample(<span class="hljs-number">100</span>)</code></pre><h4 id="分层采样"><a href="#分层采样" class="headerlink" title="分层采样"></a>分层采样</h4><p>分层抽样在解决样本不平衡，避免过拟合有一定效果。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,  test_size=<span class="hljs-number">0.25</span>)</code></pre><p><code>train_test_split</code>中的  <code>stratify</code>是指定某一特征或者Label，按照正负样本的比例抽取：</p><h4 id="随机欠采样和过采样"><a href="#随机欠采样和过采样" class="headerlink" title="随机欠采样和过采样"></a>随机欠采样和过采样</h4><p>我们经常会遇到不平衡的数据集。</p><p>一种广泛采用的处理高度不平衡数据集的技术称为重采样。它包括从多数类（欠采样）中删除样本或向少数类（过采样）中添加更多示例。<strong>但是在机器学习实践中，欠采样会丢失数据信息，过采样效果更好。</strong></p><p><img src="/img/%E6%8A%BD%E6%A0%B7.png" srcset="/img/loading.gif" alt="chouyang"></p><p>让我们先创建一些不平衡数据示例。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_classification<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pdX, y = make_classification( n_classes=<span class="hljs-number">2</span>, class_sep=<span class="hljs-number">1.5</span>, weights=[<span class="hljs-number">0.9</span>, <span class="hljs-number">0.1</span>], n_informative=<span class="hljs-number">3</span>, n_redundant=<span class="hljs-number">1</span>, flip_y=<span class="hljs-number">0</span>, n_features=<span class="hljs-number">20</span>, n_clusters_per_class=<span class="hljs-number">1</span>, n_samples=<span class="hljs-number">100</span>, random_state=<span class="hljs-number">10</span>)X = pd.DataFrame(X)X[ target ] = y</code></pre><p>我们现在可以使用以下方法进行随机过采样和欠采样：</p><pre><code class="hljs python">num_0 = len(X[X[ target ]==<span class="hljs-number">0</span>])num_1 = len(X[X[ target ]==<span class="hljs-number">1</span>])print(num_0,num_1)<span class="hljs-comment"># random 欠采样</span>undersampleundersampled_data = pd.concat([ X[X[ target ]==<span class="hljs-number">0</span>].sample(num_1) , X[X[ target ]==<span class="hljs-number">1</span>] ])print(len(undersampled_data))<span class="hljs-comment"># random 过采样</span>oversampleoversampled_data = pd.concat([ X[X[ target ]==<span class="hljs-number">0</span>] , X[X[ target ]==<span class="hljs-number">1</span>].sample(num_0, replace=<span class="hljs-literal">True</span>) ])print(len(oversampled_data))</code></pre><h4 id="使用-imbalanced-learn-进行欠采样和过采样（-imbalanced-learn）"><a href="#使用-imbalanced-learn-进行欠采样和过采样（-imbalanced-learn）" class="headerlink" title="使用 imbalanced-learn 进行欠采样和过采样（ imbalanced-learn）"></a>使用 imbalanced-learn 进行欠采样和过采样（<a href="https://imbalanced-learn.readthedocs.io/en/stable/index.html" target="_blank" rel="noopener"> imbalanced-learn</a>）</h4><p>imbalanced-learn（imblearn）是一个用于解决不平衡数据集问题的 python 包，它提供了多种方法来进行欠采样和过采样。</p><ul><li><strong>使用 Tomek Links 进行欠采样：</strong></li></ul><p>imbalanced-learn 提供的一种方法叫做 Tomek Links。Tomek Links 是邻近的两个相反类的例子。<br>在这个算法中，我们最终从 Tomek Links 中删除了大多数元素，这为分类器提供了一个更好的决策边界。</p><p><img src="/img/Tomek.png" srcset="/img/loading.gif" alt="Tomek"></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> imblearn.under_sampling <span class="hljs-keyword">import</span> TomekLinkstl = TomekLinks(return_indices=<span class="hljs-literal">True</span>, ratio= majority )X_tl, y_tl, id_tl = tl.fit_sample(X, y)</code></pre><ul><li><strong>使用 SMOTE 进行过采样：</strong></li></ul><p>在 SMOE（Synthetic Minority Oversampling Technique）中，我们在现有元素附近合并少数类的元素。</p><p><img src="/img/smote.png" srcset="/img/loading.gif" alt="smote"></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> imblearn.over_sampling <span class="hljs-keyword">import</span> SMOTEsmote = SMOTE(ratio= minority )X_sm, y_sm = smote.fit_sample(X, y)</code></pre><p>imbLearn 包中还有许多其他方法，可以用于欠采样（Cluster Centroids, NearMiss 等）和过采样（ADASYN 和 bSMOTE）。</p><p>抽样是数据科学中的一个重要课题，但我们实际上并没有讨论得足够多。</p><p>有时，一个好的抽样策略会大大推进项目的进展。错误的抽样策略可能会给我们带来错误的结果。因此，在选择抽样策略时应该小心。</p>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模型融合</title>
    <link href="/2020/05/22/undefined/"/>
    <url>/2020/05/22/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h2><p>模型融合采用的的思想，即多个模型的组合可以改善整体的表现。集成模型是一种能在各种的机器学习任务上提高准确率的强有力技术。模型融合是kaggle竞赛后期一个重要的环节，大体来说有如下的类型方式：</p><p>1、简单加权融合：</p><ul><li>回归（分类概率）：算术平均融合（Arithmetic mean），几何平均融合（Geometric mean）</li><li>分类：投票（Voting）</li><li>综合：排序融合(Rank averaging)，log融合</li></ul><p>2、stacking/blending:</p><ul><li>构建多层模型，并利用预测结果再拟合预测。</li></ul><p>3、 boosting/bagging:</p><ul><li>多树的提升方法，在xgboost，Adaboost，GBDT等中已经用到</li></ul><h4 id="平均法（Averaging）"><a href="#平均法（Averaging）" class="headerlink" title="平均法（Averaging）"></a>平均法（Averaging）</h4><p><strong>基本思想</strong>：对于回归问题，一个简单直接的思路是取平均。稍稍改进的方法是进行加权平均。权值可以用排序的方法确定，举个例子，比如A、B、C三种基本模型，模型效果进行排名，假设排名分别是1，2，3，那么给这三个模型赋予的权值分别是3/6、2/6、1/6。</p><p>平均法或加权平均法看似简单，其实后面的高级算法也可以说是基于此而产生的，Bagging或者Boosting都是一种把许多弱分类器这样融合成强分类器的思想。</p><ul><li><p><strong>简单算术平均法：</strong>Averaging方法就多个模型预测的结果进行平均。这种方法既可以用于回归问题，也可以用于对分类问题的概率进行平均。</p></li><li><p><strong>加权算术平均法：</strong>这种方法是平均法的扩展。考虑不同模型的能力不同，对最终结果的贡献也有差异，需要用权重来表征不同模型的重要性importance。</p></li></ul><h4 id="投票法（voting）"><a href="#投票法（voting）" class="headerlink" title="投票法（voting）"></a>投票法（voting）</h4><p><strong>基本思想</strong>：假设对于一个二分类问题，有3个基础模型，现在我们可以在这些基学习器的基础上得到一个投票的分类器，把票数最多的类作为我们要预测的类别。</p><ul><li><p><strong>绝对多数投票法：</strong>最终结果必须在投票中占一半以上。</p></li><li><p><strong>相对多数投票法：</strong>最终结果在投票中票数最多。</p></li><li><p><strong>加权投票法</strong></p></li><li><p><strong>硬投票</strong>：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。</p></li><li><p><strong>软投票：</strong>增加了设置权重的功能，可以为不同模型设置不同权重，进而区别模型不同的重要度。</p></li></ul><p>投票法实现代码：</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier<span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> VotingClassifiermodel1 = LogisticRegression(random_state=<span class="hljs-number">2020</span>)model2 = DecisionTreeClassifier(random_state=<span class="hljs-number">2020</span>)model = VotingClassifier(estimators=[(<span class="hljs-string">'lr'</span>, model1), (<span class="hljs-string">'dt'</span>, model2)], voting=<span class="hljs-string">'hard'</span>)model.fit(x_train, y_train)model.score(x_test, y_test)</code></pre><h4 id="堆叠法（Stacking）"><a href="#堆叠法（Stacking）" class="headerlink" title="堆叠法（Stacking）"></a>堆叠法（Stacking）</h4><p><strong>基本思想</strong>：stacking 就是当用初始训练数据学习出若干个基学习器后，将这几个学习器的预测结果作为新的训练集，来学习一个新的学习器。对不同模型预测的结果再进行建模。</p><p><img src="/img/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88.png" srcset="/img/loading.gif" alt="模型融合"></p><p>在stacking方法中，我们把个体学习器叫做初级学习器，用于结合的学习器叫做次级学习器或元学习器，次级学习器用于训练的数据叫做次级训练集。次级训练集是在训练集上用初级学习器得到的。</p><p>Stacking本质上就是这么直接的思路，但是直接这样有时对于如果训练集和测试集分布不那么一致的情况下是有一点问题的，其问题在于用初始模型训练的标签再利用真实标签进行再训练，毫无疑问会导致一定的模型过拟合训练集，这样或许模型在测试集上的泛化能力或者说效果会有一定的下降，因此现在的问题变成了如何降低再训练的过拟合性，这里我们一般有两种方法：</p><ul><li>次级模型尽量选择简单的线性模型，但是经过本人实践线性模型效果并不好，我推荐使用LightGBM</li><li>利用K折交叉验证</li></ul><p>调用APi实现stacking：</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> heamy.dataset <span class="hljs-keyword">import</span> Dataset<span class="hljs-keyword">from</span> heamy.estimator <span class="hljs-keyword">import</span> Regressor, Classifier<span class="hljs-keyword">from</span> heamy.pipeline <span class="hljs-keyword">import</span> ModelsPipeline<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> cross_validation<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestRegressor<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_absolute_error<span class="hljs-comment">#加载数据集</span><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_bostondata = load_boston()X, y = data[<span class="hljs-string">'data'</span>], data[<span class="hljs-string">'target'</span>]X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=<span class="hljs-number">0.1</span>, random_state=<span class="hljs-number">111</span>)<span class="hljs-comment">#创建数据集</span>dataset = Dataset(X_train,y_train,X_test)<span class="hljs-comment">#创建RF模型和LR模型</span>model_rf = Regressor(dataset=dataset, estimator=RandomForestRegressor, parameters=&#123;<span class="hljs-string">'n_estimators'</span>: <span class="hljs-number">50</span>&#125;,name=<span class="hljs-string">'rf'</span>)model_lr = Regressor(dataset=dataset, estimator=LinearRegression, parameters=&#123;<span class="hljs-string">'normalize'</span>: <span class="hljs-literal">True</span>&#125;,name=<span class="hljs-string">'lr'</span>)<span class="hljs-comment"># Stack两个模型</span><span class="hljs-comment"># Returns new dataset with out-of-fold predictions</span>pipeline = ModelsPipeline(model_rf,model_lr)stack_ds = pipeline.stack(k=<span class="hljs-number">10</span>,seed=<span class="hljs-number">111</span>)<span class="hljs-comment">#第二层使用lr模型stack</span>stacker = Regressor(dataset=stack_ds, estimator=LinearRegression)results = stacker.predict()<span class="hljs-comment"># 使用5折交叉验证结果</span>results10 = stacker.validate(k=<span class="hljs-number">5</span>,scorer=mean_absolute_error)</code></pre><p>手动实现：只创建一个基学习器为例</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> StratifiedKFold,KFold,RepeatedKFold<span class="hljs-keyword">import</span> xgboost.sklearn <span class="hljs-keyword">import</span> XGBClassifier<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npskf = StratifiedKFold(n_splits = <span class="hljs-number">5</span>, shuffle = <span class="hljs-literal">True</span> ,random_state=<span class="hljs-number">16</span>)oof_xgb = np.zeros(len(train))  <span class="hljs-comment">#创建数组</span>pre_xgb = np.zeros(len(test))<span class="hljs-keyword">for</span> k,(train_in,test_in) <span class="hljs-keyword">in</span> enumerate(skf.split(train,train_y)):    X_train,X_test,y_train,y_test = X[train_in],X[test_in],y[train_in],y[test_in]    params = &#123;<span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">0.008</span>,               <span class="hljs-string">'n_estimators'</span>: <span class="hljs-number">1000</span>              <span class="hljs-string">'max_depth'</span>: <span class="hljs-number">5</span>,              <span class="hljs-string">'subsample'</span>: <span class="hljs-number">0.8</span>,               <span class="hljs-string">'colsample_bytree'</span>: <span class="hljs-number">0.8</span>,               <span class="hljs-string">'objective'</span>:<span class="hljs-string">'binary:logistic'</span>,              <span class="hljs-string">'eval_metric'</span>:<span class="hljs-string">'auc'</span>,              <span class="hljs-string">'silent'</span>: <span class="hljs-literal">True</span>,               <span class="hljs-string">'nthread'</span>: <span class="hljs-number">4</span>,              &#125;    <span class="hljs-comment"># train</span>    clf = XGBClassifier(params)    clf.fit(trn_x, trn_y,eval_set=[(val_x, val_y)],eval_metric=<span class="hljs-string">'auc'</span>,early_stopping_rounds=<span class="hljs-number">100</span>,verbose=<span class="hljs-number">100</span>)    print(<span class="hljs-string">'Start predicting...'</span>)    oof_xgb[test_in] = clf.predict(X_test)    pre_xgb += clf.predict(test) / skf.n_splits    print(<span class="hljs-string">'XGB predict over'</span>)</code></pre><h4 id="混合法（Blending）"><a href="#混合法（Blending）" class="headerlink" title="混合法（Blending）"></a>混合法（Blending）</h4><p><strong>基本思想：</strong>Blending采用了和stacking同样的方法，不过只从训练集中选择一个fold的结果，再和原始特征进行concat作为元学习器meta learner的特征，测试集上进行同样的操作。<br>把原始的训练集先分成两部分，比如70%的数据作为新的训练集，剩下30%的数据作为测试集。</p><ul><li>第一层，我们在这70%的数据上训练多个模型，然后去预测那30%数据的label，同时也预测test集的label</li><li>在第二层，我们就直接用这30%数据在第一层预测的结果做为新特征继续训练，然后用test集第一层预测的label做特征，用第二层训练的模型做进一步预测</li></ul><p><strong>Stacking与Blending的对比：</strong></p><p>优点在于：</p><ul><li>blending比stacking简单，因为不用进行k次的交叉验证来获得stacker feature</li><li>blending避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集</li></ul><p>缺点在于：</p><ul><li>blending使用了很少的数据（第二阶段的blender只使用training set10%的量）</li><li>blender可能会过拟合</li><li>stacking使用多次的交叉验证会比较稳健</li></ul>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>连续特征处理</title>
    <link href="/2019/05/21/undefined/"/>
    <url>/2019/05/21/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="连续特征离散化"><a href="#连续特征离散化" class="headerlink" title="连续特征离散化"></a>连续特征离散化</h2><h4 id="为什么要对连续变量进行离散化"><a href="#为什么要对连续变量进行离散化" class="headerlink" title="为什么要对连续变量进行离散化"></a>为什么要对连续变量进行离散化</h4><ul><li>捕捉非线性效应</li><li>方便捕捉交叉效应—groupby</li><li>可以减少过拟合的风险，因为分箱相当于对于数据去粗粒度描述</li><li>离散化后可以提升模型的鲁棒性</li></ul><h4 id="常见的离散化方法"><a href="#常见的离散化方法" class="headerlink" title="常见的离散化方法"></a>常见的离散化方法</h4><h5 id="分箱方法"><a href="#分箱方法" class="headerlink" title="分箱方法"></a>分箱方法</h5><p>等距分箱（<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html" target="_blank" rel="noopener">pandas.cut</a>）</p><pre><code class="hljs python">pd.cut(serise, [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>], labels=<span class="hljs-literal">False</span>)  <span class="hljs-comment">#label指定标签</span></code></pre><p>等频分箱（<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html" target="_blank" rel="noopener">pandas.qcut</a>）</p><pre><code class="hljs python">pd.qcut(range(<span class="hljs-number">5</span>), <span class="hljs-number">3</span>, labels=[<span class="hljs-string">"good"</span>, <span class="hljs-string">"medium"</span>, <span class="hljs-string">"bad"</span>])---[good, good, medium, bad, bad]</code></pre><h5 id="聚类（sklearn-preprocessing-KBinsDiscretizer）"><a href="#聚类（sklearn-preprocessing-KBinsDiscretizer）" class="headerlink" title="聚类（sklearn.preprocessing.KBinsDiscretizer）"></a>聚类（<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html?highlight=kbinsdiscretizer#" target="_blank" rel="noopener"><code>sklearn.preprocessing</code>.KBinsDiscretizer</a>）</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> KBinsDiscretizerest = KBinsDiscretizer(n_bins=<span class="hljs-number">3</span>, encode=<span class="hljs-string">'ordinal'</span>, strategy=<span class="hljs-string">'kmeans'</span>)est.fit(X)Xt = est.transform(X)</code></pre><h5 id="卡方分箱"><a href="#卡方分箱" class="headerlink" title="卡方分箱"></a>卡方分箱</h5><p><a href="https://github.com/Liucoren/Chimerge/blob/master/ChiMerge.py" target="_blank" rel="noopener">参考github上</a></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pdiris = pd.read_csv(<span class="hljs-string">'iris.csv'</span>, header=<span class="hljs-literal">None</span>)iris.columns = [<span class="hljs-string">'sepal_length'</span>, <span class="hljs-string">'sepal_width'</span>,                <span class="hljs-string">'petal_length'</span>, <span class="hljs-string">'petal_width'</span>, <span class="hljs-string">'target_class'</span>]<span class="hljs-comment">#将最小卡方值添加进DataFrame</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge_rows</span><span class="hljs-params">(df,feature)</span>:</span>    tdf = df[:<span class="hljs-number">-1</span>]    distinct_values = sorted(set(tdf[<span class="hljs-string">'chi2'</span>]), reverse=<span class="hljs-literal">False</span>)    col_names =  [feature,<span class="hljs-string">'Iris-setosa'</span>, <span class="hljs-string">'Iris-versicolor'</span>,                   <span class="hljs-string">'Iris-virginica'</span>,<span class="hljs-string">'chi2'</span>]    updated_df  = pd.DataFrame(columns = col_names)  <span class="hljs-comment">#加入卡方值，生成新的数组</span>        updated_df_index=<span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> df.iterrows():         <span class="hljs-keyword">if</span>(index==<span class="hljs-number">0</span>):            updated_df.loc[len(updated_df)] = df.loc[index]            updated_df_index+=<span class="hljs-number">1</span>        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">if</span>(df.loc[index<span class="hljs-number">-1</span>][<span class="hljs-string">'chi2'</span>]==distinct_values[<span class="hljs-number">0</span>]):                updated_df.loc[updated_df_index<span class="hljs-number">-1</span>][<span class="hljs-string">'Iris-setosa'</span>]+=df.loc[index][<span class="hljs-string">'Iris-setosa'</span>]                updated_df.loc[updated_df_index<span class="hljs-number">-1</span>][<span class="hljs-string">'Iris-versicolor'</span>]+=df.loc[index][<span class="hljs-string">'Iris-versicolor'</span>]                updated_df.loc[updated_df_index<span class="hljs-number">-1</span>][<span class="hljs-string">'Iris-virginica'</span>]+=df.loc[index][<span class="hljs-string">'Iris-virginica'</span>]            <span class="hljs-keyword">else</span>:                updated_df.loc[len(updated_df)] = df.loc[index]                updated_df_index+=<span class="hljs-number">1</span>                    updated_df[<span class="hljs-string">'chi2'</span>] = <span class="hljs-number">0.</span>      <span class="hljs-keyword">return</span> updated_df        <span class="hljs-comment">#计算卡方值</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calc_chi2</span><span class="hljs-params">(array)</span>:</span>    shape = array.shape    n = float(array.sum())    row=&#123;&#125;    column=&#123;&#125;        <span class="hljs-comment">#计算每行的和</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(shape[<span class="hljs-number">0</span>]):        row[i] = array[i].sum()        <span class="hljs-comment">#计算每列的和</span>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(shape[<span class="hljs-number">1</span>]):        column[j] = array[:,j].sum()    chi2 = <span class="hljs-number">0</span>        <span class="hljs-comment">#卡方计算公式</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(shape[<span class="hljs-number">0</span>]):        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(shape[<span class="hljs-number">1</span>]):            eij = row[i]*column[j] / n             oij = array[i,j]              <span class="hljs-keyword">if</span> eij==<span class="hljs-number">0.</span>:                chi2 += <span class="hljs-number">0.</span>  <span class="hljs-comment">#确保不存在NaN值</span>            <span class="hljs-keyword">else</span>:                chi2 += math.pow((oij - eij),<span class="hljs-number">2</span>) / float(eij)      <span class="hljs-keyword">return</span> chi2    <span class="hljs-comment">#计算每一个类别的卡方值</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_chi2_column</span><span class="hljs-params">(contingency_table,feature)</span>:</span>    <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> contingency_table.iterrows():        <span class="hljs-keyword">if</span>(index!=contingency_table.shape[<span class="hljs-number">0</span>]<span class="hljs-number">-1</span>):            list1=[]            list2=[]            list1.append(contingency_table.loc[index][<span class="hljs-string">'Iris-setosa'</span>])            list1.append(contingency_table.loc[index][<span class="hljs-string">'Iris-versicolor'</span>])            list1.append(contingency_table.loc[index][<span class="hljs-string">'Iris-virginica'</span>])            list2.append(contingency_table.loc[index+<span class="hljs-number">1</span>][<span class="hljs-string">'Iris-setosa'</span>])            list2.append(contingency_table.loc[index+<span class="hljs-number">1</span>][<span class="hljs-string">'Iris-versicolor'</span>])            list2.append(contingency_table.loc[index+<span class="hljs-number">1</span>][<span class="hljs-string">'Iris-virginica'</span>])            prep_chi2 = np.array([np.array(list1),np.array(list2)])            c2 = calc_chi2(prep_chi2)            contingency_table.loc[index][<span class="hljs-string">'chi2'</span>] = c2    <span class="hljs-keyword">return</span> contingency_table<span class="hljs-comment">#计算频次表</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_contingency_table</span><span class="hljs-params">(dataframe,feature)</span>:</span>    distinct_values = sorted(set(dataframe[feature]), reverse=<span class="hljs-literal">False</span>)    col_names =  [feature,<span class="hljs-string">'Iris-setosa'</span>, <span class="hljs-string">'Iris-versicolor'</span>,<span class="hljs-string">'Iris-virginica'</span>,<span class="hljs-string">'chi2'</span>]    my_contingency  = pd.DataFrame(columns = col_names)        <span class="hljs-comment">#计算唯一值</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(distinct_values)):         temp_df=dataframe.loc[dataframe[feature]==distinct_values[i]]        count_dict = temp_df[<span class="hljs-string">"target_class"</span>].value_counts().to_dict()        setosa_count = <span class="hljs-number">0</span>        versicolor_count = <span class="hljs-number">0</span>        virginica_count = <span class="hljs-number">0</span>        <span class="hljs-keyword">if</span> <span class="hljs-string">'Iris-setosa'</span> <span class="hljs-keyword">in</span> count_dict:            setosa_count = count_dict[<span class="hljs-string">'Iris-setosa'</span>]        <span class="hljs-keyword">if</span> <span class="hljs-string">'Iris-versicolor'</span> <span class="hljs-keyword">in</span> count_dict:            versicolor_count = count_dict[<span class="hljs-string">'Iris-versicolor'</span>]        <span class="hljs-keyword">if</span> <span class="hljs-string">'Iris-virginica'</span> <span class="hljs-keyword">in</span> count_dict:            virginica_count = count_dict[<span class="hljs-string">'Iris-virginica'</span>]        new_row = [distinct_values[i],setosa_count,versicolor_count,virginica_count,<span class="hljs-number">0</span>]        my_contingency.loc[len(my_contingency)] = new_row    <span class="hljs-keyword">return</span> my_contingency<span class="hljs-comment">#ChiMerge</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">chimerge</span><span class="hljs-params">(feature, data, max_interval)</span>:</span>    df = data.sort_values(by=[feature],ascending=<span class="hljs-literal">True</span>).reset_index()        <span class="hljs-comment">#传入频次表</span>    contingency_table = create_contingency_table(df,feature)    <span class="hljs-comment">#计算初始间隔值</span>    num_intervals= contingency_table.shape[<span class="hljs-number">0</span>]     <span class="hljs-comment">#是否满足最大间隔</span>    <span class="hljs-keyword">while</span> num_intervals &gt; max_interval:         <span class="hljs-comment">#相邻列的卡方值</span>        chi2_df = update_chi2_column(contingency_table,feature)         contingency_table = merge_rows(chi2_df,feature)        num_intervals= contingency_table.shape[<span class="hljs-number">0</span>]                   <span class="hljs-comment">#得出结果</span>    print(<span class="hljs-string">'The split points for '</span>+feature+<span class="hljs-string">' are:'</span>)    <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> contingency_table.iterrows():        print(contingency_table.loc[index][feature])        print(<span class="hljs-string">'The final intervals for '</span>+feature+<span class="hljs-string">' are:'</span>)    <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> contingency_table.iterrows():        <span class="hljs-keyword">if</span>(index!=contingency_table.shape[<span class="hljs-number">0</span>]<span class="hljs-number">-1</span>):            <span class="hljs-keyword">for</span> index2, row2 <span class="hljs-keyword">in</span> df.iterrows():                <span class="hljs-keyword">if</span> df.loc[index2][feature]&lt;contingency_table.loc[index+<span class="hljs-number">1</span>][feature]:                    temp = df.loc[index2][feature]        <span class="hljs-keyword">else</span>:            temp = df[feature].iloc[<span class="hljs-number">-1</span>]        print(<span class="hljs-string">"["</span>+str(contingency_table.loc[index][feature])+<span class="hljs-string">","</span>+str(temp)+<span class="hljs-string">"]"</span>)    print(<span class="hljs-string">" "</span>)    <span class="hljs-keyword">if</span> __name__==<span class="hljs-string">'__main__'</span>:    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> [<span class="hljs-string">'sepal_length'</span>, <span class="hljs-string">'sepal_width'</span>, <span class="hljs-string">'petal_length'</span>,<span class="hljs-string">'petal_width'</span>]:        chimerge(feature=feature, data=iris, max_interval=<span class="hljs-number">6</span>)</code></pre><h2 id="特征斜度处理"><a href="#特征斜度处理" class="headerlink" title="特征斜度处理"></a>特征斜度处理</h2><p>使用skew判断斜度，然后Compute the Box-Cox transformation of boxcox_normmax</p><pre><code class="hljs python"><span class="hljs-comment">#修改斜度特征</span><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> skew, norm<span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> boxcox1p<span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> boxcox_normmax<span class="hljs-comment"># 选取数字特征</span>numeric_dtypes = [<span class="hljs-string">'int16'</span>, <span class="hljs-string">'int32'</span>, <span class="hljs-string">'int64'</span>, <span class="hljs-string">'float16'</span>, <span class="hljs-string">'float32'</span>, <span class="hljs-string">'float64'</span>]numeric = []<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X_cleaned.columns:    <span class="hljs-keyword">if</span> X_cleaned[i].dtype <span class="hljs-keyword">in</span> numeric_dtypes:        numeric.append(i)        <span class="hljs-comment"># 找到斜度特征，大于0.5的</span>skew_features = X_cleaned[numeric].apply(<span class="hljs-keyword">lambda</span> x: skew(x)).sort_values(ascending=<span class="hljs-literal">False</span>)high_skew = skew_features[skew_features &gt; <span class="hljs-number">0.5</span>]skew_index = high_skew.indexprint(<span class="hljs-string">"There are &#123;&#125; numerical features with Skew &gt; 0.5 :"</span>.format(high_skew.shape[<span class="hljs-number">0</span>]))skewness = pd.DataFrame(&#123;<span class="hljs-string">'Skew'</span> :high_skew&#125;)print(skew_features.head(<span class="hljs-number">10</span>))<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> skew_index:    X_cleaned[i] = boxcox1p(X_cleaned[i], boxcox_normmax(X_cleaned[i] + <span class="hljs-number">1</span>))</code></pre>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
