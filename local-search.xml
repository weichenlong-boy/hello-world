<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>贝叶斯优化调参</title>
    <link href="/2020/05/31/undefined/"/>
    <url>/2020/05/31/undefined/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>自动特征挖掘</title>
    <link href="/2020/05/30/undefined/"/>
    <url>/2020/05/30/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="Symbolic-Learning"><a href="#Symbolic-Learning" class="headerlink" title="Symbolic Learning"></a>Symbolic Learning</h2><h4 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h4><p>在讲自动化挖掘特征之前，先来了解一下传统的<strong>遗传算法</strong></p><p>遗传算法（Genetic Algorithm, GA）起源于对生物系统所进行的计算机模拟研究。它是模仿自然界生物进化机制发展起来的随机全局搜索和优化方法，借鉴了达尔文的进化论和孟德尔的遗传学说。其本质是一种高效、并行、全局搜索的方法，能在搜索过程中自动获取和积累有关搜索空间的知识，并自适应地控制搜索过程以求得最佳解。</p><p>其主要特点是直接对结构对象进行操作，不存在求导和函数连续性的限定；具有内在的隐并行性和更好的全局寻优能力；采用概率化的寻优方法，能自动获取和指导优化的搜索空间，自适应地调整搜索方向，不需要确定的规则。遗传算法的这些性质，已被人们广泛地应用于组合优化、机器学习、信号处理、自适应控制和人工生命等领域。</p><table><thead><tr><th align="center">生物遗传概念</th><th align="center">遗传算法中的作用</th></tr></thead><tbody><tr><td align="center">适者生存</td><td align="center">在算法停止时，最优目标值的解有最大的可能被保留</td></tr><tr><td align="center">个体（individual）</td><td align="center">解</td></tr><tr><td align="center">染色体（chromosome）</td><td align="center">解的编码（字符串，向量等）</td></tr><tr><td align="center">基因（gene）</td><td align="center">解中每一分量的特征（如各分量的值）</td></tr><tr><td align="center">适应度（fitness）</td><td align="center">适应函数值</td></tr><tr><td align="center">种群（population）</td><td align="center">根据适应函数值选取的一组解</td></tr><tr><td align="center">交叉（crossover）</td><td align="center">通过交配原则产出一组新解的过程</td></tr><tr><td align="center">变异（mutation）</td><td align="center">编码的某一个分量发生变化的过程</td></tr></tbody></table><p><strong>遗传算法流程：</strong></p><p>一次迭代包括以下几个过程：</p><ol><li>染色体变异。即改变某个染色体的值；适应度越优的个体染色体变化范围越小，通过一个随机数让染色体值变化</li><li>染色体交叉。任意选择两个染色体交换部分基因；随机选择一对节点，相互交换对应的值即可</li><li>计算适应度。计算每个染色体在当前迭代下对应的适应度</li><li>优胜劣汰。杀死排名最后的那个个体。杀死之后种群数量就变少了，所以就必须要让比较优良的个体多生点来把种群数量补回来</li></ol><h4 id="Symbolic-Transformer（SymbolicTransformer）"><a href="#Symbolic-Transformer（SymbolicTransformer）" class="headerlink" title="Symbolic Transformer（SymbolicTransformer）"></a>Symbolic Transformer（<a href="https://gplearn.readthedocs.io/en/stable/reference.html#gplearn.genetic.SymbolicTransformer" target="_blank" rel="noopener"><code>SymbolicTransformer</code></a>）</h4><p>遗传程序符号转换器是一种受监督的转换器，它首先构建一组简单的随机公式来表示关系。这些公式以树状结构表示，数学函数递归地应用于变量和常量。然后，每一代连续的程序都是通过从人群中选择最适合的个体进行遗传操作，如交叉、突变或繁殖，从之前的程序进化而来。搜索最终种群，寻找彼此相关性最小的个体。</p><p>下例演示了如何使用Symbolic Transformer自动生成新的非线性特性。</p><pre><code class="hljs python"><span class="hljs-comment">#生成数据</span>rng = check_random_state(<span class="hljs-number">0</span>)  <span class="hljs-comment">#随机种子</span>boston = load_boston()perm = rng.permutation(boston.target.size)boston.data = boston.data[perm]boston.target = boston.target[perm]</code></pre><p>在这个例子中，我们将使用岭回归，并在前300个样本上训练我们的回归器，然后看看它如何在不可见的最后200个样本上执行。要击败的基准是简单的岭回归运行在数据集作为:</p><pre><code class="hljs python">est = Ridge()est.fit(boston.data[:<span class="hljs-number">300</span>, :], boston.target[:<span class="hljs-number">300</span>])print(est.score(boston.data[<span class="hljs-number">300</span>:, :], boston.target[<span class="hljs-number">300</span>:]))<span class="hljs-number">0.759145222183</span></code></pre><p>因此，现在我们将在相同的前300个示例上transformer，以生成一些新特征。让我们以20代人的2000个体为例。我们将选择其中最好的100个作为hall_of_fame，然后使用相关性最小的10个作为我们的新特性。max_samples=0.9可以控制膨胀，但是我们将其余的演变选项保留为默认值。 此处默认的metric =’pearson’是合适的，因为我们使用线性模型作为估计量。 如果我们要使用基于树的估计器，那么也可以尝试使用Spearman相关性：</p><pre><code class="hljs python">function_set = [<span class="hljs-string">'add'</span>, <span class="hljs-string">'sub'</span>, <span class="hljs-string">'mul'</span>, <span class="hljs-string">'div'</span>,                <span class="hljs-string">'sqrt'</span>, <span class="hljs-string">'log'</span>, <span class="hljs-string">'abs'</span>, <span class="hljs-string">'neg'</span>, <span class="hljs-string">'inv'</span>,                <span class="hljs-string">'max'</span>, <span class="hljs-string">'min'</span>]   <span class="hljs-comment">#交叉、突变或繁殖方式</span>gp = SymbolicTransformer(generations=<span class="hljs-number">20</span>, population_size=<span class="hljs-number">2000</span>,                         hall_of_fame=<span class="hljs-number">100</span>, n_components=<span class="hljs-number">10</span>,                         function_set=function_set,                         parsimony_coefficient=<span class="hljs-number">0.0005</span>,                         max_samples=<span class="hljs-number">0.9</span>, verbose=<span class="hljs-number">1</span>,                         random_state=<span class="hljs-number">0</span>, n_jobs=<span class="hljs-number">3</span>)gp.fit(boston.data[:<span class="hljs-number">300</span>, :], boston.target[:<span class="hljs-number">300</span>])</code></pre><p>然后，我们将把经过训练的transformer应用到整个Boston数据集(请记住，它仍然没有看到最后的200个样本)，并将其连接到原始数据:</p><pre><code class="hljs python">gp_features = gp.transform(boston.data)new_boston = np.hstack((boston.data, gp_features))  <span class="hljs-comment">#连接特征和原始数据</span></code></pre><p>现在，我们在转换后的数据集的前300个样本上训练岭回归器，看看它在最后200个样本上的表现如何:</p><pre><code class="hljs python">est = Ridge()est.fit(new_boston[:<span class="hljs-number">300</span>, :], boston.target[:<span class="hljs-number">300</span>])print(est.score(new_boston[<span class="hljs-number">300</span>:, :], boston.target[<span class="hljs-number">300</span>:]))<span class="hljs-number">0.841750404385</span></code></pre><p>可以看出效果还是比较明显的</p>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>特征降维</title>
    <link href="/2020/05/29/undefined/"/>
    <url>/2020/05/29/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="特征降维"><a href="#特征降维" class="headerlink" title="特征降维"></a>特征降维</h2><h4 id="为什么要降维"><a href="#为什么要降维" class="headerlink" title="为什么要降维"></a>为什么要降维</h4><ul><li>找到宏观信息</li><li>找到交叉效应</li><li>减少，防止过拟合</li><li>不建议先降维后拟合模型(丢失部分特征信息)</li></ul><h4 id="常用的几种降维方法"><a href="#常用的几种降维方法" class="headerlink" title="常用的几种降维方法"></a>常用的几种降维方法</h4><h5 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h5><p>API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#" target="_blank" rel="noopener"><code>sklearn.decomposition</code>.PCA</a></p><p>设有 m 条 n 维数据</p><ul><li>将原始数据按列组成 n 行 m 列矩阵 X</li><li>将 X 的每一行进行零均值化，即减去这一行的均值(标准化或者归一化，消除量纲不一致问题)</li><li>求出协方差矩阵 </li><li>求出协方差矩阵的特征值及对应的特征向量</li><li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P</li><li>Y = PX 即为降维到 k 维后的数据</li></ul><p>然而实际上，当样本维度很高时，协方差矩阵计算太慢，方针特征值分解计算效率不高，所以PCA经常是使用SVD(奇异值分解)进行求解，具体可看API里面的参数<strong>svd_solver</strong></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCAX = np.array([[<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>], [<span class="hljs-number">-2</span>, <span class="hljs-number">-1</span>], [<span class="hljs-number">-3</span>, <span class="hljs-number">-2</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">2</span>]])pca = PCA(n_components=<span class="hljs-number">2</span>)   <span class="hljs-comment">#隐变量个数，可变参数</span>pca.fit(X)result = pca.transform(X)</code></pre><h5 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h5><p>上面已经解释过。API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html?highlight=svd#" target="_blank" rel="noopener"><code>sklearn.decomposition</code>.TruncatedSVD</a></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> TruncatedSVDsvd = TruncatedSVD(n_components=<span class="hljs-number">5</span>, n_iter=<span class="hljs-number">7</span>, random_state=<span class="hljs-number">42</span>)  <span class="hljs-comment">#可变参数</span>svd.fit(X)result = svd.transform(X)</code></pre><p>对于密集数据使用PCA，对于稀疏数据使用TruncatedSVD</p><h5 id="TSNE"><a href="#TSNE" class="headerlink" title="TSNE"></a>TSNE</h5><p>t分布随机邻接嵌入。API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html?highlight=tsne#" target="_blank" rel="noopener"><code>sklearn.manifold</code>.TSNE</a></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNEX_embedded = TSNE(n_components=<span class="hljs-number">2</span>,perplexity=<span class="hljs-number">30.0</span>).fit_transform(X)<span class="hljs-comment">#tSNE中重要的参数是n_components和perplexity</span></code></pre><h5 id="NMF"><a href="#NMF" class="headerlink" title="NMF"></a>NMF</h5><p>找到两个非负矩阵(W, H)，它们的乘积近似于非负矩阵x。这种分解可以用于降维、源分离或主题提取。</p><p>API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#" target="_blank" rel="noopener"><code>sklearn.decomposition</code>.NMF</a></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> NMFmodel = NMF(n_components=<span class="hljs-number">2</span>, init=<span class="hljs-string">'random'</span>, random_state=<span class="hljs-number">0</span>)W = model.fit_transform(X)</code></pre><p>以上是常用的几种降维方法，还有其他的方法请读者自己学习。</p>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动特征编码</title>
    <link href="/2020/05/28/undefined/"/>
    <url>/2020/05/28/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="类别编码—Category-Encoders"><a href="#类别编码—Category-Encoders" class="headerlink" title="类别编码—Category Encoders"></a>类别编码—Category Encoders</h2><p>一组scikit-learn-style转换器，用于使用不同的技术将分类变量编码为数字。 在现有的scikit-learn版本中，序数编码，一键编码和哈希编码具有相似的等效项，但该库中的转换器都共享一些有用的属性：</p><ul><li>对pandas dataframes的一流支持，可作为输入（也可以作为输出）</li><li>可以显式配置按名称或索引对数据中的哪些列进行编码，或者不管输入类型如何，推断非数字列</li><li>可以基于训练集随意删除方差非常低的任何列</li><li>可移植性:对转换器进行数据培训，对其进行pickle，然后重用它，最后得到相同的结果</li><li>与sklearn完全兼容，像其他任何转换器一样输入类似数组的数据集</li></ul><p>安装：</p><pre><code class="hljs plain">pip install category_encoders</code></pre><p>导入包：</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> category_encoders <span class="hljs-keyword">as</span> ce</code></pre><p>需要注意的参数：handle_unknown和handle_missing</p><p>在目标编码中，handle_unknown 和 handle_missing 仅接受 ‘error’, ‘return_nan’ 及 ‘value’ 设定<br>两者的默认值均为 ‘value’, 即对未知类别或缺失值填充训练集的因变量平均值</p><h4 id="Label-Encoder-Ordinal"><a href="#Label-Encoder-Ordinal" class="headerlink" title="Label Encoder (Ordinal)"></a>Label Encoder (<a href="http://contrib.scikit-learn.org/category_encoders/ordinal.html" target="_blank" rel="noopener">Ordinal</a>)</h4><pre><code class="hljs python">encoder = ce.OrdinalEncoder(feature_list,handle_unknown=<span class="hljs-string">'value'</span>,handle_missing=<span class="hljs-string">'value'</span>) train_le = encoder.fit_transform(train)  <span class="hljs-comment">#训练数据,fit，然后对其进行转换</span>test_le = encoder.transform(test)    <span class="hljs-comment">#编码到test数据</span></code></pre><h4 id="One-Hot-Encoder-One-Hot"><a href="#One-Hot-Encoder-One-Hot" class="headerlink" title="One-Hot Encoder(One Hot)"></a>One-Hot Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/onehot.html" target="_blank" rel="noopener">One Hot</a>)</h4><p>一个编码分类特征的编码，每个分类产生一个特征，每个二进制。</p><pre><code class="hljs python">OHE_encoder = ce.OneHotEncoder(feature_list)train_ohe = OHE_encoder.fit_transform(train)test_ohe = OHE_encoder.transform(test)</code></pre><h4 id="Target-Encoder-Target-Encoder"><a href="#Target-Encoder-Target-Encoder" class="headerlink" title="Target Encoder(Target Encoder)"></a>Target Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/targetencoder.html" target="_blank" rel="noopener">Target Encoder</a>)</h4><pre><code class="hljs python"><span class="hljs-comment">#筛选类别小于60的特征</span>feature_list = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> data.columns <span class="hljs-keyword">if</span> len(set(data[i])) &lt; <span class="hljs-number">60</span> <span class="hljs-keyword">and</span> i != label]TE_encoder = ce.TargetEncoder(feature_list)train_te = TE_encoder.fit_transform(train[feature_list], target)test_te = TE_encoder.transform(test[feature_list])</code></pre><h4 id="Weight-of-Evidence-Encoder-Weight-of-Evidence"><a href="#Weight-of-Evidence-Encoder-Weight-of-Evidence" class="headerlink" title="Weight of Evidence Encoder(Weight of Evidence)"></a>Weight of Evidence Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/woe.html" target="_blank" rel="noopener">Weight of Evidence</a>)</h4><p>WOE是信用评分中常用的基于目标的编码器，WOE计算公式：</p><p><img src="/img/woe.png" srcset="/img/loading.gif" alt="woe"></p><p>其中Event%是每个类别中正样本占总样本比例。</p><pre><code class="hljs python">WOE_encoder = ce.WOEEncoder()train_woe = WOE_encoder.fit_transform(train[feature_list], target)test_woe = WOE_encoder.transform(test[feature_list])</code></pre><h4 id="James-Stein-Encoder-James-Stein-Encoder"><a href="#James-Stein-Encoder-James-Stein-Encoder" class="headerlink" title="James-Stein Encoder(James-Stein Encoder)"></a>James-Stein Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/jamesstein.html" target="_blank" rel="noopener">James-Stein Encoder</a>)</h4><p>James-Stein编码也是一种基于目标编码的编码方法。与M估计量编码一样，James-Stein编码器也尝试通过参数B来平衡先验概率与观测到的条件概率。但与目标编码与M估计量编码不同的是，James-Stein编码器通过方差比而不是样本大小来平衡两个概率。</p><pre><code class="hljs python">JSE_encoder = ce.JamesSteinEncoder()train_jse = JSE_encoder.fit_transform(train[feature_list], target)test_jse = JSE_encoder.transform(test[feature_list])</code></pre><h4 id="Leave-one-out-Encoder-Leave-One-Out"><a href="#Leave-one-out-Encoder-Leave-One-Out" class="headerlink" title="Leave-one-out Encoder(Leave One Out)"></a>Leave-one-out Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/leaveoneout.html" target="_blank" rel="noopener">Leave One Out</a>)</h4><p>留一法编码器通过组因变量均值对每个组进行编码，此处组指的是类别变量中的不同类别。</p><pre><code class="hljs python">LOOE_encoder = ce.LeaveOneOutEncoder()train_looe = LOOE_encoder.fit_transform(train[feature_list], target)test_looe = LOOE_encoder.transform(test[feature_list])</code></pre><h4 id="Catboost-Encoder-CatBoost-Encoder"><a href="#Catboost-Encoder-CatBoost-Encoder" class="headerlink" title="Catboost Encoder(CatBoost Encoder)"></a>Catboost Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/catboost.html" target="_blank" rel="noopener">CatBoost Encoder</a>)</h4><pre><code class="hljs python">CBE_encoder = ce.CatBoostEncoder()train_cbe = CBE_encoder.fit_transform(train[feature_list], target)test_cbe = CBE_encoder.transform(test[feature_list])</code></pre><h4 id="Helmert-Encoder-Helmert-Coding"><a href="#Helmert-Encoder-Helmert-Coding" class="headerlink" title="Helmert Encoder(Helmert Coding)"></a>Helmert Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/helmert.html" target="_blank" rel="noopener">Helmert Coding</a>)</h4><pre><code class="hljs python">HE_encoder = ce.HelmertEncoder(feature_list)train_he = HE_encoder.fit_transform(train[feature_list], target)test_he = HE_encoder.transform(test[feature_list])</code></pre><h4 id="Polynomial-Encoder-Polynomial-Coding"><a href="#Polynomial-Encoder-Polynomial-Coding" class="headerlink" title="Polynomial Encoder(Polynomial Coding)"></a>Polynomial Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/polynomial.html" target="_blank" rel="noopener">Polynomial Coding</a>)</h4><pre><code class="hljs python">PE_encoder = ce.PolynomialEncoder(feature_list)train_pe = PE_encoder.fit_transform(train[feature_list], target)test_pe = PE_encoder.transform(test[feature_list])</code></pre><p>以上编码在实战中都可以尝试，选择或者全部使用。</p>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pdpipe库的使用和实战</title>
    <link href="/2020/05/27/undefined/"/>
    <url>/2020/05/27/undefined/</url>
    
    <content type="html"><![CDATA[<h3 id="Pdpipe初识"><a href="#Pdpipe初识" class="headerlink" title="Pdpipe初识"></a>Pdpipe初识</h3><p>“在<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/#pdpipe" target="_blank" rel="noopener"><code>pdpipe</code></a>Python包为构建简洁的界面<code>pandas</code> 是有先决条件，是冗长的管线，支持scikit学习变压器的装修改造设计，具有很强的可序列化。<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/#pdpipe" target="_blank" rel="noopener"><code>pdpipe</code></a> 管道具有简单的界面，提供有关管道应用程序的信息和错误信息，支持管道算术，并使混合类型数据的处理更加容易。”   这是官方文档的解释，我个人认为pdpipe在处理生成特征变量方面确实很方便。</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>win10的Anaconda(推荐)环境下就可以。打开命令行输入：</p><pre><code class="hljs python">pip install pdpipe</code></pre><p>pdpipe的一些模块需要scikit-learn和nltk库的支持，如果没有，pdpipe则会警告，提示你需要安装。</p><h4 id="实战学习"><a href="#实战学习" class="headerlink" title="实战学习"></a>实战学习</h4><p>我们的数据集是汽车行驶信息，如下图：</p><p><img src="/img/Snipaste_2020-05-27_19-05-27.png" srcset="/img/loading.gif" alt="Snipaste_2020-05-27_19-05-27"></p><h5 id="读取代码："><a href="#读取代码：" class="headerlink" title="读取代码："></a>读取代码：</h5><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">'./data.csv'</span>,nrows=<span class="hljs-number">100000</span>,encoding=<span class="hljs-string">'gbk'</span>)  <span class="hljs-comment">#读取数据</span>df.columns = [<span class="hljs-string">'字段'</span>+str(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">14</span>)]   <span class="hljs-comment">#设置列名</span>df = df.drop([<span class="hljs-string">'字段5'</span>,<span class="hljs-string">'字段12'</span>], axis=<span class="hljs-number">1</span>)  <span class="hljs-comment">#删除无用变量</span></code></pre><p>这里需要说的一点是read_csv里的nrows参数，含义是指定读取文件的前n行，我这里n取10w，减少计算时间。</p><h5 id="数据描述："><a href="#数据描述：" class="headerlink" title="数据描述："></a>数据描述：</h5><p>字段1为id，字段3为车牌，字段4为车牌颜色，日期为时间戳，字段11和字段13为路口编码。</p><h4 id="一、清理数据"><a href="#一、清理数据" class="headerlink" title="一、清理数据"></a>一、清理数据</h4><p>使用pdpipe下的<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/basic_stages.html" target="_blank" rel="noopener">basic_stages</a>模块，该模块主要是做数据清洗。</p><ol><li>删除掉字段2为99、16、23、31、32的行数据</li><li>删除字段3不是车牌的行数据</li><li>删除掉字段3只出现过一次的行数据</li><li>将日期转化为时间戳</li></ol><p><img src="/img/carbon.png" srcset="/img/loading.gif" alt="carbon"></p><p>其中调用的函数：</p><p><img src="/img/match.png" srcset="/img/loading.gif" alt="match"></p><p>这里使用正则来配备车牌，调用re下的group(0)属性返回配备值，使用异常处理来避免不是车牌的情况下配备值不存在。</p><h4 id="二、处理特征"><a href="#二、处理特征" class="headerlink" title="二、处理特征"></a>二、处理特征</h4><p>使用pdpipe下的<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/col_generation.html" target="_blank" rel="noopener">col_generation</a>模块，该模块使用函数来生成新的变量。</p><p>将剩下的数据按照字段3相同（同一车牌）的时间顺序排列。使用groupby函数按照字段3聚合，然后对每个group排序。</p><pre><code class="hljs python">data1 = data.groupby(<span class="hljs-string">'字段3'</span>).apply(<span class="hljs-keyword">lambda</span> x: x.sort_values(<span class="hljs-string">'日期'</span>))data1.index = data.index</code></pre><p>然后开始处理：</p><ol><li>前后相邻两行记录时间差小于30min，则删除靠后一条的行数据</li><li>前后相邻两行记录的字段13相同，则删除靠前的行数据</li><li>若最后同一车牌下只剩一个行数据，则删除</li></ol><p><img src="/img/2.png" srcset="/img/loading.gif" alt="2"></p><p>函数模块：</p><p><img src="/img/1.png" srcset="/img/loading.gif" alt="1"></p><h4 id="三、小知识点"><a href="#三、小知识点" class="headerlink" title="三、小知识点"></a>三、小知识点</h4><p>groupby函数下，可以对每个group操作，group可以和dataframe一样处理。</p><pre><code class="hljs python">groupby(<span class="hljs-string">'字段3'</span>).apply(<span class="hljs-keyword">lambda</span> j: j[<span class="hljs-string">'字段13'</span>].iloc[<span class="hljs-number">0</span>])   <span class="hljs-comment">#取出每个group的第一行</span></code></pre><pre><code class="hljs python"><span class="hljs-comment">#对每个group应用函数，还可以实时操作</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f4</span><span class="hljs-params">(x)</span>:</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x)):        x[<span class="hljs-string">'OD'</span>].iloc[i] = x[<span class="hljs-string">'字段11'</span>].iloc[i][i]    <span class="hljs-keyword">return</span> xx.groupby(<span class="hljs-string">'字段3'</span>).apply(<span class="hljs-keyword">lambda</span> j: f4(j))</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>python练习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>连续特征处理</title>
    <link href="/2019/05/21/undefined/"/>
    <url>/2019/05/21/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="连续特征离散化"><a href="#连续特征离散化" class="headerlink" title="连续特征离散化"></a>连续特征离散化</h2><h4 id="为什么要对连续变量进行离散化"><a href="#为什么要对连续变量进行离散化" class="headerlink" title="为什么要对连续变量进行离散化"></a>为什么要对连续变量进行离散化</h4><ul><li>捕捉非线性效应</li><li>方便捕捉交叉效应—groupby</li><li>可以减少过拟合的风险，因为分箱相当于对于数据去粗粒度描述</li><li>离散化后可以提升模型的鲁棒性</li></ul><h4 id="常见的离散化方法"><a href="#常见的离散化方法" class="headerlink" title="常见的离散化方法"></a>常见的离散化方法</h4><h5 id="分箱方法"><a href="#分箱方法" class="headerlink" title="分箱方法"></a>分箱方法</h5><p>等距分箱（<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html" target="_blank" rel="noopener">pandas.cut</a>）</p><pre><code class="hljs python">pd.cut(serise, [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>], labels=<span class="hljs-literal">False</span>)  <span class="hljs-comment">#label指定标签</span></code></pre><p>等频分箱（<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html" target="_blank" rel="noopener">pandas.qcut</a>）</p><pre><code class="hljs python">pd.qcut(range(<span class="hljs-number">5</span>), <span class="hljs-number">3</span>, labels=[<span class="hljs-string">"good"</span>, <span class="hljs-string">"medium"</span>, <span class="hljs-string">"bad"</span>])---[good, good, medium, bad, bad]</code></pre><h5 id="聚类（sklearn-preprocessing-KBinsDiscretizer）"><a href="#聚类（sklearn-preprocessing-KBinsDiscretizer）" class="headerlink" title="聚类（sklearn.preprocessing.KBinsDiscretizer）"></a>聚类（<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html?highlight=kbinsdiscretizer#" target="_blank" rel="noopener"><code>sklearn.preprocessing</code>.KBinsDiscretizer</a>）</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> KBinsDiscretizerest = KBinsDiscretizer(n_bins=<span class="hljs-number">3</span>, encode=<span class="hljs-string">'ordinal'</span>, strategy=<span class="hljs-string">'kmeans'</span>)est.fit(X)Xt = est.transform(X)</code></pre><h5 id="基于树"><a href="#基于树" class="headerlink" title="基于树"></a>基于树</h5><p>下面用sklearn的api和sklearn的决策树简单做了一个基于cart tree的决策树分箱，实现了sklearn原生的cart tree和lgb以及xgb tree分箱的功能，参考<a href="https://zhuanlan.zhihu.com/p/68865422" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/68865422</a></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.base <span class="hljs-keyword">import</span> BaseEstimator, TransformerMixin<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">supervised_bins_Transformer</span><span class="hljs-params">(BaseEstimator, TransformerMixin)</span>:</span><span class="hljs-comment">#我们这里写一个分箱的方法</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, task=<span class="hljs-string">'Classification'</span>, method=<span class="hljs-string">'xgboost_tree'</span>)</span>:</span>        self.task=task        self.method=method <span class="hljs-comment">#method 支持4种类型，一种是xgboost tree，一种是lightgbm tree，一种是sklearn tree</span>        <span class="hljs-comment">#一种是chimerge </span>     <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, X=None, y=None)</span>:</span>        <span class="hljs-keyword">if</span> self.method==<span class="hljs-string">'xgboost_tree'</span>:            <span class="hljs-keyword">if</span> self.task==<span class="hljs-string">'Classification'</span>:                clf=xgb.XGBClassifier(n_estimators=<span class="hljs-number">1</span>)            <span class="hljs-keyword">else</span>:                clf=xgb.XGBRegressor(n_estimators=<span class="hljs-number">1</span>)            clf.fit(X,y)                            <span class="hljs-keyword">if</span> self.method==<span class="hljs-string">'lightgbm_tree'</span>:            <span class="hljs-keyword">if</span> self.task==<span class="hljs-string">'Classification'</span>:                clf=lgb.LGBMClassifier(n_estimators=<span class="hljs-number">1</span>)            <span class="hljs-keyword">else</span>:                clf=lgb.LGBMRegressor(n_estimators=<span class="hljs-number">1</span>)            clf.fit(X.values.reshape(<span class="hljs-number">-1</span>,<span class="hljs-number">1</span>),y)                            <span class="hljs-keyword">if</span> self.method==<span class="hljs-string">'sklearn_tree'</span>:            <span class="hljs-keyword">if</span> self.task==<span class="hljs-string">'Classification'</span>:                clf=tree.DecisionTreeClassifier()            <span class="hljs-keyword">else</span>:                clf=tree.DecisionTreeRegressor()            clf.fit(X.values.reshape(<span class="hljs-number">-1</span>,<span class="hljs-number">1</span>),y)            tree_rules=clf.tree_.threshold            self.tree_rules=tree_rules[tree_rules!=<span class="hljs-number">-2</span>] <span class="hljs-comment">#sklearn 中的tree用-2判断叶节点，-2不是一个分裂阈值要删除</span>     <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transform</span><span class="hljs-params">(self, X)</span>:</span>        <span class="hljs-keyword">return</span> np.digitize(X,self.tree_rules)    tree = supervised_bins_Transformer(task=<span class="hljs-string">'Classification'</span>, method=<span class="hljs-string">'xgboost_tree'</span>)tree.fit(X,y)tree.transform(X)</code></pre><h2 id="特征斜度处理"><a href="#特征斜度处理" class="headerlink" title="特征斜度处理"></a>特征斜度处理</h2><p>使用skew判断斜度，然后Compute the Box-Cox transformation of boxcox_normmax</p><pre><code class="hljs python"><span class="hljs-comment">#修改斜度特征</span><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> skew, norm<span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> boxcox1p<span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> boxcox_normmax<span class="hljs-comment"># 选取数字特征</span>numeric_dtypes = [<span class="hljs-string">'int16'</span>, <span class="hljs-string">'int32'</span>, <span class="hljs-string">'int64'</span>, <span class="hljs-string">'float16'</span>, <span class="hljs-string">'float32'</span>, <span class="hljs-string">'float64'</span>]numeric = []<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X_cleaned.columns:    <span class="hljs-keyword">if</span> X_cleaned[i].dtype <span class="hljs-keyword">in</span> numeric_dtypes:        numeric.append(i)        <span class="hljs-comment"># 找到斜度特征，大于0.5的</span>skew_features = X_cleaned[numeric].apply(<span class="hljs-keyword">lambda</span> x: skew(x)).sort_values(ascending=<span class="hljs-literal">False</span>)high_skew = skew_features[skew_features &gt; <span class="hljs-number">0.5</span>]skew_index = high_skew.indexprint(<span class="hljs-string">"There are &#123;&#125; numerical features with Skew &gt; 0.5 :"</span>.format(high_skew.shape[<span class="hljs-number">0</span>]))skewness = pd.DataFrame(&#123;<span class="hljs-string">'Skew'</span> :high_skew&#125;)print(skew_features.head(<span class="hljs-number">10</span>))<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> skew_index:    X_cleaned[i] = boxcox1p(X_cleaned[i], boxcox_normmax(X_cleaned[i] + <span class="hljs-number">1</span>))</code></pre>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
