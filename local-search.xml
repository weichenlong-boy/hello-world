<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>XGBoost构造特征</title>
    <link href="/2020/06/03/undefined/"/>
    <url>/2020/06/03/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="XGBoost特征构造"><a href="#XGBoost特征构造" class="headerlink" title="XGBoost特征构造"></a>XGBoost特征构造</h2><p>本文利用XGBoost构造特征，返回是每棵树的叶子节点序列，构造的特征可以用于Logistic Regression和LibFFM，下面代码实现：</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> OneHotEncoder<span class="hljs-keyword">from</span> xgboost.sklearn <span class="hljs-keyword">import</span> XGBClassifier<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">XgboostFeature</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">'''</span><span class="hljs-string">    可以传入xgboost的参数</span><span class="hljs-string">    常用传入特征的个数 即树的个数 默认30</span><span class="hljs-string">    name表示构造之后的特征用于LR还是LibFFM</span><span class="hljs-string">    '''</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,name=<span class="hljs-string">'LR'</span>,n_estimators=<span class="hljs-number">30</span>,learning_rate =<span class="hljs-number">0.3</span>,max_depth=<span class="hljs-number">3</span>,min_child_weight=<span class="hljs-number">1</span>,gamma=<span class="hljs-number">0.3</span>,subsample=<span class="hljs-number">0.8</span>,colsample_bytree=<span class="hljs-number">0.8</span>,objective= <span class="hljs-string">'binary:logistic'</span>,nthread=<span class="hljs-number">4</span>,scale_pos_weight=<span class="hljs-number">1</span>,reg_alpha=<span class="hljs-number">1e-05</span>,reg_lambda=<span class="hljs-number">1</span>,seed=<span class="hljs-number">27</span>)</span>:</span>        self.n_estimators=n_estimators        self.learning_rate=learning_rate        self.max_depth=max_depth        self.min_child_weight=min_child_weight        self.gamma=gamma        self.subsample=subsample        self.colsample_bytree=colsample_bytree        self.objective=objective        self.nthread=nthread        self.scale_pos_weight=scale_pos_weight        self.reg_alpha=reg_alpha        self.reg_lambda=reg_lambda        self.seed=seed        self.name=name        <span class="hljs-keyword">print</span> (<span class="hljs-string">'Xgboost Feature start, new_feature number:'</span>,n_estimators)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LR</span><span class="hljs-params">(self,X,Y)</span>:</span>        onehot = OneHotEncoder()        onehot_X = onehot.fit_transform(X)        onehot_Y = onehot.fit_transform(Y)        <span class="hljs-keyword">return</span> onehot_X,onehot_Y    <span class="hljs-comment">##整体训练</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit_model</span><span class="hljs-params">(self,X_train,y_train,X_test)</span>:</span>        clf = XGBClassifier(             learning_rate =self.learning_rate,             n_estimators=self.n_estimators,             max_depth=self.max_depth,             min_child_weight=self.min_child_weight,             gamma=self.gamma,             subsample=self.subsample,             colsample_bytree=self.colsample_bytree,             objective= self.objective,             nthread=self.nthread,             scale_pos_weight=self.scale_pos_weight,             reg_alpha=self.reg_alpha,             reg_lambda=self.reg_lambda,             seed=self.seed)        clf.fit(X_train, y_train)        auc = cross_val_score(clf, X_train, y_train, scoring=<span class="hljs-string">'roc_auc'</span>, cv=<span class="hljs-number">10</span>)        <span class="hljs-keyword">print</span> (<span class="hljs-string">f'XGBoost交叉验证分数:<span class="hljs-subst">&#123;auc.mean()&#125;</span>'</span>)        new_feature= clf.apply(X_train)        new_feature_test= clf.apply(X_test)        <span class="hljs-keyword">if</span> self.name == <span class="hljs-string">'LR'</span>:            onehot_x,onehot_y = self.LR(new_feature,new_feature_test)            <span class="hljs-keyword">print</span> (<span class="hljs-string">"Training set sample number remains the same"</span>)            <span class="hljs-keyword">return</span> onehot_x,onehot_y        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">return</span> new_feature,new_feature_test</code></pre><p>调用示例，<code>name</code>为<code>&#39;LR&#39;</code>，即是onehot后的用于Logistic Regression</p><pre><code class="hljs python">xgb = XgboostFeature(name=<span class="hljs-string">'LR'</span>,n_estimators=<span class="hljs-number">100</span>)X_train_new,X_test_new = xgb.fit_model(train,label,test)</code></pre>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>XGBoost</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>XGBoost调参演示</title>
    <link href="/2020/06/02/undefined/"/>
    <url>/2020/06/02/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="XGBoost调参演示"><a href="#XGBoost调参演示" class="headerlink" title="XGBoost调参演示"></a>XGBoost调参演示</h2><p><a href="https://github.com/lytforgood/MachineLearningTrick" target="_blank" rel="noopener">本文参考</a></p><h4 id="导入模型库"><a href="#导入模型库" class="headerlink" title="导入模型库"></a>导入模型库</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_hastie_10_2<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> GradientBoostingClassifier<span class="hljs-keyword">from</span> xgboost.sklearn <span class="hljs-keyword">import</span> XGBClassifier<span class="hljs-comment">#载入示例数据 10维度</span>X, y = make_hastie_10_2(random_state=<span class="hljs-number">0</span>)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.5</span>, random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">##test_size测试集合所占比例</span></code></pre><h4 id="默认GBDT参数"><a href="#默认GBDT参数" class="headerlink" title="默认GBDT参数"></a>默认GBDT参数</h4><pre><code class="hljs python">clf = GradientBoostingClassifier()clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre))AUC Score : <span class="hljs-number">0.974248</span>Accuracy : <span class="hljs-number">0.8995</span></code></pre><h4 id="默认XGBoost参数"><a href="#默认XGBoost参数" class="headerlink" title="默认XGBoost参数"></a>默认XGBoost参数</h4><pre><code class="hljs python">auc_Score=[]accuracy=[]clf = XGBClassifier()clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre))auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))AUC Score : <span class="hljs-number">0.972424</span>Accuracy : <span class="hljs-number">0.8993</span></code></pre><h4 id="调整XGBoost参数"><a href="#调整XGBoost参数" class="headerlink" title="调整XGBoost参数"></a>调整XGBoost参数</h4><p>第一步：初始学习速率0.1和tree_based参数调优的估计器数目100。</p><p>给其他参数一个初始值。</p><ul><li>max_depth = 5 :默认6树的最大深度，这个参数的取值最好在3-10之间。</li><li>min_child_weight = 1:默认是1决定最小叶子节点样本权重和。如果是一个极不平衡的分类问题，某些叶子节点下的值会比较小，这个值取小点。</li><li>gamma = 0: 默认0，在0.1到0.2之间就可以。树的叶子节点上作进一步分裂所需的最小损失减少。这个参数后继也是要调整的。</li><li>subsample, colsample_bytree = 0.8: 样本采样、列采样。典型值的范围在0.5-0.9之间。</li><li>scale_pos_weight = 1:默认1,如果类别十分不平衡取较大正值。 </li></ul><pre><code class="hljs python">clf = XGBClassifier(         learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span>         n_estimators=<span class="hljs-number">100</span>, <span class="hljs-comment">#树的个数</span>         max_depth=<span class="hljs-number">5</span>,         min_child_weight=<span class="hljs-number">1</span>,         gamma=<span class="hljs-number">0</span>,         subsample=<span class="hljs-number">0.8</span>,         colsample_bytree=<span class="hljs-number">0.8</span>,         objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span>         nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span>         scale_pos_weight=<span class="hljs-number">1</span>,         seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))AUC Score : <span class="hljs-number">0.978546</span>Accuracy : <span class="hljs-number">0.9133</span></code></pre><p>‘n_estimators’:[100,200,500,1000,1500] ，取1000最好</p><pre><code class="hljs python">clf = XGBClassifier(         learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span>         n_estimators=<span class="hljs-number">1000</span>, <span class="hljs-comment">#树的个数</span>         max_depth=<span class="hljs-number">5</span>,         min_child_weight=<span class="hljs-number">1</span>,         gamma=<span class="hljs-number">0</span>,         subsample=<span class="hljs-number">0.8</span>,         colsample_bytree=<span class="hljs-number">0.8</span>,         objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span>         nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span>         scale_pos_weight=<span class="hljs-number">1</span>,         seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))AUC Score : <span class="hljs-number">0.992504</span>Accuracy : <span class="hljs-number">0.954</span></code></pre><p>第二步： max_depth 和 min_weight 它们对最终结果有很大的影响</p><p>max_depth range(3,10,2)=[3, 5, 7, 9]</p><p>min_weight range(1,6,2)=[1, 3, 5]</p><p>max_depth=3  min_weight=1 最好</p><pre><code class="hljs python">clf = XGBClassifier(         learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span>         n_estimators=<span class="hljs-number">1000</span>, <span class="hljs-comment">#树的个数</span>         max_depth=<span class="hljs-number">3</span>,         min_child_weight=<span class="hljs-number">1</span>,         gamma=<span class="hljs-number">0</span>,         subsample=<span class="hljs-number">0.8</span>,         colsample_bytree=<span class="hljs-number">0.8</span>,         objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span>         nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span>         scale_pos_weight=<span class="hljs-number">1</span>,         seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))</code></pre><pre><code>AUC Score : 0.991693Accuracy : 0.9485</code></pre><p>第三步：gamma参数调优</p><p>‘gamma’:[i/10.0 for i in range(0,7)]=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]</p><p>gamma=0.5 最好</p><pre><code class="hljs python">clf = XGBClassifier( learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span> n_estimators=<span class="hljs-number">1000</span>, <span class="hljs-comment">#树的个数</span> max_depth=<span class="hljs-number">3</span>, min_child_weight=<span class="hljs-number">1</span>, gamma=<span class="hljs-number">0.5</span>, subsample=<span class="hljs-number">0.8</span>, colsample_bytree=<span class="hljs-number">0.8</span>, objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span> nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span> scale_pos_weight=<span class="hljs-number">1</span>, seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))</code></pre><pre><code>AUC Score : 0.991749Accuracy : 0.9497</code></pre><p>第四步：调整subsample 和 colsample_bytree 参数</p><p> ‘subsample’:[i/10.0 for i in range(6,10)]=[0.6, 0.7, 0.8, 0.9]</p><p> ‘colsample_bytree’:[i/10.0 for i in range(6,10)]=[0.6, 0.7, 0.8, 0.9]</p><p> ‘subsample’: 0.6, ‘colsample_bytree’: 0.6 最好</p><pre><code class="hljs python">clf = XGBClassifier( learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span> n_estimators=<span class="hljs-number">1000</span>, <span class="hljs-comment">#树的个数</span> max_depth=<span class="hljs-number">3</span>, min_child_weight=<span class="hljs-number">1</span>, gamma=<span class="hljs-number">0.5</span>, subsample=<span class="hljs-number">0.6</span>, colsample_bytree=<span class="hljs-number">0.6</span>, objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span> nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span> scale_pos_weight=<span class="hljs-number">1</span>, seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))</code></pre><pre><code>AUC Score : 0.992504Accuracy : 0.954</code></pre><p>第五步：正则化参数调优</p><p>‘reg_alpha’:[1e-5, 1e-2, 0.1, 1, 100]=[1e-05, 0.01, 0.1, 1, 100] 默认0 L1正则项参数，参数值越大，模型越不容易过拟合</p><p>‘reg_lambda’:[1,5,10,50]        默认1L2正则项参数，参数值越大，模型越不容易过拟合</p><p>{‘reg_alpha’: 1e-05, ‘reg_lambda’: 1}  正则变化不大</p><pre><code class="hljs python">clf = XGBClassifier( learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span> n_estimators=<span class="hljs-number">1000</span>, <span class="hljs-comment">#树的个数</span> max_depth=<span class="hljs-number">3</span>, min_child_weight=<span class="hljs-number">1</span>, gamma=<span class="hljs-number">0.5</span>, subsample=<span class="hljs-number">0.6</span>, colsample_bytree=<span class="hljs-number">0.6</span>, objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span> nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span> scale_pos_weight=<span class="hljs-number">1</span>, reg_alpha=<span class="hljs-number">1e-05</span>, reg_lambda=<span class="hljs-number">1</span>, seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))</code></pre><pre><code>AUC Score : 0.992504Accuracy : 0.954</code></pre><p>第6步：进一步 降低学习速率 增加更多的树</p><p>‘learning_rate’:[0.01,0.1,0.3]</p><p>‘learning_rate’: 0.1 不变</p><p>‘n_estimators’:[1000,1200,1500,2000,2500]</p><p>‘n_estimators’: 2000 较好</p><pre><code class="hljs python">clf = XGBClassifier(         learning_rate =<span class="hljs-number">0.1</span>, <span class="hljs-comment">#默认0.3</span>         n_estimators=<span class="hljs-number">2000</span>, <span class="hljs-comment">#树的个数</span>         max_depth=<span class="hljs-number">3</span>,         min_child_weight=<span class="hljs-number">1</span>,         gamma=<span class="hljs-number">0.5</span>,         subsample=<span class="hljs-number">0.6</span>,         colsample_bytree=<span class="hljs-number">0.6</span>,         objective= <span class="hljs-string">'binary:logistic'</span>, <span class="hljs-comment">#逻辑回归损失函数</span>         nthread=<span class="hljs-number">4</span>,  <span class="hljs-comment">#cpu线程数</span>         scale_pos_weight=<span class="hljs-number">1</span>,         reg_alpha=<span class="hljs-number">1e-05</span>,         reg_lambda=<span class="hljs-number">1</span>,         seed=<span class="hljs-number">27</span>)  <span class="hljs-comment">#随机种子</span>clf.fit(X_train, y_train)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>] <span class="hljs-keyword">print</span> (<span class="hljs-string">"AUC Score : %f"</span> % metrics.roc_auc_score(y_test, y_pro))<span class="hljs-keyword">print</span> (<span class="hljs-string">"Accuracy : %.4g"</span> % metrics.accuracy_score(y_test, y_pre)) auc_Score.append(metrics.roc_auc_score(y_test, y_pro))accuracy.append(metrics.accuracy_score(y_test, y_pre))AUC Score : <span class="hljs-number">0.993114</span>Accuracy : <span class="hljs-number">0.957</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>XGBoost</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GBDT特征+LR</title>
    <link href="/2020/06/01/undefined/"/>
    <url>/2020/06/01/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="GBDT特征构造"><a href="#GBDT特征构造" class="headerlink" title="GBDT特征构造"></a>GBDT特征构造</h2><p><a href="https://flashgene.com/archives/71504.html" target="_blank" rel="noopener">本文参考</a></p><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>GBDT 是一种常用的非线性模型，基于集成学习中 boosting 的思想，由于GBDT本身可以发现多种有区分性的特征以及特征组合，决策树的路径可以直接作为 LR 输入特征使用，省去了人工寻找特征、特征组合的步骤。所以可以将 GBDT 的叶子结点输出，作为LR的输入，如图所示：</p><p><img src="/img/GBDT.png" srcset="/img/loading.gif" alt="GBDT"></p><p>就是先用已有特征训练GBDT模型，然后利用GBDT模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值0/1的，向量的每个元素对应于GBDT模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于GBDT模型里所有树包含的叶子结点数之和。图中的两棵树是GBDT学习到的，第一棵树有3个叶子结点，而第二棵树有2个叶子节点。对于一个输入样本点x，如果它在第一棵树最后落在其中的第二个叶子结点，而在第二棵树里最后落在其中的第一个叶子结点。那么通过GBDT获得的新特征向量为[0, 1, 0, 1, 0]，其中向量中的前三位对应第一棵树的3个叶子结点，后两位对应第二棵树的2个叶子结点。</p><p>这种通过 GBDT 生成LR特征的方式（GBDT+LR），业界已有实践（Facebook，Kaggle-2014），且效果不错，是非常值得尝试的思路。</p><h4 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h4><ul><li><strong>采用ensemble决策树而非单颗树</strong></li></ul><p>一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT 每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少颗树。按 paper 以及 Kaggle 竞赛中的 GBDT+LR 融合方式，多棵树正好满足 LR 每条训练样本可以通过 GBDT 映射成多个特征的需求。</p><ul><li><strong>采用 GBDT 而非 RF</strong></li></ul><p>RF 也是多棵树，但从效果上有实践证明不如 GBDT。且 GBDT 前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前 N 颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用 GBDT 的原因。</p><h4 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> GradientBoostingClassifier<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> OneHotEncoder<span class="hljs-comment"># 训练GBDT模型</span>gbdt = GradientBoostingClassifier(n_estimators=<span class="hljs-number">100</span>)  <span class="hljs-comment">#100颗树生成100个特征</span>gbdt.fit(data[col],data[<span class="hljs-string">'Label'</span>])<span class="hljs-comment"># 对GBDT预测结果进行onehot编码</span>onehot = OneHotEncoder()onehot1 = onehot.fit_transform(gbdt.apply(data[col])[:, :, <span class="hljs-number">0</span>])test_onehot = onehot.transform(gbdt.apply(test[col])[:, :, <span class="hljs-number">0</span>])<span class="hljs-comment"># 训练LR模型</span>lr = LogisticRegression()lr.fit(onehot1, Y_train)<span class="hljs-comment"># 测试集预测</span>Y_pred = lr.predict_proba(test_onehot)[:, <span class="hljs-number">1</span>]</code></pre><p>原文论文谈到GBDT的参数，树的数量最多500颗（500以上就没有提升了），每棵树的节点不多于12。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>贝叶斯优化调参</title>
    <link href="/2020/05/31/undefined/"/>
    <url>/2020/05/31/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="Hyperopt-Bayesian-optimization"><a href="#Hyperopt-Bayesian-optimization" class="headerlink" title="Hyperopt Bayesian optimization"></a>Hyperopt Bayesian optimization</h2><h4 id="安装（Hyperopt-Distributed-Asynchronous-Hyper-parameter-Optimization）"><a href="#安装（Hyperopt-Distributed-Asynchronous-Hyper-parameter-Optimization）" class="headerlink" title="安装（Hyperopt: Distributed Asynchronous Hyper-parameter Optimization）"></a>安装（<a href="http://hyperopt.github.io/hyperopt/#hyperopt-distributed-asynchronous-hyper-parameter-optimization" target="_blank" rel="noopener">Hyperopt: Distributed Asynchronous Hyper-parameter Optimization</a>）</h4><pre><code class="hljs python">pip install hyperopt</code></pre><h4 id="Hyperopt步骤"><a href="#Hyperopt步骤" class="headerlink" title="Hyperopt步骤"></a>Hyperopt步骤</h4><ul><li>the objective function to minimize（定义最小化函数）</li><li>the space over which to search（搜索空间）</li><li>the database to store all point evaluations of the search（存储搜索的所有点计算的数据库）</li><li>the search algorithm to use（使用的搜索算法）</li></ul><h4 id="定义搜索空间"><a href="#定义搜索空间" class="headerlink" title="定义搜索空间"></a>定义搜索空间</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> hyperopt <span class="hljs-keyword">import</span> hpspace = hp.choice(<span class="hljs-string">'a'</span>,    [        (<span class="hljs-string">'case 1'</span>, <span class="hljs-number">1</span> + hp.lognormal(<span class="hljs-string">'c1'</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)),        (<span class="hljs-string">'case 2'</span>, hp.uniform(<span class="hljs-string">'c2'</span>, <span class="hljs-number">-10</span>, <span class="hljs-number">10</span>))    ])</code></pre><p>所描述的搜索空间<code>space</code>具有3个参数：</p><ul><li><code>a</code>-选择大小写</li><li><code>c1</code>-在情况1中使用的正值参数</li><li><code>c2</code>-在情况2中使用的有界实值参数</li></ul><h5 id="常用参数表达式"><a href="#常用参数表达式" class="headerlink" title="常用参数表达式"></a>常用参数表达式</h5><p><strong>hp.choice(label, options)</strong></p><p>返回选项之一，options应该是列表或元组。元素<code>options</code>本身可以是[嵌套]随机表达。在这种情况下，仅出现在某些选项中的随机选择成为<em>条件</em>参数。</p><p><strong>hp.randint(label, upper)</strong></p><p>返回范围为[0，upper）的随机整数。这种分布的语义是，与更远的整数值相比，附近的整数值之间的损失函数<em>不再</em>具有相关性。例如，这是用于描述随机种子的适当分布。如果损失函数可能更多附近的整数值相关联，那么你或许应该用“量化”连续分布的一个，比如要么<code>quniform</code>，<code>qloguniform</code>，<code>qnormal</code>或<code>qlognormal</code>。</p><p><strong>hp.uniform(label, low, high)</strong></p><ul><li>在<code>low</code>和<code>high</code>之间返回一个值。</li><li>优化时，此变量被限制为两侧间隔。</li></ul><p><strong>hp.normal(label, mu, sigma)</strong></p><p>返回一个正态分布的实数值，平均值为<code>mu</code>，标准偏差为<code>sigma</code>。优化时，这是一个不受约束的变量。</p><h4 id="调参逻辑回归示例"><a href="#调参逻辑回归示例" class="headerlink" title="调参逻辑回归示例"></a>调参逻辑回归示例</h4><pre><code class="hljs python"><span class="hljs-comment">#hyperopt调参</span><span class="hljs-keyword">from</span> hyperopt <span class="hljs-keyword">import</span> fmin, tpe, hp, partial, Trials<span class="hljs-comment"># 自定义hyperopt的参数空间</span>space = &#123;<span class="hljs-string">'max_iter'</span>: hp.choice(<span class="hljs-string">"max_iter"</span>, range(<span class="hljs-number">1</span>,<span class="hljs-number">1000</span>)),         <span class="hljs-string">'C'</span>: hp.uniform(<span class="hljs-string">"C"</span>, <span class="hljs-number">0.001</span>,<span class="hljs-number">1000</span>),         <span class="hljs-string">'tol'</span>:hp.uniform(<span class="hljs-string">'tol'</span>, <span class="hljs-number">0.00001</span>,<span class="hljs-number">0.1</span>)         &#125;<span class="hljs-comment">#这里使用的sklearn的cross_val_score交叉验证调参，但与实际交叉验证结果会有出入</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forest1</span><span class="hljs-params">(params)</span>:</span>    logis = LogisticRegression(max_iter=params[<span class="hljs-string">"max_iter"</span>],C=params[<span class="hljs-string">"C"</span>],tol=params[<span class="hljs-string">"tol"</span>])    accuracy = cross_val_score(logis, X_cleaned, data[Y_], scoring=<span class="hljs-string">'roc_auc'</span>, cv=<span class="hljs-number">10</span>)    <span class="hljs-keyword">return</span> -accuracy.max()<span class="hljs-comment"># 开始使用hyperopt进行自动调参</span>best = fmin(forest1, space, algo=tpe.suggest,trials=Trials(), max_evals=<span class="hljs-number">50</span>)</code></pre><h4 id="调参XGBoost示例"><a href="#调参XGBoost示例" class="headerlink" title="调参XGBoost示例"></a>调参XGBoost示例</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> hyperopt <span class="hljs-keyword">import</span> fmin, tpe, hp, partial, Trials<span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb<span class="hljs-keyword">from</span> numpy.random <span class="hljs-keyword">import</span> RandomStatedtrain = xgb.DMatrix(data=x_train,label=y_train)dtest = xgb.DMatrix(data=x_test,label=y_test)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hyperopt_objective</span><span class="hljs-params">(params)</span>:</span>    model = xgb.XGBRegressor(        max_depth=int(params[<span class="hljs-string">'max_depth'</span>])+<span class="hljs-number">5</span>,        learning_rate=params[<span class="hljs-string">'learning_rate'</span>],        silent=<span class="hljs-number">1</span>,        objective=<span class="hljs-string">'reg:linear'</span>,        eval_metric=<span class="hljs-string">'rmse'</span>,        seed=<span class="hljs-number">42</span>,        nthread=<span class="hljs-number">-1</span>,    )    res = xgb.cv(model.get_params(), dtrain, num_boost_round=<span class="hljs-number">10</span>, nfold=<span class="hljs-number">5</span>,             callbacks=[xgb.callback.print_evaluation(show_stdv=<span class="hljs-literal">False</span>),                        xgb.callback.early_stop(<span class="hljs-number">3</span>)])    <span class="hljs-keyword">return</span> np.min(res[<span class="hljs-string">'test-rmse-mean'</span>]) <span class="hljs-comment"># as hyperopt minimises</span>params_space = &#123;    <span class="hljs-string">'max_depth'</span>: hp.randint(<span class="hljs-string">'max_depth'</span>, <span class="hljs-number">6</span>),    <span class="hljs-string">'learning_rate'</span>: hp.uniform(<span class="hljs-string">'learning_rate'</span>, <span class="hljs-number">1e-3</span>, <span class="hljs-number">5e-1</span>),&#125;trials = hyperopt.Trials()best = fmin(    hyperopt_objective,    space=params_space,    algo=tpe.suggest,    max_evals=<span class="hljs-number">50</span>,    trials=trials,    rstate=RandomState(<span class="hljs-number">123</span>))print(<span class="hljs-string">"\n展示hyperopt获取的最佳结果，但是要注意的是我们对hyperopt最初的取值范围做过一次转换"</span>)print(best)</code></pre><h4 id="调参LightGBM示例"><a href="#调参LightGBM示例" class="headerlink" title="调参LightGBM示例"></a>调参LightGBM示例</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> hyperopt <span class="hljs-keyword">import</span> fmin, tpe, hp, partial, Trials<span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb<span class="hljs-keyword">from</span> numpy.random <span class="hljs-keyword">import</span> RandomStatedtrain = xgb.DMatrix(data=x_train,label=y_train)dtest = xgb.DMatrix(data=x_test,label=y_test)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hyperopt_objective</span><span class="hljs-params">(params)</span>:</span>    model = lgb.LGBMRegressor(        num_leaves = <span class="hljs-number">31</span>,        max_depth=int(params[<span class="hljs-string">'max_depth'</span>]),        learning_rate=params[<span class="hljs-string">'learning_rate'</span>],        objective=<span class="hljs-string">'regressor'</span>,        eval_metric=<span class="hljs-string">'rmse'</span>,        seed=<span class="hljs-number">42</span>,        nthread=<span class="hljs-number">-1</span>,    )    res = lgb.cv(model.get_params(), dtrain, num_boost_round=<span class="hljs-number">10</span>, nfold=<span class="hljs-number">5</span>,             callbacks=[xgb.callback.print_evaluation(show_stdv=<span class="hljs-literal">False</span>),                        xgb.callback.early_stop(<span class="hljs-number">3</span>)])    <span class="hljs-keyword">return</span> np.min(res[<span class="hljs-string">'test-rmse-mean'</span>]) <span class="hljs-comment"># as hyperopt minimises</span>params_space = &#123;    <span class="hljs-string">'max_depth'</span>: hp.randint(<span class="hljs-string">'max_depth'</span>, <span class="hljs-number">6</span>),    <span class="hljs-string">'learning_rate'</span>: hp.uniform(<span class="hljs-string">'learning_rate'</span>, <span class="hljs-number">1e-3</span>, <span class="hljs-number">5e-1</span>),&#125;trials = hyperopt.Trials()best = fmin(    hyperopt_objective,    space=params_space,    algo=tpe.suggest,    max_evals=<span class="hljs-number">50</span>,    trials=trials,    rstate=RandomState(<span class="hljs-number">123</span>))print(<span class="hljs-string">"\n展示hyperopt获取的最佳结果，但是要注意的是我们对hyperopt最初的取值范围做过一次转换"</span>)print(best)</code></pre>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动特征挖掘</title>
    <link href="/2020/05/30/undefined/"/>
    <url>/2020/05/30/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="Symbolic-Learning"><a href="#Symbolic-Learning" class="headerlink" title="Symbolic Learning"></a>Symbolic Learning</h2><h4 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h4><p>在讲自动化挖掘特征之前，先来了解一下传统的<strong>遗传算法</strong></p><p>遗传算法（Genetic Algorithm, GA）起源于对生物系统所进行的计算机模拟研究。它是模仿自然界生物进化机制发展起来的随机全局搜索和优化方法，借鉴了达尔文的进化论和孟德尔的遗传学说。其本质是一种高效、并行、全局搜索的方法，能在搜索过程中自动获取和积累有关搜索空间的知识，并自适应地控制搜索过程以求得最佳解。</p><p>其主要特点是直接对结构对象进行操作，不存在求导和函数连续性的限定；具有内在的隐并行性和更好的全局寻优能力；采用概率化的寻优方法，能自动获取和指导优化的搜索空间，自适应地调整搜索方向，不需要确定的规则。遗传算法的这些性质，已被人们广泛地应用于组合优化、机器学习、信号处理、自适应控制和人工生命等领域。</p><table><thead><tr><th align="center">生物遗传概念</th><th align="center">遗传算法中的作用</th></tr></thead><tbody><tr><td align="center">适者生存</td><td align="center">在算法停止时，最优目标值的解有最大的可能被保留</td></tr><tr><td align="center">个体（individual）</td><td align="center">解</td></tr><tr><td align="center">染色体（chromosome）</td><td align="center">解的编码（字符串，向量等）</td></tr><tr><td align="center">基因（gene）</td><td align="center">解中每一分量的特征（如各分量的值）</td></tr><tr><td align="center">适应度（fitness）</td><td align="center">适应函数值</td></tr><tr><td align="center">种群（population）</td><td align="center">根据适应函数值选取的一组解</td></tr><tr><td align="center">交叉（crossover）</td><td align="center">通过交配原则产出一组新解的过程</td></tr><tr><td align="center">变异（mutation）</td><td align="center">编码的某一个分量发生变化的过程</td></tr></tbody></table><p><strong>遗传算法流程：</strong></p><p>一次迭代包括以下几个过程：</p><ol><li>染色体变异。即改变某个染色体的值；适应度越优的个体染色体变化范围越小，通过一个随机数让染色体值变化</li><li>染色体交叉。任意选择两个染色体交换部分基因；随机选择一对节点，相互交换对应的值即可</li><li>计算适应度。计算每个染色体在当前迭代下对应的适应度</li><li>优胜劣汰。杀死排名最后的那个个体。杀死之后种群数量就变少了，所以就必须要让比较优良的个体多生点来把种群数量补回来</li></ol><h4 id="Symbolic-Transformer（SymbolicTransformer）"><a href="#Symbolic-Transformer（SymbolicTransformer）" class="headerlink" title="Symbolic Transformer（SymbolicTransformer）"></a>Symbolic Transformer（<a href="https://gplearn.readthedocs.io/en/stable/reference.html#gplearn.genetic.SymbolicTransformer" target="_blank" rel="noopener"><code>SymbolicTransformer</code></a>）</h4><p>遗传程序符号转换器是一种受监督的转换器，它首先构建一组简单的随机公式来表示关系。这些公式以树状结构表示，数学函数递归地应用于变量和常量。然后，每一代连续的程序都是通过从人群中选择最适合的个体进行遗传操作，如交叉、突变或繁殖，从之前的程序进化而来。搜索最终种群，寻找彼此相关性最小的个体。</p><p>下例演示了如何使用Symbolic Transformer自动生成新的非线性特性。</p><pre><code class="hljs python"><span class="hljs-comment">#生成数据</span>rng = check_random_state(<span class="hljs-number">0</span>)  <span class="hljs-comment">#随机种子</span>boston = load_boston()perm = rng.permutation(boston.target.size)boston.data = boston.data[perm]boston.target = boston.target[perm]</code></pre><p>在这个例子中，我们将使用岭回归，并在前300个样本上训练我们的回归器，然后看看它如何在不可见的最后200个样本上执行。要击败的基准是简单的岭回归运行在数据集作为:</p><pre><code class="hljs python">est = Ridge()est.fit(boston.data[:<span class="hljs-number">300</span>, :], boston.target[:<span class="hljs-number">300</span>])print(est.score(boston.data[<span class="hljs-number">300</span>:, :], boston.target[<span class="hljs-number">300</span>:]))<span class="hljs-number">0.759145222183</span></code></pre><p>因此，现在我们将在相同的前300个示例上transformer，以生成一些新特征。让我们以20代人的2000个体为例。我们将选择其中最好的100个作为hall_of_fame，然后使用相关性最小的10个作为我们的新特性。max_samples=0.9可以控制膨胀，但是我们将其余的演变选项保留为默认值。 此处默认的metric =’pearson’是合适的，因为我们使用线性模型作为估计量。 如果我们要使用基于树的估计器，那么也可以尝试使用Spearman相关性：</p><pre><code class="hljs python">function_set = [<span class="hljs-string">'add'</span>, <span class="hljs-string">'sub'</span>, <span class="hljs-string">'mul'</span>, <span class="hljs-string">'div'</span>,                <span class="hljs-string">'sqrt'</span>, <span class="hljs-string">'log'</span>, <span class="hljs-string">'abs'</span>, <span class="hljs-string">'neg'</span>, <span class="hljs-string">'inv'</span>,                <span class="hljs-string">'max'</span>, <span class="hljs-string">'min'</span>]   <span class="hljs-comment">#交叉、突变或繁殖方式</span>gp = SymbolicTransformer(generations=<span class="hljs-number">20</span>, population_size=<span class="hljs-number">2000</span>,                         hall_of_fame=<span class="hljs-number">100</span>, n_components=<span class="hljs-number">10</span>,                         function_set=function_set,                         parsimony_coefficient=<span class="hljs-number">0.0005</span>,                         max_samples=<span class="hljs-number">0.9</span>, verbose=<span class="hljs-number">1</span>,                         random_state=<span class="hljs-number">0</span>, n_jobs=<span class="hljs-number">3</span>)gp.fit(boston.data[:<span class="hljs-number">300</span>, :], boston.target[:<span class="hljs-number">300</span>])</code></pre><p>然后，我们将把经过训练的transformer应用到整个Boston数据集(请记住，它仍然没有看到最后的200个样本)，并将其连接到原始数据:</p><pre><code class="hljs python">gp_features = gp.transform(boston.data)new_boston = np.hstack((boston.data, gp_features))  <span class="hljs-comment">#连接特征和原始数据</span></code></pre><p>现在，我们在转换后的数据集的前300个样本上训练岭回归器，看看它在最后200个样本上的表现如何:</p><pre><code class="hljs python">est = Ridge()est.fit(new_boston[:<span class="hljs-number">300</span>, :], boston.target[:<span class="hljs-number">300</span>])print(est.score(new_boston[<span class="hljs-number">300</span>:, :], boston.target[<span class="hljs-number">300</span>:]))<span class="hljs-number">0.841750404385</span></code></pre><p>可以看出效果还是比较明显的</p>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>特征降维</title>
    <link href="/2020/05/29/undefined/"/>
    <url>/2020/05/29/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="特征降维"><a href="#特征降维" class="headerlink" title="特征降维"></a>特征降维</h2><h4 id="为什么要降维"><a href="#为什么要降维" class="headerlink" title="为什么要降维"></a>为什么要降维</h4><ul><li>找到宏观信息</li><li>找到交叉效应</li><li>减少，防止过拟合</li><li>不建议先降维后拟合模型(丢失部分特征信息)</li></ul><h4 id="常用的几种降维方法"><a href="#常用的几种降维方法" class="headerlink" title="常用的几种降维方法"></a>常用的几种降维方法</h4><h5 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h5><p>API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#" target="_blank" rel="noopener"><code>sklearn.decomposition</code>.PCA</a></p><p>设有 m 条 n 维数据</p><ul><li>将原始数据按列组成 n 行 m 列矩阵 X</li><li>将 X 的每一行进行零均值化，即减去这一行的均值(标准化或者归一化，消除量纲不一致问题)</li><li>求出协方差矩阵 </li><li>求出协方差矩阵的特征值及对应的特征向量</li><li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P</li><li>Y = PX 即为降维到 k 维后的数据</li></ul><p>然而实际上，当样本维度很高时，协方差矩阵计算太慢，方针特征值分解计算效率不高，所以PCA经常是使用SVD(奇异值分解)进行求解，具体可看API里面的参数<strong>svd_solver</strong></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCAX = np.array([[<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>], [<span class="hljs-number">-2</span>, <span class="hljs-number">-1</span>], [<span class="hljs-number">-3</span>, <span class="hljs-number">-2</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">2</span>]])pca = PCA(n_components=<span class="hljs-number">2</span>)   <span class="hljs-comment">#隐变量个数，可变参数</span>pca.fit(X)result = pca.transform(X)</code></pre><h5 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h5><p>上面已经解释过。API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html?highlight=svd#" target="_blank" rel="noopener"><code>sklearn.decomposition</code>.TruncatedSVD</a></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> TruncatedSVDsvd = TruncatedSVD(n_components=<span class="hljs-number">5</span>, n_iter=<span class="hljs-number">7</span>, random_state=<span class="hljs-number">42</span>)  <span class="hljs-comment">#可变参数</span>svd.fit(X)result = svd.transform(X)</code></pre><p>对于密集数据使用PCA，对于稀疏数据使用TruncatedSVD</p><h5 id="TSNE"><a href="#TSNE" class="headerlink" title="TSNE"></a>TSNE</h5><p>t分布随机邻接嵌入。API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html?highlight=tsne#" target="_blank" rel="noopener"><code>sklearn.manifold</code>.TSNE</a></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNEX_embedded = TSNE(n_components=<span class="hljs-number">2</span>,perplexity=<span class="hljs-number">30.0</span>).fit_transform(X)<span class="hljs-comment">#tSNE中重要的参数是n_components和perplexity</span></code></pre><h5 id="NMF"><a href="#NMF" class="headerlink" title="NMF"></a>NMF</h5><p>找到两个非负矩阵(W, H)，它们的乘积近似于非负矩阵x。这种分解可以用于降维、源分离或主题提取。</p><p>API <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#" target="_blank" rel="noopener"><code>sklearn.decomposition</code>.NMF</a></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> NMFmodel = NMF(n_components=<span class="hljs-number">2</span>, init=<span class="hljs-string">'random'</span>, random_state=<span class="hljs-number">0</span>)W = model.fit_transform(X)</code></pre><p>以上是常用的几种降维方法，还有其他的方法请读者自己学习。</p>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动特征编码</title>
    <link href="/2020/05/28/undefined/"/>
    <url>/2020/05/28/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="类别编码—Category-Encoders"><a href="#类别编码—Category-Encoders" class="headerlink" title="类别编码—Category Encoders"></a>类别编码—Category Encoders</h2><p>一组scikit-learn-style转换器，用于使用不同的技术将分类变量编码为数字。 在现有的scikit-learn版本中，序数编码，一键编码和哈希编码具有相似的等效项，但该库中的转换器都共享一些有用的属性：</p><ul><li>对pandas dataframes的一流支持，可作为输入（也可以作为输出）</li><li>可以显式配置按名称或索引对数据中的哪些列进行编码，或者不管输入类型如何，推断非数字列</li><li>可以基于训练集随意删除方差非常低的任何列</li><li>可移植性:对转换器进行数据培训，对其进行pickle，然后重用它，最后得到相同的结果</li><li>与sklearn完全兼容，像其他任何转换器一样输入类似数组的数据集</li></ul><p>安装：</p><pre><code class="hljs plain">pip install category_encoders</code></pre><p>导入包：</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> category_encoders <span class="hljs-keyword">as</span> ce</code></pre><p>需要注意的参数：handle_unknown和handle_missing</p><p>在目标编码中，handle_unknown 和 handle_missing 仅接受 ‘error’, ‘return_nan’ 及 ‘value’ 设定<br>两者的默认值均为 ‘value’, 即对未知类别或缺失值填充训练集的因变量平均值</p><h4 id="Label-Encoder-Ordinal"><a href="#Label-Encoder-Ordinal" class="headerlink" title="Label Encoder (Ordinal)"></a>Label Encoder (<a href="http://contrib.scikit-learn.org/category_encoders/ordinal.html" target="_blank" rel="noopener">Ordinal</a>)</h4><pre><code class="hljs python">encoder = ce.OrdinalEncoder(feature_list,handle_unknown=<span class="hljs-string">'value'</span>,handle_missing=<span class="hljs-string">'value'</span>) train_le = encoder.fit_transform(train)  <span class="hljs-comment">#训练数据,fit，然后对其进行转换</span>test_le = encoder.transform(test)    <span class="hljs-comment">#编码到test数据</span></code></pre><h4 id="One-Hot-Encoder-One-Hot"><a href="#One-Hot-Encoder-One-Hot" class="headerlink" title="One-Hot Encoder(One Hot)"></a>One-Hot Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/onehot.html" target="_blank" rel="noopener">One Hot</a>)</h4><p>一个编码分类特征的编码，每个分类产生一个特征，每个二进制。</p><pre><code class="hljs python">OHE_encoder = ce.OneHotEncoder(feature_list)train_ohe = OHE_encoder.fit_transform(train)test_ohe = OHE_encoder.transform(test)</code></pre><h4 id="Target-Encoder-Target-Encoder"><a href="#Target-Encoder-Target-Encoder" class="headerlink" title="Target Encoder(Target Encoder)"></a>Target Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/targetencoder.html" target="_blank" rel="noopener">Target Encoder</a>)</h4><pre><code class="hljs python"><span class="hljs-comment">#筛选类别小于60的特征</span>feature_list = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> data.columns <span class="hljs-keyword">if</span> len(set(data[i])) &lt; <span class="hljs-number">60</span> <span class="hljs-keyword">and</span> i != label]TE_encoder = ce.TargetEncoder(feature_list)train_te = TE_encoder.fit_transform(train[feature_list], target)test_te = TE_encoder.transform(test[feature_list])</code></pre><h4 id="Weight-of-Evidence-Encoder-Weight-of-Evidence"><a href="#Weight-of-Evidence-Encoder-Weight-of-Evidence" class="headerlink" title="Weight of Evidence Encoder(Weight of Evidence)"></a>Weight of Evidence Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/woe.html" target="_blank" rel="noopener">Weight of Evidence</a>)</h4><p>WOE是信用评分中常用的基于目标的编码器，WOE计算公式：</p><p><img src="/img/woe.png" srcset="/img/loading.gif" alt="woe"></p><p>其中Event%是每个类别中正样本占总样本比例。</p><pre><code class="hljs python">WOE_encoder = ce.WOEEncoder()train_woe = WOE_encoder.fit_transform(train[feature_list], target)test_woe = WOE_encoder.transform(test[feature_list])</code></pre><h4 id="James-Stein-Encoder-James-Stein-Encoder"><a href="#James-Stein-Encoder-James-Stein-Encoder" class="headerlink" title="James-Stein Encoder(James-Stein Encoder)"></a>James-Stein Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/jamesstein.html" target="_blank" rel="noopener">James-Stein Encoder</a>)</h4><p>James-Stein编码也是一种基于目标编码的编码方法。与M估计量编码一样，James-Stein编码器也尝试通过参数B来平衡先验概率与观测到的条件概率。但与目标编码与M估计量编码不同的是，James-Stein编码器通过方差比而不是样本大小来平衡两个概率。</p><pre><code class="hljs python">JSE_encoder = ce.JamesSteinEncoder()train_jse = JSE_encoder.fit_transform(train[feature_list], target)test_jse = JSE_encoder.transform(test[feature_list])</code></pre><h4 id="Leave-one-out-Encoder-Leave-One-Out"><a href="#Leave-one-out-Encoder-Leave-One-Out" class="headerlink" title="Leave-one-out Encoder(Leave One Out)"></a>Leave-one-out Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/leaveoneout.html" target="_blank" rel="noopener">Leave One Out</a>)</h4><p>留一法编码器通过组因变量均值对每个组进行编码，此处组指的是类别变量中的不同类别。</p><pre><code class="hljs python">LOOE_encoder = ce.LeaveOneOutEncoder()train_looe = LOOE_encoder.fit_transform(train[feature_list], target)test_looe = LOOE_encoder.transform(test[feature_list])</code></pre><h4 id="Catboost-Encoder-CatBoost-Encoder"><a href="#Catboost-Encoder-CatBoost-Encoder" class="headerlink" title="Catboost Encoder(CatBoost Encoder)"></a>Catboost Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/catboost.html" target="_blank" rel="noopener">CatBoost Encoder</a>)</h4><pre><code class="hljs python">CBE_encoder = ce.CatBoostEncoder()train_cbe = CBE_encoder.fit_transform(train[feature_list], target)test_cbe = CBE_encoder.transform(test[feature_list])</code></pre><h4 id="Helmert-Encoder-Helmert-Coding"><a href="#Helmert-Encoder-Helmert-Coding" class="headerlink" title="Helmert Encoder(Helmert Coding)"></a>Helmert Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/helmert.html" target="_blank" rel="noopener">Helmert Coding</a>)</h4><pre><code class="hljs python">HE_encoder = ce.HelmertEncoder(feature_list)train_he = HE_encoder.fit_transform(train[feature_list], target)test_he = HE_encoder.transform(test[feature_list])</code></pre><h4 id="Polynomial-Encoder-Polynomial-Coding"><a href="#Polynomial-Encoder-Polynomial-Coding" class="headerlink" title="Polynomial Encoder(Polynomial Coding)"></a>Polynomial Encoder(<a href="http://contrib.scikit-learn.org/category_encoders/polynomial.html" target="_blank" rel="noopener">Polynomial Coding</a>)</h4><pre><code class="hljs python">PE_encoder = ce.PolynomialEncoder(feature_list)train_pe = PE_encoder.fit_transform(train[feature_list], target)test_pe = PE_encoder.transform(test[feature_list])</code></pre><p>以上编码在实战中都可以尝试，选择或者全部使用。</p>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pdpipe库的使用和实战</title>
    <link href="/2020/05/27/undefined/"/>
    <url>/2020/05/27/undefined/</url>
    
    <content type="html"><![CDATA[<h3 id="Pdpipe初识"><a href="#Pdpipe初识" class="headerlink" title="Pdpipe初识"></a>Pdpipe初识</h3><p>“在<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/#pdpipe" target="_blank" rel="noopener"><code>pdpipe</code></a>Python包为构建简洁的界面<code>pandas</code> 是有先决条件，是冗长的管线，支持scikit学习变压器的装修改造设计，具有很强的可序列化。<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/#pdpipe" target="_blank" rel="noopener"><code>pdpipe</code></a> 管道具有简单的界面，提供有关管道应用程序的信息和错误信息，支持管道算术，并使混合类型数据的处理更加容易。”   这是官方文档的解释，我个人认为pdpipe在处理生成特征变量方面确实很方便。</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>win10的Anaconda(推荐)环境下就可以。打开命令行输入：</p><pre><code class="hljs python">pip install pdpipe</code></pre><p>pdpipe的一些模块需要scikit-learn和nltk库的支持，如果没有，pdpipe则会警告，提示你需要安装。</p><h4 id="实战学习"><a href="#实战学习" class="headerlink" title="实战学习"></a>实战学习</h4><p>我们的数据集是汽车行驶信息，如下图：</p><p><img src="/img/Snipaste_2020-05-27_19-05-27.png" srcset="/img/loading.gif" alt="Snipaste_2020-05-27_19-05-27"></p><h5 id="读取代码："><a href="#读取代码：" class="headerlink" title="读取代码："></a>读取代码：</h5><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">'./data.csv'</span>,nrows=<span class="hljs-number">100000</span>,encoding=<span class="hljs-string">'gbk'</span>)  <span class="hljs-comment">#读取数据</span>df.columns = [<span class="hljs-string">'字段'</span>+str(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">14</span>)]   <span class="hljs-comment">#设置列名</span>df = df.drop([<span class="hljs-string">'字段5'</span>,<span class="hljs-string">'字段12'</span>], axis=<span class="hljs-number">1</span>)  <span class="hljs-comment">#删除无用变量</span></code></pre><p>这里需要说的一点是read_csv里的nrows参数，含义是指定读取文件的前n行，我这里n取10w，减少计算时间。</p><h5 id="数据描述："><a href="#数据描述：" class="headerlink" title="数据描述："></a>数据描述：</h5><p>字段1为id，字段3为车牌，字段4为车牌颜色，日期为时间戳，字段11和字段13为路口编码。</p><h4 id="一、清理数据"><a href="#一、清理数据" class="headerlink" title="一、清理数据"></a>一、清理数据</h4><p>使用pdpipe下的<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/basic_stages.html" target="_blank" rel="noopener">basic_stages</a>模块，该模块主要是做数据清洗。</p><ol><li>删除掉字段2为99、16、23、31、32的行数据</li><li>删除字段3不是车牌的行数据</li><li>删除掉字段3只出现过一次的行数据</li><li>将日期转化为时间戳</li></ol><p><img src="/img/carbon.png" srcset="/img/loading.gif" alt="carbon"></p><p>其中调用的函数：</p><p><img src="/img/match.png" srcset="/img/loading.gif" alt="match"></p><p>这里使用正则来配备车牌，调用re下的group(0)属性返回配备值，使用异常处理来避免不是车牌的情况下配备值不存在。</p><h4 id="二、处理特征"><a href="#二、处理特征" class="headerlink" title="二、处理特征"></a>二、处理特征</h4><p>使用pdpipe下的<a href="https://pdpipe.github.io/pdpipe/doc/pdpipe/col_generation.html" target="_blank" rel="noopener">col_generation</a>模块，该模块使用函数来生成新的变量。</p><p>将剩下的数据按照字段3相同（同一车牌）的时间顺序排列。使用groupby函数按照字段3聚合，然后对每个group排序。</p><pre><code class="hljs python">data1 = data.groupby(<span class="hljs-string">'字段3'</span>).apply(<span class="hljs-keyword">lambda</span> x: x.sort_values(<span class="hljs-string">'日期'</span>))data1.index = data.index</code></pre><p>然后开始处理：</p><ol><li>前后相邻两行记录时间差小于30min，则删除靠后一条的行数据</li><li>前后相邻两行记录的字段13相同，则删除靠前的行数据</li><li>若最后同一车牌下只剩一个行数据，则删除</li></ol><p><img src="/img/2.png" srcset="/img/loading.gif" alt="2"></p><p>函数模块：</p><p><img src="/img/1.png" srcset="/img/loading.gif" alt="1"></p><h4 id="三、小知识点"><a href="#三、小知识点" class="headerlink" title="三、小知识点"></a>三、小知识点</h4><p>groupby函数下，可以对每个group操作，group可以和dataframe一样处理。</p><pre><code class="hljs python">groupby(<span class="hljs-string">'字段3'</span>).apply(<span class="hljs-keyword">lambda</span> j: j[<span class="hljs-string">'字段13'</span>].iloc[<span class="hljs-number">0</span>])   <span class="hljs-comment">#取出每个group的第一行</span></code></pre><pre><code class="hljs python"><span class="hljs-comment">#对每个group应用函数，还可以实时操作</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f4</span><span class="hljs-params">(x)</span>:</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x)):        x[<span class="hljs-string">'OD'</span>].iloc[i] = x[<span class="hljs-string">'字段11'</span>].iloc[i][i]    <span class="hljs-keyword">return</span> xx.groupby(<span class="hljs-string">'字段3'</span>).apply(<span class="hljs-keyword">lambda</span> j: f4(j))</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>python练习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 10个习惯用法</title>
    <link href="/2020/05/24/undefined/"/>
    <url>/2020/05/24/undefined/</url>
    
    <content type="html"><![CDATA[<h3 id="1、-if-not-x"><a href="#1、-if-not-x" class="headerlink" title="1、 if not x"></a>1、 if not x</h3><p>直接使用 x 和 not x 判断 x 是否为 None 或空</p><pre><code class="hljs python">x = [<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">5</span>]<span class="hljs-keyword">if</span> x:    print(<span class="hljs-string">'x is not empty '</span>)<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> x:    print(<span class="hljs-string">'x is empty'</span>)</code></pre><h3 id="2、enumerate-枚举"><a href="#2、enumerate-枚举" class="headerlink" title="2、enumerate 枚举"></a>2、enumerate 枚举</h3><p>直接使用 enumerate 枚举容器，第二个参数表示索引的起始值</p><pre><code class="hljs python">x = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>]<span class="hljs-keyword">for</span> i, e <span class="hljs-keyword">in</span> enumerate(x, <span class="hljs-number">10</span>): <span class="hljs-comment"># 枚举</span>    print(i, e)<span class="hljs-number">10</span> <span class="hljs-number">1</span><span class="hljs-number">11</span> <span class="hljs-number">3</span><span class="hljs-number">12</span> <span class="hljs-number">5</span></code></pre><h3 id="3-、in"><a href="#3-、in" class="headerlink" title="3.、in"></a>3.、in</h3><p>判断字符串是否包含某个子串，使用<code>in</code>明显更加可读：</p><pre><code class="hljs python">x = <span class="hljs-string">'zen_of_python'</span><span class="hljs-keyword">if</span> <span class="hljs-string">'zen'</span> <span class="hljs-keyword">in</span> x:    print(<span class="hljs-string">'zen is in'</span>)    zen <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span></code></pre><h3 id="4、zip-打包"><a href="#4、zip-打包" class="headerlink" title="4、zip 打包"></a>4、zip 打包</h3><p>使用 zip 打包后结合 for 使用输出一对，更加符合习惯：</p><pre><code class="hljs python">keys = [<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>]values = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>]<span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> zip(keys, values):    print(k, v)    a <span class="hljs-number">1</span>b <span class="hljs-number">3</span>c <span class="hljs-number">5</span></code></pre><h3 id="5、一对-‘’’"><a href="#5、一对-‘’’" class="headerlink" title="5、一对 ‘’’"></a>5、一对 ‘’’</h3><p>打印被分为多行的字符串，使用一对 <code>&#39;&#39;&#39;</code> 更加符合 Python 习惯：</p><pre><code class="hljs python">print(<span class="hljs-string">'''"Oh no!" He exclaimed.</span><span class="hljs-string">"It's the blemange!"'''</span>)<span class="hljs-string">"Oh no!"</span> He exclaimed.<span class="hljs-string">"It's the blemange!"</span></code></pre><h3 id="6、交换元素"><a href="#6、交换元素" class="headerlink" title="6、交换元素"></a>6、交换元素</h3><p>直接解包赋值，不要再用临时变量 ，更加符合 Python 风格：</p><pre><code class="hljs python">a, b = <span class="hljs-number">1</span>, <span class="hljs-number">3</span>a, b = b, a  <span class="hljs-comment"># 交换a,b</span></code></pre><h3 id="7、join-串联"><a href="#7、join-串联" class="headerlink" title="7、join 串联"></a>7、join 串联</h3><p>串联字符串，dataframe，更习惯使用 join：</p><pre><code class="hljs python">chars = [<span class="hljs-string">'P'</span>, <span class="hljs-string">'y'</span>, <span class="hljs-string">'t'</span>, <span class="hljs-string">'h'</span>, <span class="hljs-string">'o'</span>, <span class="hljs-string">'n'</span>]name = <span class="hljs-string">''</span>.join(chars)print(name)Python</code></pre><h3 id="8、列表生成式"><a href="#8、列表生成式" class="headerlink" title="8、列表生成式"></a>8、列表生成式</h3><p>列表生成式构建高效，符合 Python 习惯：</p><pre><code class="hljs python">data = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span>]result = [i * <span class="hljs-number">2</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> data <span class="hljs-keyword">if</span> i &amp; <span class="hljs-number">1</span>] <span class="hljs-comment"># 奇数则乘以2</span>print(result) <span class="hljs-comment"># [2, 6, 10]</span>[<span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">10</span>]</code></pre><h3 id="9、字典生成式"><a href="#9、字典生成式" class="headerlink" title="9、字典生成式"></a>9、字典生成式</h3><p>除了列表生成式，还有字典生成式：</p><pre><code class="hljs python">keys = [<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>]values = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>]d = &#123;k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> zip(keys, values)&#125;print(d)&#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">3</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">5</span>&#125;</code></pre><h3 id="10、-name-39-main-39-有啥用"><a href="#10、-name-39-main-39-有啥用" class="headerlink" title="10、__name__ == &#39;__main__&#39;有啥用"></a>10、<code>__name__ == &#39;__main__&#39;</code>有啥用</h3><p>曾几何时，看这别人代码这么写，我们也就跟着这么用吧，其实还没有完全弄清楚这行到底干啥。</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mymain</span><span class="hljs-params">()</span>:</span>    print(<span class="hljs-string">'Doing something in module'</span>, __name__)<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:    print(<span class="hljs-string">'Executed from command line'</span>)     mymain()</code></pre><p>加入上面脚本命名为 MyModule，不管在 vscode 还是 pycharm 直接启动，则直接打印出：</p><pre><code class="hljs python">Executed <span class="hljs-keyword">from</span> command lineDoing something <span class="hljs-keyword">in</span> module __main__</code></pre><p>这并不奇怪，和我们预想一样，因为有无这句 <code>__main__</code> ，都会打印出这些。</p><p>但是当我们 <code>import MyModule</code> 时，如果没有这句，直接就打印出：</p><pre><code class="hljs python">In [<span class="hljs-number">2</span>]: <span class="hljs-keyword">import</span> MyModuleExecuted <span class="hljs-keyword">from</span> command lineDoing something <span class="hljs-keyword">in</span> module MyModule</code></pre><p>只是导入就直接执行 mymain 函数，这不符合我们预期。</p><p>如果有主句，导入后符合预期：</p><pre><code class="hljs python">In [<span class="hljs-number">6</span>]: <span class="hljs-keyword">import</span> MyModuleIn [<span class="hljs-number">7</span>]: MyModule.mymain()Doing something <span class="hljs-keyword">in</span> module MyModule</code></pre>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python练习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模型融合</title>
    <link href="/2020/05/22/undefined/"/>
    <url>/2020/05/22/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h2><p>模型融合采用的的思想，即多个模型的组合可以改善整体的表现。集成模型是一种能在各种的机器学习任务上提高准确率的强有力技术。模型融合是kaggle竞赛后期一个重要的环节，大体来说有如下的类型方式：</p><p>1、简单加权融合：</p><ul><li>回归（分类概率）：算术平均融合（Arithmetic mean），几何平均融合（Geometric mean）</li><li>分类：投票（Voting）</li><li>综合：排序融合(Rank averaging)，log融合</li></ul><p>2、stacking/blending:</p><ul><li>构建多层模型，并利用预测结果再拟合预测。</li></ul><p>3、 boosting/bagging:</p><ul><li>多树的提升方法，在xgboost，Adaboost，GBDT等中已经用到</li></ul><h4 id="平均法（Averaging）"><a href="#平均法（Averaging）" class="headerlink" title="平均法（Averaging）"></a>平均法（Averaging）</h4><p><strong>基本思想</strong>：对于回归问题，一个简单直接的思路是取平均。稍稍改进的方法是进行加权平均。权值可以用排序的方法确定，举个例子，比如A、B、C三种基本模型，模型效果进行排名，假设排名分别是1，2，3，那么给这三个模型赋予的权值分别是3/6、2/6、1/6。</p><p>平均法或加权平均法看似简单，其实后面的高级算法也可以说是基于此而产生的，Bagging或者Boosting都是一种把许多弱分类器这样融合成强分类器的思想。</p><ul><li><p><strong>简单算术平均法：</strong>Averaging方法就多个模型预测的结果进行平均。这种方法既可以用于回归问题，也可以用于对分类问题的概率进行平均。</p></li><li><p><strong>加权算术平均法：</strong>这种方法是平均法的扩展。考虑不同模型的能力不同，对最终结果的贡献也有差异，需要用权重来表征不同模型的重要性importance。</p></li></ul><h4 id="投票法（voting）"><a href="#投票法（voting）" class="headerlink" title="投票法（voting）"></a>投票法（voting）</h4><p><strong>基本思想</strong>：假设对于一个二分类问题，有3个基础模型，现在我们可以在这些基学习器的基础上得到一个投票的分类器，把票数最多的类作为我们要预测的类别。</p><ul><li><p><strong>绝对多数投票法：</strong>最终结果必须在投票中占一半以上。</p></li><li><p><strong>相对多数投票法：</strong>最终结果在投票中票数最多。</p></li><li><p><strong>加权投票法</strong></p></li><li><p><strong>硬投票</strong>：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。</p></li><li><p><strong>软投票：</strong>增加了设置权重的功能，可以为不同模型设置不同权重，进而区别模型不同的重要度。</p></li></ul><p>投票法实现代码：</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier<span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> VotingClassifiermodel1 = LogisticRegression(random_state=<span class="hljs-number">2020</span>)model2 = DecisionTreeClassifier(random_state=<span class="hljs-number">2020</span>)model = VotingClassifier(estimators=[(<span class="hljs-string">'lr'</span>, model1), (<span class="hljs-string">'dt'</span>, model2)], voting=<span class="hljs-string">'hard'</span>)model.fit(x_train, y_train)model.score(x_test, y_test)</code></pre><h4 id="堆叠法（Stacking）"><a href="#堆叠法（Stacking）" class="headerlink" title="堆叠法（Stacking）"></a>堆叠法（Stacking）</h4><p><strong>基本思想</strong>：stacking 就是当用初始训练数据学习出若干个基学习器后，将这几个学习器的预测结果作为新的训练集，来学习一个新的学习器。对不同模型预测的结果再进行建模。</p><p><img src="/img/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88.png" srcset="/img/loading.gif" alt="模型融合"></p><p>在stacking方法中，我们把个体学习器叫做初级学习器，用于结合的学习器叫做次级学习器或元学习器，次级学习器用于训练的数据叫做次级训练集。次级训练集是在训练集上用初级学习器得到的。</p><p>Stacking本质上就是这么直接的思路，但是直接这样有时对于如果训练集和测试集分布不那么一致的情况下是有一点问题的，其问题在于用初始模型训练的标签再利用真实标签进行再训练，毫无疑问会导致一定的模型过拟合训练集，这样或许模型在测试集上的泛化能力或者说效果会有一定的下降，因此现在的问题变成了如何降低再训练的过拟合性，这里我们一般有两种方法：</p><ul><li>次级模型尽量选择简单的线性模型，但是经过本人实践线性模型效果并不好，我推荐使用LightGBM</li><li>利用K折交叉验证</li></ul><p>调用APi实现stacking：</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> heamy.dataset <span class="hljs-keyword">import</span> Dataset<span class="hljs-keyword">from</span> heamy.estimator <span class="hljs-keyword">import</span> Regressor, Classifier<span class="hljs-keyword">from</span> heamy.pipeline <span class="hljs-keyword">import</span> ModelsPipeline<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> cross_validation<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestRegressor<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_absolute_error<span class="hljs-comment">#加载数据集</span><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_bostondata = load_boston()X, y = data[<span class="hljs-string">'data'</span>], data[<span class="hljs-string">'target'</span>]X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=<span class="hljs-number">0.1</span>, random_state=<span class="hljs-number">111</span>)<span class="hljs-comment">#创建数据集</span>dataset = Dataset(X_train,y_train,X_test)<span class="hljs-comment">#创建RF模型和LR模型</span>model_rf = Regressor(dataset=dataset, estimator=RandomForestRegressor, parameters=&#123;<span class="hljs-string">'n_estimators'</span>: <span class="hljs-number">50</span>&#125;,name=<span class="hljs-string">'rf'</span>)model_lr = Regressor(dataset=dataset, estimator=LinearRegression, parameters=&#123;<span class="hljs-string">'normalize'</span>: <span class="hljs-literal">True</span>&#125;,name=<span class="hljs-string">'lr'</span>)<span class="hljs-comment"># Stack两个模型</span><span class="hljs-comment"># Returns new dataset with out-of-fold predictions</span>pipeline = ModelsPipeline(model_rf,model_lr)stack_ds = pipeline.stack(k=<span class="hljs-number">10</span>,seed=<span class="hljs-number">111</span>)<span class="hljs-comment">#第二层使用lr模型stack</span>stacker = Regressor(dataset=stack_ds, estimator=LinearRegression)results = stacker.predict()<span class="hljs-comment"># 使用5折交叉验证结果</span>results10 = stacker.validate(k=<span class="hljs-number">5</span>,scorer=mean_absolute_error)</code></pre><p>手动实现：只创建一个基学习器为例</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> StratifiedKFold,KFold,RepeatedKFold<span class="hljs-keyword">import</span> xgboost.sklearn <span class="hljs-keyword">import</span> XGBClassifier<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npskf = StratifiedKFold(n_splits = <span class="hljs-number">5</span>, shuffle = <span class="hljs-literal">True</span> ,random_state=<span class="hljs-number">16</span>)oof_xgb = np.zeros(len(train))  <span class="hljs-comment">#创建数组</span>pre_xgb = np.zeros(len(test))<span class="hljs-keyword">for</span> k,(train_in,test_in) <span class="hljs-keyword">in</span> enumerate(skf.split(train,train_y)):    X_train,X_test,y_train,y_test = X[train_in],X[test_in],y[train_in],y[test_in]    params = &#123;<span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">0.008</span>,               <span class="hljs-string">'n_estimators'</span>: <span class="hljs-number">1000</span>              <span class="hljs-string">'max_depth'</span>: <span class="hljs-number">5</span>,              <span class="hljs-string">'subsample'</span>: <span class="hljs-number">0.8</span>,               <span class="hljs-string">'colsample_bytree'</span>: <span class="hljs-number">0.8</span>,               <span class="hljs-string">'objective'</span>:<span class="hljs-string">'binary:logistic'</span>,              <span class="hljs-string">'eval_metric'</span>:<span class="hljs-string">'auc'</span>,              <span class="hljs-string">'silent'</span>: <span class="hljs-literal">True</span>,               <span class="hljs-string">'nthread'</span>: <span class="hljs-number">4</span>,              &#125;    <span class="hljs-comment"># train</span>    clf = XGBClassifier(params)    clf.fit(trn_x, trn_y,eval_set=[(val_x, val_y)],eval_metric=<span class="hljs-string">'auc'</span>,early_stopping_rounds=<span class="hljs-number">100</span>,verbose=<span class="hljs-number">100</span>)    print(<span class="hljs-string">'Start predicting...'</span>)    oof_xgb[test_in] = clf.predict(X_test)    pre_xgb += clf.predict(test) / skf.n_splits    print(<span class="hljs-string">'XGB predict over'</span>)</code></pre><h4 id="混合法（Blending）"><a href="#混合法（Blending）" class="headerlink" title="混合法（Blending）"></a>混合法（Blending）</h4><p><strong>基本思想：</strong>Blending采用了和stacking同样的方法，不过只从训练集中选择一个fold的结果，再和原始特征进行concat作为元学习器meta learner的特征，测试集上进行同样的操作。<br>把原始的训练集先分成两部分，比如70%的数据作为新的训练集，剩下30%的数据作为测试集。</p><ul><li>第一层，我们在这70%的数据上训练多个模型，然后去预测那30%数据的label，同时也预测test集的label</li><li>在第二层，我们就直接用这30%数据在第一层预测的结果做为新特征继续训练，然后用test集第一层预测的label做特征，用第二层训练的模型做进一步预测</li></ul><p><strong>Stacking与Blending的对比：</strong></p><p>优点在于：</p><ul><li>blending比stacking简单，因为不用进行k次的交叉验证来获得stacker feature</li><li>blending避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集</li></ul><p>缺点在于：</p><ul><li>blending使用了很少的数据（第二阶段的blender只使用training set10%的量）</li><li>blender可能会过拟合</li><li>stacking使用多次的交叉验证会比较稳健</li></ul>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>连续特征处理</title>
    <link href="/2019/05/21/undefined/"/>
    <url>/2019/05/21/undefined/</url>
    
    <content type="html"><![CDATA[<h2 id="连续特征离散化"><a href="#连续特征离散化" class="headerlink" title="连续特征离散化"></a>连续特征离散化</h2><h4 id="为什么要对连续变量进行离散化"><a href="#为什么要对连续变量进行离散化" class="headerlink" title="为什么要对连续变量进行离散化"></a>为什么要对连续变量进行离散化</h4><ul><li>捕捉非线性效应</li><li>方便捕捉交叉效应—groupby</li><li>可以减少过拟合的风险，因为分箱相当于对于数据去粗粒度描述</li><li>离散化后可以提升模型的鲁棒性</li></ul><h4 id="常见的离散化方法"><a href="#常见的离散化方法" class="headerlink" title="常见的离散化方法"></a>常见的离散化方法</h4><h5 id="分箱方法"><a href="#分箱方法" class="headerlink" title="分箱方法"></a>分箱方法</h5><p>等距分箱（<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html" target="_blank" rel="noopener">pandas.cut</a>）</p><pre><code class="hljs python">pd.cut(serise, [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>], labels=<span class="hljs-literal">False</span>)  <span class="hljs-comment">#label指定标签</span></code></pre><p>等频分箱（<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html" target="_blank" rel="noopener">pandas.qcut</a>）</p><pre><code class="hljs python">pd.qcut(range(<span class="hljs-number">5</span>), <span class="hljs-number">3</span>, labels=[<span class="hljs-string">"good"</span>, <span class="hljs-string">"medium"</span>, <span class="hljs-string">"bad"</span>])---[good, good, medium, bad, bad]</code></pre><h5 id="聚类（sklearn-preprocessing-KBinsDiscretizer）"><a href="#聚类（sklearn-preprocessing-KBinsDiscretizer）" class="headerlink" title="聚类（sklearn.preprocessing.KBinsDiscretizer）"></a>聚类（<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html?highlight=kbinsdiscretizer#" target="_blank" rel="noopener"><code>sklearn.preprocessing</code>.KBinsDiscretizer</a>）</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> KBinsDiscretizerest = KBinsDiscretizer(n_bins=<span class="hljs-number">3</span>, encode=<span class="hljs-string">'ordinal'</span>, strategy=<span class="hljs-string">'kmeans'</span>)est.fit(X)Xt = est.transform(X)</code></pre><h5 id="卡方分箱"><a href="#卡方分箱" class="headerlink" title="卡方分箱"></a>卡方分箱</h5><p><a href="https://github.com/Liucoren/Chimerge/blob/master/ChiMerge.py" target="_blank" rel="noopener">参考github上</a></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pdiris = pd.read_csv(<span class="hljs-string">'iris.csv'</span>, header=<span class="hljs-literal">None</span>)iris.columns = [<span class="hljs-string">'sepal_length'</span>, <span class="hljs-string">'sepal_width'</span>,                <span class="hljs-string">'petal_length'</span>, <span class="hljs-string">'petal_width'</span>, <span class="hljs-string">'target_class'</span>]<span class="hljs-comment">#将最小卡方值添加进DataFrame</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge_rows</span><span class="hljs-params">(df,feature)</span>:</span>    tdf = df[:<span class="hljs-number">-1</span>]    distinct_values = sorted(set(tdf[<span class="hljs-string">'chi2'</span>]), reverse=<span class="hljs-literal">False</span>)    col_names =  [feature,<span class="hljs-string">'Iris-setosa'</span>, <span class="hljs-string">'Iris-versicolor'</span>,                   <span class="hljs-string">'Iris-virginica'</span>,<span class="hljs-string">'chi2'</span>]    updated_df  = pd.DataFrame(columns = col_names)  <span class="hljs-comment">#加入卡方值，生成新的数组</span>        updated_df_index=<span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> df.iterrows():         <span class="hljs-keyword">if</span>(index==<span class="hljs-number">0</span>):            updated_df.loc[len(updated_df)] = df.loc[index]            updated_df_index+=<span class="hljs-number">1</span>        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">if</span>(df.loc[index<span class="hljs-number">-1</span>][<span class="hljs-string">'chi2'</span>]==distinct_values[<span class="hljs-number">0</span>]):                updated_df.loc[updated_df_index<span class="hljs-number">-1</span>][<span class="hljs-string">'Iris-setosa'</span>]+=df.loc[index][<span class="hljs-string">'Iris-setosa'</span>]                updated_df.loc[updated_df_index<span class="hljs-number">-1</span>][<span class="hljs-string">'Iris-versicolor'</span>]+=df.loc[index][<span class="hljs-string">'Iris-versicolor'</span>]                updated_df.loc[updated_df_index<span class="hljs-number">-1</span>][<span class="hljs-string">'Iris-virginica'</span>]+=df.loc[index][<span class="hljs-string">'Iris-virginica'</span>]            <span class="hljs-keyword">else</span>:                updated_df.loc[len(updated_df)] = df.loc[index]                updated_df_index+=<span class="hljs-number">1</span>                    updated_df[<span class="hljs-string">'chi2'</span>] = <span class="hljs-number">0.</span>      <span class="hljs-keyword">return</span> updated_df        <span class="hljs-comment">#计算卡方值</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calc_chi2</span><span class="hljs-params">(array)</span>:</span>    shape = array.shape    n = float(array.sum())    row=&#123;&#125;    column=&#123;&#125;        <span class="hljs-comment">#计算每行的和</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(shape[<span class="hljs-number">0</span>]):        row[i] = array[i].sum()        <span class="hljs-comment">#计算每列的和</span>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(shape[<span class="hljs-number">1</span>]):        column[j] = array[:,j].sum()    chi2 = <span class="hljs-number">0</span>        <span class="hljs-comment">#卡方计算公式</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(shape[<span class="hljs-number">0</span>]):        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(shape[<span class="hljs-number">1</span>]):            eij = row[i]*column[j] / n             oij = array[i,j]              <span class="hljs-keyword">if</span> eij==<span class="hljs-number">0.</span>:                chi2 += <span class="hljs-number">0.</span>  <span class="hljs-comment">#确保不存在NaN值</span>            <span class="hljs-keyword">else</span>:                chi2 += math.pow((oij - eij),<span class="hljs-number">2</span>) / float(eij)      <span class="hljs-keyword">return</span> chi2    <span class="hljs-comment">#计算每一个类别的卡方值</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_chi2_column</span><span class="hljs-params">(contingency_table,feature)</span>:</span>    <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> contingency_table.iterrows():        <span class="hljs-keyword">if</span>(index!=contingency_table.shape[<span class="hljs-number">0</span>]<span class="hljs-number">-1</span>):            list1=[]            list2=[]            list1.append(contingency_table.loc[index][<span class="hljs-string">'Iris-setosa'</span>])            list1.append(contingency_table.loc[index][<span class="hljs-string">'Iris-versicolor'</span>])            list1.append(contingency_table.loc[index][<span class="hljs-string">'Iris-virginica'</span>])            list2.append(contingency_table.loc[index+<span class="hljs-number">1</span>][<span class="hljs-string">'Iris-setosa'</span>])            list2.append(contingency_table.loc[index+<span class="hljs-number">1</span>][<span class="hljs-string">'Iris-versicolor'</span>])            list2.append(contingency_table.loc[index+<span class="hljs-number">1</span>][<span class="hljs-string">'Iris-virginica'</span>])            prep_chi2 = np.array([np.array(list1),np.array(list2)])            c2 = calc_chi2(prep_chi2)            contingency_table.loc[index][<span class="hljs-string">'chi2'</span>] = c2    <span class="hljs-keyword">return</span> contingency_table<span class="hljs-comment">#计算频次表</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_contingency_table</span><span class="hljs-params">(dataframe,feature)</span>:</span>    distinct_values = sorted(set(dataframe[feature]), reverse=<span class="hljs-literal">False</span>)    col_names =  [feature,<span class="hljs-string">'Iris-setosa'</span>, <span class="hljs-string">'Iris-versicolor'</span>,<span class="hljs-string">'Iris-virginica'</span>,<span class="hljs-string">'chi2'</span>]    my_contingency  = pd.DataFrame(columns = col_names)        <span class="hljs-comment">#计算唯一值</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(distinct_values)):         temp_df=dataframe.loc[dataframe[feature]==distinct_values[i]]        count_dict = temp_df[<span class="hljs-string">"target_class"</span>].value_counts().to_dict()        setosa_count = <span class="hljs-number">0</span>        versicolor_count = <span class="hljs-number">0</span>        virginica_count = <span class="hljs-number">0</span>        <span class="hljs-keyword">if</span> <span class="hljs-string">'Iris-setosa'</span> <span class="hljs-keyword">in</span> count_dict:            setosa_count = count_dict[<span class="hljs-string">'Iris-setosa'</span>]        <span class="hljs-keyword">if</span> <span class="hljs-string">'Iris-versicolor'</span> <span class="hljs-keyword">in</span> count_dict:            versicolor_count = count_dict[<span class="hljs-string">'Iris-versicolor'</span>]        <span class="hljs-keyword">if</span> <span class="hljs-string">'Iris-virginica'</span> <span class="hljs-keyword">in</span> count_dict:            virginica_count = count_dict[<span class="hljs-string">'Iris-virginica'</span>]        new_row = [distinct_values[i],setosa_count,versicolor_count,virginica_count,<span class="hljs-number">0</span>]        my_contingency.loc[len(my_contingency)] = new_row    <span class="hljs-keyword">return</span> my_contingency<span class="hljs-comment">#ChiMerge</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">chimerge</span><span class="hljs-params">(feature, data, max_interval)</span>:</span>    df = data.sort_values(by=[feature],ascending=<span class="hljs-literal">True</span>).reset_index()        <span class="hljs-comment">#传入频次表</span>    contingency_table = create_contingency_table(df,feature)    <span class="hljs-comment">#计算初始间隔值</span>    num_intervals= contingency_table.shape[<span class="hljs-number">0</span>]     <span class="hljs-comment">#是否满足最大间隔</span>    <span class="hljs-keyword">while</span> num_intervals &gt; max_interval:         <span class="hljs-comment">#相邻列的卡方值</span>        chi2_df = update_chi2_column(contingency_table,feature)         contingency_table = merge_rows(chi2_df,feature)        num_intervals= contingency_table.shape[<span class="hljs-number">0</span>]                   <span class="hljs-comment">#得出结果</span>    print(<span class="hljs-string">'The split points for '</span>+feature+<span class="hljs-string">' are:'</span>)    <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> contingency_table.iterrows():        print(contingency_table.loc[index][feature])        print(<span class="hljs-string">'The final intervals for '</span>+feature+<span class="hljs-string">' are:'</span>)    <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> contingency_table.iterrows():        <span class="hljs-keyword">if</span>(index!=contingency_table.shape[<span class="hljs-number">0</span>]<span class="hljs-number">-1</span>):            <span class="hljs-keyword">for</span> index2, row2 <span class="hljs-keyword">in</span> df.iterrows():                <span class="hljs-keyword">if</span> df.loc[index2][feature]&lt;contingency_table.loc[index+<span class="hljs-number">1</span>][feature]:                    temp = df.loc[index2][feature]        <span class="hljs-keyword">else</span>:            temp = df[feature].iloc[<span class="hljs-number">-1</span>]        print(<span class="hljs-string">"["</span>+str(contingency_table.loc[index][feature])+<span class="hljs-string">","</span>+str(temp)+<span class="hljs-string">"]"</span>)    print(<span class="hljs-string">" "</span>)    <span class="hljs-keyword">if</span> __name__==<span class="hljs-string">'__main__'</span>:    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> [<span class="hljs-string">'sepal_length'</span>, <span class="hljs-string">'sepal_width'</span>, <span class="hljs-string">'petal_length'</span>,<span class="hljs-string">'petal_width'</span>]:        chimerge(feature=feature, data=iris, max_interval=<span class="hljs-number">6</span>)</code></pre><h2 id="特征斜度处理"><a href="#特征斜度处理" class="headerlink" title="特征斜度处理"></a>特征斜度处理</h2><p>使用skew判断斜度，然后Compute the Box-Cox transformation of boxcox_normmax</p><pre><code class="hljs python"><span class="hljs-comment">#修改斜度特征</span><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> skew, norm<span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> boxcox1p<span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> boxcox_normmax<span class="hljs-comment"># 选取数字特征</span>numeric_dtypes = [<span class="hljs-string">'int16'</span>, <span class="hljs-string">'int32'</span>, <span class="hljs-string">'int64'</span>, <span class="hljs-string">'float16'</span>, <span class="hljs-string">'float32'</span>, <span class="hljs-string">'float64'</span>]numeric = []<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X_cleaned.columns:    <span class="hljs-keyword">if</span> X_cleaned[i].dtype <span class="hljs-keyword">in</span> numeric_dtypes:        numeric.append(i)        <span class="hljs-comment"># 找到斜度特征，大于0.5的</span>skew_features = X_cleaned[numeric].apply(<span class="hljs-keyword">lambda</span> x: skew(x)).sort_values(ascending=<span class="hljs-literal">False</span>)high_skew = skew_features[skew_features &gt; <span class="hljs-number">0.5</span>]skew_index = high_skew.indexprint(<span class="hljs-string">"There are &#123;&#125; numerical features with Skew &gt; 0.5 :"</span>.format(high_skew.shape[<span class="hljs-number">0</span>]))skewness = pd.DataFrame(&#123;<span class="hljs-string">'Skew'</span> :high_skew&#125;)print(skew_features.head(<span class="hljs-number">10</span>))<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> skew_index:    X_cleaned[i] = boxcox1p(X_cleaned[i], boxcox_normmax(X_cleaned[i] + <span class="hljs-number">1</span>))</code></pre>]]></content>
    
    
    <categories>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征工程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
